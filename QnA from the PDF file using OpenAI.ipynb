{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekrapid/learning-langchain-ai/blob/PYT-106-r-d-on-lang-chain/QnA%20from%20the%20PDF%20file%20using%20OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages\n",
        "# This is not necessary if the packages have already been installed\n",
        "# using pip or a package manager like conda\n",
        "!pip install langchain\n",
        "!pip install unstructured\n",
        "!pip install unstructured[local-inference]\n",
        "!pip install pinecone-client\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kL33Owm7EQ9H",
        "outputId": "da9640e7-01a3-4d48-cb49-d147641fcc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.146-py3-none-any.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.7/600.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.8.4)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy<2,>=1\n",
            "  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.9\n",
            "    Uninstalling SQLAlchemy-2.0.9:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.9\n",
            "Successfully installed SQLAlchemy-1.4.47 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.146 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.5.13-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.9/dist-packages (from unstructured) (3.4.3)\n",
            "Collecting python-pptx\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from unstructured) (1.5.3)\n",
            "Collecting msg-parser\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argilla\n",
            "  Downloading argilla-1.6.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.9/dist-packages (from unstructured) (3.0.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from unstructured) (4.9.2)\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from unstructured) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2022.12.07 in /usr/local/lib/python3.9/dist-packages (from unstructured) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from unstructured) (2.27.1)\n",
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured) (1.22.4)\n",
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured) (4.65.0)\n",
            "Collecting rich<=13.0.1\n",
            "  Downloading rich-13.0.1-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured) (1.10.7)\n",
            "Collecting deprecated~=1.2.0\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting httpx<0.24,>=0.15\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<1.15,>=1.13 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured) (23.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->unstructured) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->unstructured) (2.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown->unstructured) (6.4.1)\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured) (2022.10.31)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.9/dist-packages (from openpyxl->unstructured) (1.1.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.1.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured) (1.26.15)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown->unstructured) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.7.1->argilla->unstructured) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<=13.0.1->argilla->unstructured) (2.14.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (3.6.2)\n",
            "Building wheels for collected packages: python-docx, python-pptx, olefile\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=27b2fb41364b1e47951f5485f835a04ba4c6548ef4d7b8aeec77fae871076a3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470949 sha256=52966042fcb46c6b5f3ab2c26c1aa4fce2ebda263e991f8191c419dd2a8e46de\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/4a/ed/9653bc799915f52dce3f04d14946fbd85cce9c3cdedc9cfa71\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=33947ed117349eb400b682e5e0bc2ee581428ac4d85ff3b629eecc1614483da4\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/b8/ba/ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
            "Successfully built python-docx python-pptx olefile\n",
            "Installing collected packages: rfc3986, monotonic, commonmark, XlsxWriter, rich, python-magic, python-docx, pypandoc, olefile, h11, deprecated, backoff, python-pptx, msg-parser, httpcore, httpx, argilla, unstructured\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.4\n",
            "    Uninstalling rich-13.3.4:\n",
            "      Successfully uninstalled rich-13.3.4\n",
            "Successfully installed XlsxWriter-3.1.0 argilla-1.6.0 backoff-2.2.1 commonmark-0.9.1 deprecated-1.2.13 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 monotonic-1.6 msg-parser-1.2.0 olefile-0.46 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 rfc3986-1.5.0 rich-13.0.1 unstructured-0.5.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unstructured[local-inference] in /usr/local/lib/python3.9/dist-packages (0.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (2.27.1)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (0.4.27)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (1.5.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (3.0.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2022.12.07 in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (2022.12.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (8.4.0)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (1.11)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (3.4.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (4.9.2)\n",
            "Requirement already satisfied: msg-parser in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (1.2.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (0.8.11)\n",
            "Requirement already satisfied: argilla in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (1.6.0)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.9/dist-packages (from unstructured[local-inference]) (0.6.21)\n",
            "Collecting unstructured-inference==0.3.2\n",
            "  Downloading unstructured_inference-0.3.2.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser[layoutmodels,tesseract]\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.6.0.66\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python==4.6.0.66->unstructured-inference==0.3.2->unstructured[local-inference]) (1.22.4)\n",
            "Requirement already satisfied: deprecated~=1.2.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (1.2.13)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.13 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (1.14.1)\n",
            "Requirement already satisfied: httpx<0.24,>=0.15 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (0.23.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (1.6)\n",
            "Requirement already satisfied: rich<=13.0.1 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (13.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (23.1)\n",
            "Requirement already satisfied: pydantic>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (1.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (4.65.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.9/dist-packages (from argilla->unstructured[local-inference]) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->unstructured[local-inference]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->unstructured[local-inference]) (2022.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown->unstructured[local-inference]) (6.4.1)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.9/dist-packages (from msg-parser->unstructured[local-inference]) (0.46)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured[local-inference]) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured[local-inference]) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->unstructured[local-inference]) (8.1.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.9/dist-packages (from openpyxl->unstructured[local-inference]) (1.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from python-pptx->unstructured[local-inference]) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured[local-inference]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured[local-inference]) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->unstructured[local-inference]) (1.26.15)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (0.16.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown->unstructured[local-inference]) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.7.1->argilla->unstructured[local-inference]) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured[local-inference]) (1.16.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<=13.0.1->argilla->unstructured[local-inference]) (2.14.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich<=13.0.1->argilla->unstructured[local-inference]) (0.9.1)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->unstructured-inference==0.3.2->unstructured[local-inference]) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->unstructured-inference==0.3.2->unstructured[local-inference]) (6.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (1.10.1)\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (2.0.0+cu118)\n",
            "Collecting effdet\n",
            "  Downloading effdet-0.3.0-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (0.15.1+cu118)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime->unstructured-inference==0.3.2->unstructured[local-inference]) (1.11.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime->unstructured-inference==0.3.2->unstructured[local-inference]) (23.3.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime->unstructured-inference==0.3.2->unstructured[local-inference]) (3.20.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn->unstructured-inference==0.3.2->unstructured[local-inference]) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (3.6.2)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf>=2.0\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm>=0.4.12\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (3.25.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting Wand>=0.6.10\n",
            "  Downloading Wand-0.6.11-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.6/143.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow\n",
            "  Downloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (40.0.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime->unstructured-inference==0.3.2->unstructured[local-inference]) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (2.1.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (1.15.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (1.4.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.3.2->unstructured[local-inference]) (2.21)\n",
            "Building wheels for collected packages: unstructured-inference, iopath, antlr4-python3-runtime\n",
            "  Building wheel for unstructured-inference (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unstructured-inference: filename=unstructured_inference-0.3.2-py3-none-any.whl size=33506 sha256=77afba3917980ff38b498da785e701f4d9251da0cfacb3f4bdf703e727badb22\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/e1/fa/ba42c58d1a92c07799d65196bb47bb7a6f9ae2f51cbf5388f5\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=b179d60246ad605bcb2132970a40acabe118e4964ad8e034e35cf43df96e51e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/13/6d/441d8f2af76ee6d2a3e67eebb1d0c556fefcee0a8b32266a8e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=377009ee09986aa20d892a131941e40e8e70a784d57f2ed07d2e9f22d29790af\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "Successfully built unstructured-inference iopath antlr4-python3-runtime\n",
            "Installing collected packages: Wand, tokenizers, antlr4-python3-runtime, uvicorn, python-multipart, portalocker, pillow, opencv-python, omegaconf, humanfriendly, starlette, pytesseract, pdf2image, iopath, huggingface-hub, coloredlogs, transformers, pdfminer.six, onnxruntime, fastapi, pdfplumber, layoutparser, timm, effdet, unstructured-inference\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.7.0.72\n",
            "    Uninstalling opencv-python-4.7.0.72:\n",
            "      Successfully uninstalled opencv-python-4.7.0.72\n",
            "Successfully installed Wand-0.6.11 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 effdet-0.3.0 fastapi-0.95.1 huggingface-hub-0.13.4 humanfriendly-10.0 iopath-0.1.10 layoutparser-0.3.4 omegaconf-2.3.0 onnxruntime-1.14.1 opencv-python-4.6.0.66 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.9.0 pillow-9.5.0 portalocker-2.7.0 pytesseract-0.3.10 python-multipart-0.0.6 starlette-0.26.1 timm-0.6.13 tokenizers-0.13.3 transformers-4.28.1 unstructured-inference-0.3.2 uvicorn-0.21.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting loguru>=0.5.0\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (2.27.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (1.26.15)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (1.22.4)\n",
            "Collecting dnspython>=2.0.0\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pinecone-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pinecone-client) (2022.12.7)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.3.0 loguru-0.7.0 pinecone-client-2.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required packages\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import pinecone\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CZrKXPxES1s",
        "outputId": "6881e4d8-c1ea-4984-ac76-1693e3f30050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the required modules from the packages\n",
        "# The modules will be used later in the code\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
        "PINECONE_SERVICE_NAME = os.getenv(\"PINECONE_SERVICE_NAME\")"
      ],
      "metadata": {
        "id": "aI7JXmEsES4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to load a document from a URL\n",
        "def load_document(url):\n",
        "    if url.startswith(\"http\"):\n",
        "        loader = OnlinePDFLoader(url)\n",
        "    else:\n",
        "        loader = UnstructuredPDFLoader(url)\n",
        "\n",
        "    return loader.load()"
      ],
      "metadata": {
        "id": "WPNobzvzES6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_pdf_loader = load_document(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")\n",
        "print(online_pdf_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Ys8nohES8t",
        "outputId": "021c51e8-7e62-4036-b4fc-4e5502d802eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:unstructured:detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='DATA SCIENCEtoTHEFIELD GUIDE     SECOND EDITION© COPYRIGHT 2015 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.\\n\\nFOREWORDData Science touches every aspect of our lives on a daily basis. When we visit the doctor, drive our cars, get on an airplane, or shop for services, Data Science is changing the way we interact with and explore  our world.  Our world is now measured, mapped, and recorded in digital bits. Entire lives, from birth to death, are now catalogued in the digital realm. These data, originating from such diverse sources as connected vehicles, underwater microscopic cameras, and photos we post to social media, have propelled us into the greatest age of discovery humanity has ever known. It is through Data Science that we are unlocking the secrets hidden within these data. We are making discoveries that will forever change how we live and interact with the world around us. The impact of these changes is having a profound effect on humanity. We have propelled ourselves into this age of discovery through our incremental technological improvements. Data Science has become the catalyzing force behind our next evolutionary leap. Our own evolution is now inextricably linked to that of computers. The way we live our lives and the skills that are important to our very existence are directly dependent upon the functions Data Science can achieve on our behalf.   As we move into this new future, it is clearer than ever, that businesses must adjust to these changes or risk being left behind. From influencing retail markets, to setting public health and safety policies, or to addressing social unrest, organizations of all types are generating value through  Data Science. Data is our new currency and Data Science is  the mechanism by which we tap into it. Data Science is an auspicious and profound way of applying our curiosity and technical tradecraft to solve humanity’s toughest challenges. The growing power, importance, and responsibility of applying Data Science methodologies to these challenges is unimaginable. Our own biases and assumptions can have profound outcomes on business, national security, and our daily lives. A new class of practitioners and leaders are needed to navigate this new future. Data Scientists are our guides on this journey as they are creating radical new ways of thinking about data and the world around us.   We want to share our passion for Data Science and start a conversation with you. This is a journey worth taking. ››\\n\\nEveryone you\\n\\nwill ever meet\\n\\nknows something\\n\\nyou don’t.\\n\\n[1]\\n\\n››\\n\\nT H E   S TO RY\\n\\nof    T H E   F I E L D\\n\\nG U I D E\\n\\nSeveral years ago we created The Field Guide to Data Science because we wanted to help organizations of all types and sizes. There were countless industry and academic publications describing what Data Science is and why we should care, but very little information was available to explain how to make use of data as a resource. We find that situation to be just as true today as we did two years ago, when we created the first edition of the field guide.\\n\\nAt Booz Allen Hamilton, we built an industry-leading team of Data Scientists. Over the course of hundreds of analytic challenges for countless clients, we’ve unraveled the DNA of Data Science. Many people have put forth their thoughts on single aspects of Data Science. We believe we can offer a broad perspective on the conceptual models, tradecraft, processes and culture of Data Science – the what, the why, the who and the how. Companies with strong Data Science teams often focus on a single class of problems – graph algorithms for social network analysis, and recommender models for online shopping are two notable examples. Booz Allen is different. In our role as consultants, we support a diverse set of government and commercial clients across a variety of domains. This allows us to uniquely understand the DNA of Data Science.\\n\\nOur goal in creating The Field Guide to Data Science was to capture what we have learned and to share it broadly. The field of Data Science has continued to advance since we first released the field guide. As a result, we decided to release this second edition, incorporating a few new and important concepts. We also added technical depth and richness that we believe practitioners will find useful.\\n\\nWe want this effort to continue driving forward the science and art of Data Science.\\n\\nThis field guide came from the passion our team feels for its work. It is not a textbook nor is it a superficial treatment. Senior leaders will walk away with a deeper understanding of the concepts at the heart of Data Science. Practitioners will add to their toolbox. We hope everyone will enjoy the journey.\\n\\n››\\n\\nW E   A R E   A L L A U T H O R S   of    T H I S S T O R Y\\n\\nWe recognize that Data Science is a team sport. The Field Guide to Data Science provides Booz Allen Hamilton’s perspective on the complex and sometimes mysterious field of Data Science. We cannot capture all that is Data Science. Nor can we keep up - the pace at which this field progresses outdates work as fast as it is produced. As a result, we opened this field guide to the world as a living document to bend and grow with technology, expertise, and evolving techniques.\\n\\nThank you to all the people that have emailed us your ideas as well as the 100+ people who have watched, starred, or forked our GitHub repository. We truly value the input of the community, as we work together to advance the science and art of Data Science. This is why we have included authors from outside Booz Allen Hamilton on this second edition of The Field Guide to Data Science.\\n\\nIf you find the guide to be useful, neat, or even lacking, then we encourage you to add your expertise, including:\\n\\n››\\n\\n››\\n\\n››\\n\\n››\\n\\nCase studies from which you have learned\\n\\nCitations from journal articles or papers that inspire you\\n\\nAlgorithms and techniques that you love\\n\\nYour thoughts and comments on other people’s additions\\n\\nEmail us your ideas and perspectives at data_science@bah.com or submit them via a pull request on the GitHub repository.\\n\\nJoin our conversation and take the journey with us. Tell us and the world what you know. Become an author of this story.\\n\\nACKNOWLEDGEMENTSWe would like to express our sincerest gratitude to all those who have made The Field Guide to Data Science such a success. Thank you to the nearly 15,000 people who have downloaded the digital copy from our website and the 100+ people who have connected with The Field Guide on our GitHub page. We have been overwhelmed by the popularity of the work within the Data Science community. Thank you to all of the practitioners who are using The Field Guide as a resource. We are excited to know that the work has had such a strong influence, from shaping technical approaches to serving as the foundation for the very definition and role of Data Science within major government and commercial organizations. Thank you to the educators and academics who have incorporated The Field Guide into your course work. We appreciate your trusting this guide as a way to introduce your students to Data Science.   It is an honor to know that we are shaping the next generation of Data Scientists.Thank you to the organizational leaders who have shared your feedback, encouragement, and success stories. We are thrilled  to know that The Field Guide  has helped so many organizations, from energy, to life sciences,  to retail, to begin their Data Science journeys.  We hope you will all continue to find value from The Field Guide to Data Science and to share in our excitement around the release of this second edition. Please continue to be part of the conversation and take this journey with us. ››\\n\\n››\\n\\nT H E   O U T L I N E of   O U R   S T O R Y 12    ›› Meet Your Guides 17    ›› The Short Version – The Core Concepts of Data Science 18    ›› Start Here for the Basics – An Introduction to Data Science\\n\\nWhat Do We Mean by Data Science?\\n\\nHow Does Data Science Actually Work?\\n\\nWhat Does It Take to Create a Data Science Capability?\\n\\n46    ›› Take off the Training Wheels – The Practitioner’s Guide to Data Science\\n\\nGuiding Principles\\n\\nThe Importance of Reason\\n\\nComponent Parts of Data Science\\n\\nFractal Analytic Model\\n\\nThe Analytic Selection Process\\n\\nGuide to Analytic Selection\\n\\nDetailed Table of Analytics\\n\\n84    ››  Life in the Trenches – Navigating Neck Deep in Data\\n\\nGoing Deep into Machine Learning\\n\\nFeature Engineering\\n\\nFeature Selection\\n\\nEnsemble Models\\n\\nData Veracity\\n\\nApplication of Domain Knowledge\\n\\nThe Curse of Dimensionality\\n\\nModel Validation\\n\\n102    ›› Putting it all Together – Our Case Studies\\n\\nStreamlining Medication Review\\n\\nReducing Flight Delays\\n\\nMaking Vaccines Safer\\n\\nForecasting the Relative Risk for the Onset of\\n\\nMass Killings to Help Prevent Future Atrocities\\n\\nPredicting Customer Response\\n\\n114   ›› Closing Time\\n\\nParting Thoughts\\n\\nThe Future of Data Science\\n\\nReferences\\n\\nAbout Booz Allen Hamilton\\n\\nMEET your GUIDESFred Blackburn(@boozallen)Data Science is a field that is  evolving at a very rapid pace…be  part of the journey. Angela Zutavern(@angelazutavern)Data Science is about asking bigger questions, seeing future possibilities, and creating outcomes you desire. Josh Sullivan (@joshdsullivan)Leading our Data Science team shows me every day the incredible power of discovery and human curiosity. Don’t be afraid to blend art and science to advance your own view of data analytics – it can be a powerful mixture.Steve Escaravage(@sescarav)Invest your time and energy in data that is difficult to assemble.  If it doesn’t exist, find a way to make it exist.Peter Guerra (@petrguerra)Data Science is the most fascinating blend of art and math and code and sweat and tears. It can take you to the highest heights and the lowest depths in an instant, but it is the only way we will be able to understand and describe the why.Ezmeralda Khalil(@ezmeraldakhalil)The power of data science lies in the execution.››THE FIELD GUIDE to  DATA SCIENCE\\n\\nSteven Mills (@stevndmills)Data Science truly can change the world.Stephanie Beben(@boozallen)Begin every new data challenge with deep curiosity along with a healthy dose of skepticism.Alex Cosmas (@boozallen)Data scientists should be truth-seekers, not fact-seekers.Kirk Borne(@KirkDBorne)Focus on value, not volume.Brian Keller (@boozallen)Grit will get you farther than talent.Drew Farris (@drewfarris)Don’t forget to play. Play with tools, play with data, and play with algorithms. You just might discover something that will help you solve that next nagging problem. 13Meet Your Guides\\n\\nPaul Yacci(@paulyacci)In the jungle of data, don\\'t miss the forest for the trees, or the trees for the forest. Stephanie Rivera (@boozallen)I treat Data Science like I do rock climbing: awesome dedication leads to incremental improvement. Persistence leads to the top.Charles Glover(@MindAfterMath)The beauty of data science lies in satisfying curiosities about important problems by playing with data and algorithms.Aaron Sander (@ajsander) Data science is changing corporate culture to be more like the open source environment. More open, more collaborative, and faster paced.Michael Kim (@boozallen)Data science is both an art  and science. We would like to thank the following people for their contributions and edits: Tim Andrews, Mike Delurey, Greg Dupier, Jason Escaravage, Christine Fantaskey, Juergen Klenk, Dan Liebermann, Mark Rockley and Katie Wilks.THE FIELD GUIDE to  DATA SCIENCE\\n\\nCOMMUNITY CONTRIBUTORSWill Cukierski, kaggle(@kaggle) I took the one in the direction  of the negative gradient,  And that has made all the difference.Armen Kherlopian (@akherlopian)A Data Scientist must continuously seek truth in spite of\\xa0ambiguity; therein rests the basis of rigor and insight.Mark Herman(@cloudEBITDA)End every analysis with…  ‘and therefore.’Ed Kohlwey (@ekohlwey)Data Science is about formally analyzing everything around you and becoming data driven.››Two roads diverged in a wood, and I—15Meet Your Guides\\n\\n››\\n\\nThe S H O RT V E R S I O N\\n\\n›› Data Science is the art of turning data into actions. It’s all about the tradecraft. Tradecraft is the process, tools and technologies for humans and computers to work together to transform data into insights.\\n\\n›› Data Science tradecraft creates data products.\\n\\nData products provide actionable information without exposing decision makers to the underlying data or analytics (e.g., buy/sell strategies for financial instruments, a set of actions to improve product yield, or steps to improve product marketing).\\n\\n›› Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern- based) reasoning. This is a fundamental change from traditional analysis approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. Models of reality no longer need to be static. They are constantly tested, updated and improved until better models are found.\\n\\n›› Data Science is necessary for companies to stay with the\\n\\npack and compete in the future. Organizations are constantly making decisions based on gut instinct, loudest voice and best argument – sometimes they are even informed by real information. The winners and the losers in the emerging data economy are going to be determined by their Data Science teams.\\n\\n›› Data Science capabilities can be built over time. Organizations mature through a series of stages – Collect, Describe, Discover, Predict, Advise – as they move from data deluge to full Data Science maturity. At each stage, they can tackle increasingly complex analytic goals with a wider breadth of analytic capabilities. However, organizations need not reach maximum Data Science maturity to achieve success. Significant gains can be found in every stage.\\n\\n›› Data Science is a different kind of team sport.\\n\\nData Science teams need a broad view of the organization. Leaders must be key advocates who meet with stakeholders to ferret out the hardest challenges, locate the data, connect disparate parts of the business, and gain widespread buy-in.\\n\\nThe Short Version\\n\\n17\\n\\n17\\n\\nAN INTRODUCTION TO DATA SCIENCEIf you haven’t heard of Data Science, you’re behind the times. Just renaming your Business Intelligence group the Data Science group is not the solution. START HERE   for   THE BASICS\\n\\nWhat do We Mean by Data Science?\\n\\nDescribing Data Science is like trying to describe a sunset – it should be easy, but somehow capturing the words is impossible.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\n» Data Product\\n\\nA data product provides actionable information without exposing decision makers to the underlying data or analytics. Examples include:\\n\\nMovie Recommendations\\n\\nWeather Forecasts\\n\\nStock Market Predictions\\n\\nProduction Process\\n\\nImprovements\\n\\nHealth Diagnosis\\n\\nFlu Trend Predictions\\n\\nTargeted Advertising\\n\\nData Science Defined\\n\\nData Science is the art of turning data into actions. This is accomplished through the creation of data products, which provide actionable information without exposing decision makers to the underlying data or analytics (e.g., buy/sell strategies for financial instruments, a set of actions to improve product yield, or steps to improve product marketing).\\n\\nPerforming Data Science requires the extraction of timely, actionable information from diverse data sources to drive data products. Examples of data products include answers to questions such as: “Which of my products should I advertise more heavily to increase profit? How can I improve my compliance program, while reducing costs? What manufacturing process change will allow me to build a better product?” The key to answering these questions is: understand the data you have and what the data inductively tells you.\\n\\nRead this for additional background:\\n\\nThe term Data Science appeared in the computer science literature throughout the 1960s-1980s. It was not until the late 1990s however, that the field as we describe it here, began to emerge from the statistics and data mining communities (e.g., [2] and [3]). Data Science was first introduced as an independent discipline in 2001.[4] Since that time, there have been countless articles advancing the discipline, culminating with Data Scientist being declared the sexiest job of the 21st century.[5]\\n\\nWe established our first Data Science team at Booz Allen in 2010. It began as a natural extension of our Business Intelligence and cloud\\n\\ninfrastructure development work. We saw the need for a new approach to distill value from our clients’ data. We approached the problem with a multidisciplinary team of computer scientists, mathematicians and domain experts. They immediately produced new insights and analysis paths, solidifying the validity of the approach. Since that time, our Data Science team has grown to 250 staff supporting dozens of clients across a variety of domains. This breadth of experience provides a unique perspective on the conceptual models, tradecraft, processes and culture of Data Science.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n21\\n\\n21\\n\\nSource: Booz Allen Hamilton What makes Data Science Different?Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. This is a fundamental change from traditional analytic approaches. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. In fact, to do the discovery of significant insights that are the hallmark of Data Science, you must have the tradecraft and the interplay between inductive and deductive reasoning. By actively combining the ability to reason deductively and inductively, Data Science creates an environment where models of reality no longer need to be static and empirically based. Instead, they are constantly tested, updated and improved until better models are found. These concepts are summarized in the figure, The Types of Reason and Their Role in Data Science Tradecraft.THE TYPES OF REASON…DEDUCTIVE REASONING: ›Commonly associated with “formal logic.” ›Involves reasoning from known premises, or premises presumed to be true, to a certain conclusion. ›The conclusions reached are certain, inevitable, inescapable.INDUCTIVE REASONING ›Commonly known as “informal logic,” or “everyday argument.” ›Involves drawing uncertain inferences, based on probabilistic reasoning. ›The conclusions reached are probable, reasonable, plausible, believable.…AND THEIR ROLE IN DATA SCIENCE TRADECRAFT.DEDUCTIVE REASONING: ›Formulate hypotheses about relationships and underlying models. ›Carry out experiments with the data to test hypotheses and models.INDUCTIVE REASONING ›Exploratory data analysis to discover or reﬁne hypotheses. ›Discover new relationships, insights and analytic paths from the data.The Types of Reason and Their Role in Data Science TradecraftTHE FIELD GUIDE to  DATA SCIENCE\\n\\nSource: Booz Allen Hamilton 23Start Here for the Basics23Start Here for the BasicsThe differences between Data Science and traditional analytic approaches do not end at seamless shifting between deductive and inductive reasoning. Data Science offers a distinctly different perspective than capabilities such as Business Intelligence. Data Science should not replace Business Intelligence functions within an organization, however. The two capabilities are additive and complementary, each offering a necessary view of business operations and the operating environment. The figure, Business Intelligence and Data Science – A Comparison, highlights the differences between the two capabilities. Key contrasts include:››Discovery vs. Pre-canned Questions: Data Science actually works on discovering the question to ask as opposed to just asking it.››Power of Many vs. Ability of One: An entire team provides a common forum for pulling together computer science, mathematics and domain expertise. ››Prospective vs. Retrospective: Data Science is focused on obtaining actionable information from data as opposed to reporting historical facts.LOOKING BACKWARD AND FORWARDFIRST THERE WAS  BUSINESS INTELLIGENCEDeductive ReasoningBackward LookingSlice and Dice DataWarehoused and Siloed DataAnalyze the Past, Guess the FutureCreates ReportsAnalytic OutputNOW WE\\'VE ADDED  DATA SCIENCEInductive and Deductive ReasoningForward LookingInteract with DataDistributed, Real Time DataPredict and AdviseCreates Data  ProductsAnswer Questions and Create New OnesActionable AnswerBusiness Intelligence and Data Science - A Comparison (adapted in part from [6])\\n\\nWhat is the Impact of Data Science?\\n\\nAs we move into the data economy, Data Science is the competitive advantage for organizations interested in winning – in whatever way winning is defined. The manner in which the advantage is defined is through improved decision-making. A former colleague liked to describe data-informed decision making like this: If you have perfect information or zero information then your task is easy – it is in between those two extremes that the trouble begins. What he was highlighting is the stark reality that whether or not information is available, decisions must be made.\\n\\nThe way organizations make decisions has been evolving for half a century. Before the introduction of Business Intelligence, the only options were gut instinct, loudest voice, and best argument. Sadly, this method still exists today, and in some pockets it is the predominant means by which the organization acts. Take our advice and never, ever work for such a company!\\n\\nFortunately for our economy, most organizations began to inform their decisions with real information through the application of simple statistics. Those that did it well were rewarded; those that did not failed. We are outgrowing the ability of simple stats to keep pace with market demands, however. The rapid expansion of available data and the tools to access and make use of the data at scale are enabling fundamental changes to the way organizations make decisions.\\n\\nData Science is required to maintain competitiveness in the increasingly data-rich environment. Much like the application of simple statistics, organizations that embrace Data Science will be rewarded while those that do not will be challenged to keep pace. As more complex, disparate datasets become available, the chasm between these groups will only continue to widen. The figure, The Business Impacts of Data Science, highlights the value awaiting organizations that embrace Data Science.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nSource: Booz Allen Hamilton 25Start Here for the Basics25Start Here for the BasicsDATA SCIENCE IS NECESSARY...17-49%  increase in productivity when organizations increase data usability by 10%11-42%  return on assets (ROA) when organizations increase data access by 10% 241%   increase in ROI when organizations use big data to  improve competitiveness1000%  increase in ROI when deploying analytics across most of the organization, aligning daily operations with senior management\\'s goals, and incorporating big data5-6%   performance improvement for organizations making  data-driven decisions. ...TO COMPETE IN THE FUTUREThe Business Impacts of Data Science (adapted from [7], [8] and [9])\\n\\nWhat is Different Now?\\n\\nFor 20 years IT systems were built the same way. We separated the people who ran the business from the people who managed the infrastructure (and therefore saw data as simply another thing they had to manage). With the advent of new technologies and analytic techniques, this artificial – and highly ineffective – separation of critical skills is no longer necessary. For the first time, organizations can directly connect business decision makers to the data. This simple step transforms data from being ‘something to be managed’ into ‘something to be valued.’\\n\\nIn the wake of the transformation, organizations face a stark choice: you can continue to build data silos and piece together disparate information or you can consolidate your data and distill answers.\\n\\nFrom the Data Science perspective, this is a false choice: The siloed approach is untenable when you consider the (a) the opportunity cost of not making maximum use of all available data to help an organization succeed, and (b) the resource and time costs of continuing down the same path with outdated processes. The tangible benefits of data products include:\\n\\n››\\n\\n››\\n\\nOpportunity Costs: Because Data Science is an emerging field, opportunity costs arise when a competitor implements and generates value from data before you. Failure to learn and account for changing customer demands will inevitably drive customers away from your current offerings. When competitors are able to successfully leverage Data Science to gain insights, they can drive differentiated customer value propositions and lead their industries as a result.\\n\\nEnhanced Processes: As a result of the increasingly interconnected world, huge amounts of data are being generated and stored every instant. Data Science can be used to transform data into insights that help improve existing processes. Operating costs can be driven down dramatically by effectively incorporating the complex interrelationships in data like never before. This results in better quality assurance, higher product yield and more effective operations.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n27\\n\\n27\\n\\nHow does Data Science\\n\\nActually Work?\\n\\nIt’s not rocket science… it’s something better - Data Science\\n\\nLet’s not kid ourselves - Data Science is a complex field. It is difficult, intellectually taxing work, which requires the sophisticated integration of talent, tools and techniques. But as a field guide, we need to cut through the complexity and provide a clear, yet effective way to understand this new world.\\n\\nTo do this, we will transform the field of Data Science into a set of simplified activities as shown in the figure, The Four Key Activities of a Data Science Endeavor. Data Science purists will likely disagree with this approach, but then again, they probably don’t need a field guide, sitting as they do in their ivory towers! In the real world, we need clear and simple operating models to help drive us forward.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nSource: Booz Allen Hamilton 1234AcquirePrepareAnalyzeActLowHighDegreeof EffortData Science ActivitiesTryEvaluateSetupDoEvaluate29Start Here for the Basics29Start Here for the BasicsActivity 1: Acquire  This activity focuses on obtaining the data you need. Given the nature of data, the details of this activity depend heavily on who you are and what you do. As a result, we will not spend a lot of time on this activity other than to emphasize its importance and to encourage an expansive view on which data can and should be used. Activity 2: Prepare Great outcomes don’t just happen by themselves. A lot depends on preparation, and in Data Science, that means manipulating the data to ﬁt your analytic needs.  This stage can consume a great deal of time, but it is an excellent investment. The beneﬁts are immediate and long term. Activity 3: Analyze This is the activity that consumes the lion’s share of the team’s attention. It is also the most challenging and exciting (you will see a lot of ‘aha moments’ occur in this space). As the most challenging and vexing of the four activities, this ﬁeld guide focuses on helping you do this better and faster.Activity 4: Act Every effective Data Science team analyzes its data with a purpose – that is, to turn data into actions. Actionable and impactful insights are the holy grail of Data Science. Converting insights into action can be a politically charged activity, however. This activity depends heavily on the culture and character of your organization, so we will leave you to ﬁgure out those details for yourself.The Four Key Activities of a Data Science Endeavor\\n\\nAcquire\\n\\nAll analysis starts with access to data, and for the Data Scientist this axiom holds true. But there are some significant differences – particularly with respect to the question of who stores, maintains and owns the data in an organization.\\n\\nBut before we go there, lets look at what is changing. Traditionally, rigid data silos artificially define the data to be acquired. Stated another way, the silos create a filter that lets in a very small amount of data and ignores the rest. These filtered processes give us an artificial view of the world based on the ‘surviving data,’ rather than one that shows full reality and meaning. Without a broad and expansive dataset, we can never immerse ourselves in the diversity of the data. We instead make decisions based on limited and constrained information.\\n\\nEliminating the need for silos gives us access to all the data at once – including data from multiple outside sources. It embraces the reality that diversity is good and complexity is okay. This mindset creates a completely different way of thinking about data in an organization by giving it a new and differentiated role. Data represents a significant new profit and mission-enhancement opportunity for organizations.\\n\\nBut as mentioned earlier, this first activity is heavily dependent upon the situation and circumstances. We can’t leave you with anything more than general guidance to help ensure maximum value:\\n\\n››\\n\\n››\\n\\n››\\n\\n››\\n\\nLook inside first: What data do you have current access to that you are not using? This is in large part the data being left behind by the filtering process, and may be incredibly valuable.\\n\\nRemove the format constraints: Stop limiting your data acquisition mindset to the realm of structured databases. Instead, think about unstructured and semi-structured data as viable sources.\\n\\nFigure out what’s missing: Ask yourself what data would make a big difference to your processes if you had access to it. Then go find it!\\n\\nEmbrace diversity: Try to engage and connect to publicly available sources of data that may have relevance to your domain area.\\n\\n» Not All Data Is Created Equal\\n\\nAs you begin to aggregate data, remember that not all data is created equally. Organizations have a tendency to collect any data that is available. Data that is nearby (readily accessible and easily obtained) may be cheap to collect, but there is no guarantee it is the right data to collect.  Focus on the data with the highest ROI for your organization.  Your Data Science team can help identify that data. Also remember that you need to strike a balance between the data that you need and the data that you have. Collecting huge volumes of data is useless and costly if it is not the data that you need.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nPrepare\\n\\nOnce you have the data, you need to prepare it for analysis.\\n\\nOrganizations often make decisions based on inexact data. Data stovepipes mean that organizations may have blind spots. They are not able to see the whole picture and fail to look at their data and challenges holistically. The end result is that valuable information is withheld from decision makers. Research has shown almost 33% of decisions are made without good data or information. [10]\\n\\nWhen Data Scientists are able to explore and analyze all the data, new opportunities arise for analysis and data-driven decision making. The insights gained from these new opportunities will significantly change the course of action and decisions within an organization. Gaining access to an organization’s complete repository of data, however, requires preparation.\\n\\nOur experience shows time and time again that the best tool for Data Scientists to prepare for analysis is a lake – specifically, the Data Lake.[11] This is a new approach to collecting, storing and integrating data that helps organizations maximize the utility of their data. Instead of storing information in discrete data structures, the Data Lake consolidates an organization’s complete repository of data in a single, large view. It eliminates the expensive and cumbersome data-preparation process, known as Extract/Transform/Load (ETL), necessary with data silos. The entire body of information in the Data Lake is available for every inquiry – and all at once.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n31\\n\\n31\\n\\nSource: Booz Allen Hamilton   AnalyzeWe have acquired the data… we have prepared it… now it is time to analyze it. The Analyze activity requires the greatest effort of all the activities in a Data Science endeavor. The Data Scientist actually builds the analytics that create value from data. Analytics in this context is an iterative application of specialized and scalable computational resources and tools to provide relevant insights from exponentially growing data. This type of analysis enables real-time understanding of risks and opportunities by evaluating situational, operational and behavioral data.With the totality of data fully accessible in the Data Lake, organizations can use analytics to find the kinds of connections and patterns that point to promising opportunities. This high-speed analytic connection is done within the Data Lake, as opposed to older style sampling methods that could only make use of a narrow slice of the data. In order to understand what was in the lake, you had to bring the data out and study it. Now you can dive into the lake, bringing your analytics to the data. The figure, Analytic Connection in the Data Lake, highlights the concept of diving into the Data Lake to discover new connections and patterns. Analytic Connection in the Data LakeTHE FIELD GUIDE to  DATA SCIENCE\\n\\nData Scientists work across the spectrum of analytic goals – Describe, Discover, Predict and Advise. The maturity of an analytic capability determines the analytic goals encompassed. Many variables play key roles in determining the difficulty and suitability of each goal for an organization. Some of these variables are the size and budget of an organization and the type of data products needed by the decision makers. A detailed discussion on analytic maturity can be found in Data Science Maturity within an Organization.\\n\\nIn addition to consuming the greatest effort, the Analyze activity is by far the most complex. The tradecraft of Data Science is an art. While we cannot teach you how to be an artist, we can share foundational tools and techniques that can help you be successful. The entirety of Take Off the Training Wheels is dedicated to sharing insights we have learned over time while serving countless clients. This includes descriptions of a Data Science product lifecycle and the Fractal Analytic Model (FAM). The Analytic Selection Process and accompanying Guide to Analytic Selection provide key insights into one of the most challenging tasks in all of Data Science – selecting the right technique for the job.\\n\\nAct\\n\\nNow that we have analyzed the data, it’s time to take action.\\n\\nThe ability to make use of the analysis is critical. It is also very situational. Like the Acquire activity, the best we can hope for is to provide some guiding principles to help you frame the output for maximum impact. Here are some key points to keep in mind when presenting your results:\\n\\n1.  The finding must make sense with relatively little up-front\\n\\ntraining or preparation on the part of the decision maker.\\n\\n2.  The finding must make the most meaningful patterns, trends\\n\\nand exceptions easy to see and interpret.\\n\\n3.  Every effort must be made to encode quantitative data\\n\\naccurately so the decision maker can accurately interpret and compare the data.\\n\\n4.  The logic used to arrive at the finding must be clear and compelling as well as traceable back through the data.\\n\\n5.  The findings must answer real business questions.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n33\\n\\n33\\n\\nSource: Booz Allen Hamilton Proportionof EffortMaturityStages of MaturityCollectDescribeDiscoverPredictAdviseData Silos35Start Here for the Basics35Start Here for the BasicsData Science Maturity within  an OrganizationThe four activities discussed thus far provide a simplified view of Data Science. Organizations will repeat these activities with each new Data Science endeavor. Over time, however, the level of effort necessary for each activity will change. As more data is Acquired and Prepared in the Data Lake, for example, significantly less effort will need to be expended on these activities. This is indicative of a maturing Data Science capability.Assessing the maturity of your Data Science capability calls for a slightly different view. We use The Data Science Maturity Model as a common framework for describing the maturity progression and components that make up a Data Science capability. This framework can be applied to an organization’s Data Science capability or even to the maturity of a specific solution, namely a data product. At each stage of maturity, powerful insight can be gained. The Data Science Maturity Model\\n\\nWhen organizations start out, they have Data Silos. At this stage, they have not carried out any broad Aggregate activities. They may not have a sense of all the data they have or the data they need. The decision to create a Data Science capability signals the transition into the Collect stage.\\n\\nAll of your initial effort will be focused on identifying and aggregating data. Over time, you will have the data you need and a smaller proportion of your effort can focus on Collect. You can now begin to Describe your data. Note, however, that while the proportion of time spent on Collect goes down dramatically, it never goes away entirely. This is indicative of the four activities outlined earlier – you will continue to Aggregate and Prepare data as new analytic questions arise, additional data is needed and new data sources become available.\\n\\nOrganizations continue to advance in maturity as they move through the stages from Describe to Advise. At each stage they can tackle increasingly complex analytic goals with a wider breadth of analytic capabilities. As described for Collect, each stage never goes away entirely. Instead, the proportion of time spent focused on it goes down and new, more mature activities begin. A brief description of each stage of maturity is shown in the table The Stages of Data Science Maturity.\\n\\nThe Stages of Data Science Maturity\\n\\nStage\\n\\nDescription\\n\\nExample\\n\\nCollect\\n\\nFocuses on collecting internal\\n\\nor external datasets.\\n\\nGathering sales records and\\n\\ncorresponding weather data.\\n\\nDescribe\\n\\nSeeks to enhance or reﬁne raw data as well as leverage basic analytic functions such as counts.\\n\\nHow are my customers\\n\\ndistributed with respect to\\n\\nlocation, namely zip code?\\n\\nDiscover\\n\\nIdentiﬁes hidden relationships\\n\\nor patterns.\\n\\nAre there groups within\\n\\nmy regular customers that\\n\\npurchase similarly?\\n\\nPredict\\n\\nUtilizes past observations to\\n\\npredict future observations.\\n\\nCan we predict which products that certain customer groups are more likely to purchase?\\n\\nAdvise\\n\\nDeﬁnes your possible decisions, optimizes over those decisions, and advises to use the decision that gives the best outcome.\\n\\nYour advice is to target advertise to speciﬁc groups for certain products to maximize revenue.\\n\\nSource: Booz Allen Hamilton\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nThe maturity model provides a powerful tool for understanding and appreciating the maturity of a Data Science capability. Organizations need not reach maximum maturity to achieve success. Significant gains can be found in every stage. We believe strongly that one does not engage in a Data Science effort, however, unless it is intended to produce an output – that is, you have the intent to Advise. This means simply that each step forward in maturity drives you to the right in the model diagram. Moving to the right requires the correct processes, people, culture and operating model – a robust Data Science capability. What Does it Take to Create a Data Science Capability? addresses this topic.\\n\\nWe have observed very few organizations actually operating at the highest levels of maturity, the Predict and Advise stages. The tradecraft of Discover is only now maturing to the point that organizations can focus on advanced Predict and Advise activities. This is the new frontier of Data Science. This is the space in which we will begin to understand how to close the cognitive gap between humans and computers. Organizations that reach Advise will be met with true insights and real competitive advantage.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n37\\n\\n37\\n\\n»  Where does your organization\\n\\nfall in analytic maturity?\\n\\nTake the quiz!\\n\\n1.   How many data sources do\\n\\nyou collect?\\n\\na.  Why do we need a bunch of data?\\n\\n– 0 points, end here.\\n\\nb.  I don’t know the exact number.\\n\\n– 5 points\\n\\nc.  We identiﬁed the required data and\\n\\ncollect it. – 10 points\\n\\n2. Do you know what questions\\n\\nyour Data Science team is trying to answer?\\n\\na.  Why do we need questions?\\n\\n0 points\\n\\nb.  No, they ﬁgure it out for themselves.\\n\\n– 5 points\\n\\nc.  Yes, we evaluated the questions that will have the largest impact to the business. – 10 points\\n\\n3. Do you know the important factors\\n\\ndriving your business?\\n\\na. I have no idea.  – 0 points\\n\\nb.  Our quants help me ﬁgure it out.\\n\\n– 5 points\\n\\nc.  We have a data product for that.\\n\\n– 10 points\\n\\n4. Do you have an understanding of\\n\\nfuture conditions?\\n\\na. I look at the current conditions and\\n\\nread the tea leaves. – 0 points\\n\\nb.  We have a data product for that.\\n\\n– 5 points\\n\\n5. Do you know the best course of action to take for your key decisions?\\n\\na.  I look at the projections and plan a\\n\\ncourse. – 0 points\\n\\nb.  We have a data product for that.\\n\\n– 5 points\\n\\nCheck your score:\\n\\n0 – Data Silos, 5-10 – Collect, 10-20 – Describe, 20-30 – Discover, 30-35 – Predict, 35-40 - Advise\\n\\nSource: Booz Allen Hamilton\\n\\nWhat Does it Take to Create a Data Science Capability?Data Science is all about building teams and culture.Many organizations (both commercial and government) see the potential in capitalizing on data to unlock operational efficiencies, to create new services and experiences, and to propel innovation. Unfortunately, too many business leaders invest in one-off technical solutions— with a big price tag and mixed results— instead of investing in building a strategic Data Science capability. A Data Science capability embeds and operationalizes Data Science across an enterprise such that it can deliver the next level of organizational performance and return on investment. A Data Science capability moves an organization beyond performing pockets of analytics to an enterprise approach that uses analytical insights as part of the normal course of business. When building a capability, it is important for an organization to first identify its analytic goals (i.e., what it is trying to achieve through analytics) and then assess its readiness to achieve those goals – examining both technical readiness and organizational readiness. An organization can then make strategic choices on how to address gaps and begin to build their capability. THE FIELD GUIDE to  DATA SCIENCE\\n\\nSource: Booz Allen Hamilton COMPUTER SCIENCEProvides the environment in which data products are created.DOMAIN EXPERTISEProvides understanding of the reality in which a problem space exists.MATHEMATICSProvides the theoretical structure in which Data Science problems are examined.The Data Science Venn Diagram (inspired by [12])39Start Here for the Basics39Start Here for the BasicsBuilding Your Data Science TeamA critical component to any Data Science capability is having the  right team. Data Science depends on a diverse set of skills as shown  in The Data Science Venn Diagram. Computers provide the environment in which data-driven hypotheses are tested, and as such, computer science is necessary for data manipulation and processing. Mathematics provides the theoretical structure in which Data Science problems are examined. A rich background in statistics, geometry, linear algebra, and calculus are all important to understand the basis for many algorithms and tools. Finally, domain expertise contributes  to an understanding of what problems actually need to be solved,  what kind of data exists in the domain, and how the problem space may be instrumented and measured.Remember that Data Science is a team sport. Most of the time, you will not be able to find the rare “unicorns” - people with expertise across all three of the skill areas. Therefore, it is important to build a blended team that covers all three elements of the Data Science  Venn Diagram.\\n\\nSource: Booz Allen Hamilton BALANCING THE DATA  SCIENCE TEAM EQUATIONBalancing the composition of a Data Science team is much like balancing the reactants and products in a chemical reaction. Each side of the equation must represent the same quantity of any particular element. In the case of Data Science, these elements are the foundational technical skills Computer Science (CS), Mathematics (M) and Domain Expertise (DE). The reactants, your Data Scientists, each have their own unique skills compositions. You must balance the staff mix to meet the skill requirements of the Data Science team, the product in the reaction. If you don’t correctly balance the equation, your Data Science team will not have the desired impact on the organization.2 CS M2 + 2 CS + M DE → CS4 M5 DEIn the example above, your project requires four parts computer science, ﬁve parts mathematics and one part domain expertise. Given the skills mix of the staff, ﬁve people are needed to balance the equation. Throughout your Data Science project, the skills requirements of the team will change. You will need to re-balance the equation to ensure the reactants balance with  the products.THE FIELD GUIDE to  DATA SCIENCE\\n\\n» The Triple Threat Unicorn\\n\\nIndividuals who are great at all three of the Data Science foundational technical skills are like unicorns – very rare and if you’re ever lucky enough to find one they should be treated carefully. When you manage these people:\\n\\n› Encourage them to lead your team, but not manage it. Don’t bog them down with responsibilities of management that could be done by other staff.\\n\\n› Put extra effort into managing\\n\\ntheir careers and interests within your organization. Build opportunities for promotion into your organization that allow them to focus on mentoring other Data Scientists and progressing the state of the art while also advancing their careers.\\n\\n› Make sure that they have the opportunity to present and spread their ideas in many different forums, but also be sensitive to their time.\\n\\nUnderstanding What Makes\\n\\na Data Scientist\\n\\nData Science often requires a significant investment of time across a variety of tasks. Hypotheses must be generated and data must be acquired, prepared, analyzed, and acted upon. Multiple techniques are often applied before one yields interesting results. If that seems daunting, it is because it is. Data Science is difficult, intellectually taxing work, which requires lots of talent: both tangible technical skills as well as the intangible “x-factors.”\\n\\nThere are four independent yet comprehensive foundational Data Science competency clusters that, when considered together, convey the essence of what it means to be a successful Data Scientist. There are also reach back competencies that complement the foundational clusters but do not define the core tradecraft or attributes of the Data Science team.\\n\\nData Science Competency Framework (see [13] for complete framework)\\n\\nClusters\\n\\nCompetencies\\n\\nDescription\\n\\nTechnical:\\n\\n“Knows How and\\n\\nWhat to do”\\n\\nAdvanced Mathematics;\\n\\nComputer Science; Data\\n\\nMining and Integration;\\n\\nDatabase Science; Research\\n\\nDesign; Statistical Modeling;\\n\\nMachine Learning;\\n\\nOperations Research;\\n\\nProgramming and Scripting\\n\\nThe technical competency cluster depicts the foundational technical and specialty knowledge and skills needed for successful performance in each job or role.\\n\\nData Science\\n\\nConsulting:\\n\\n“Can Do in\\n\\na Client and\\n\\nCustomer\\n\\nEnvironment”\\n\\nCollaboration and Teamwork;\\n\\nCommunications; Data\\n\\nScience Consulting; Ethics\\n\\nand Integrity\\n\\nCognitive:\\n\\n“Able to Do or\\n\\nLearn to Do”\\n\\nCritical Thinking; Inductive\\n\\nand Deductive Reasoning;\\n\\nProblem Solving\\n\\nThe characteristics in the\\n\\nconsulting competency\\n\\ncluster can help Data\\n\\nScientists easily integrate\\n\\ninto various market or\\n\\ndomain contexts and partner\\n\\nwith business units to\\n\\nunderstand the environment\\n\\nand solve complex problems.\\n\\nThe cognitive competency cluster represents the type of critical thinking and reasoning abilities (both inductive and deductive) a Data Scientist should have to perform their job.\\n\\nPersonality:\\n\\n“Willing or\\n\\nMotivated to Do”\\n\\nAdaptability/Flexibility;\\n\\nAmbiguity Tolerance; Detail\\n\\nOrientation; Innovation and\\n\\nCreativity; Inquisitiveness;\\n\\nPerseverance; Resilience\\n\\nand Hardiness; Self\\n\\n\\n\\nConﬁdence; Work Ethic\\n\\nThe personality competency cluster describes the personality traits that drive behaviors that are beneﬁcial to Data Scientists, such as inquisitiveness, creativity, and perseverance.\\n\\nReach Back Competencies for Data Science Teams\\n\\nBusiness Acumen; Data Visualization; Domain Expertise; Program Management\\n\\nSource: Booz Allen Hamilton\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n41\\n\\n41\\n\\nThe most important qualities of Data Scientists tend to be the intangible aspects of their personalities. Data Scientists are by nature curious, creative, focused, and detail-oriented.\\n\\n››\\n\\n››\\n\\n››\\n\\n››\\n\\nCuriosity is necessary to peel apart a problem and examine the interrelationships between data that may appear superficially unrelated.\\n\\nCreativity is required to invent and try new approaches to solving a problem, which often times have never been applied in such a context before.\\n\\nFocus is required to design and test a technique over days and weeks, find it doesn’t work, learn from the failure, and try again.\\n\\nAttention to Detail is needed to maintain rigor, and to detect and avoid over-reliance on intuition when examining data.\\n\\nWe have found the single most important attribute is flexibility in overcoming setbacks - the willingness to abandon one idea and try a new approach. Often, Data Science is a series of dead ends before, at last, the way forward is identified. It requires a unique set of personality attributes to succeed in such an environment. Technical skills can be developed over time: the ability to be flexible – and patient, and persistent – cannot.\\n\\nFinding the Athletes for Your Team\\n\\nBuilding a Data Science team is complex. Organizations must simultaneously engage existing internal staff to create an “anchor” who can be used to recruit and grow the team, while at the same time undergo organizational change and transformation to meaningfully incorporate this new class of employee. Building a team starts with identifying existing staff within an organization who have a high aptitude for Data Science. Good candidates will have a formal background in any of the three foundational technical skills we mentioned, and will most importantly have the personality traits necessary for Data Science. They may often have advanced (masters or higher) degrees, but not always. The very first staff you identify should also have good leadership traits and a sense of purpose for the organization, as they will lead subsequent staffing and recruiting efforts. Don’t discount anyone – you will find Data Scientists in the strangest places with the oddest combinations of backgrounds.\\n\\n» Don’t judge a book by its cover, or a Data Scientist by his or her degree in this case. Amazing Data Scientists can be found anywhere. Just look at the diverse and surprising sampling of degrees held by Our Experts:\\n\\n› Bioinformatics\\n\\n› Biomedical Engineering\\n\\n› Biophysics\\n\\n› Business\\n\\n› Computer Graphics\\n\\n› Computer Science\\n\\n› English\\n\\n› Forest Management\\n\\n› History\\n\\n›\\n\\nIndustrial Engineering\\n\\n›\\n\\nInformation Technology\\n\\n› Mathematics\\n\\n› National Security Studies\\n\\n› Operations Research\\n\\n› Physics\\n\\n› Wildlife & Fisheries\\n\\nManagement\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nShaping the Culture\\n\\nIt is no surprise—building a culture is hard and there is just as much art to it as there is science. It is about deliberately creating the conditions for Data Science to flourish (for both Data Scientists and the average employee). You can then step back to empower collective ownership of an organic transformation.\\n\\nData Scientists are fundamentally curious and imaginative. We have a saying on our team, “We’re not nosy, we’re Data Scientists.” These qualities are fundamental to the success of the project and to gaining new dimensions on challenges and questions. Often Data Science projects are hampered by the lack of the ability to imagine something new and different. Fundamentally, organizations must foster trust and transparent communication across all levels, instead of deference to authority, in order to establish a strong Data Science team. Managers should be prepared to invite participation more frequently, and offer explanation or apology less frequently.\\n\\nIt is important to provide a path into the Data Science “club” and to empower the average employee to feel comfortable and conversant with Data Science. For something to be part of organizational culture, it must be part of the fabric of the employee behavior. That means employees must interact with and use data products in their daily routines. Another key ingredient to shaping the right culture is that all employees need a baseline of Data Science knowledge, starting with a common lexicon, to facilitate productive collaboration and instill confidence. While not everyone will be Data Scientists, employees need to identify with Data Science and be equipped with the knowledge, skills, and abilities to work with Data Scientists to drive smarter decisions and deliver exponential organizational performance.\\n\\n»  “I\\'m not nosey, I\\'m a Data\\n\\nScientist”\\n\\n› Always remember that\\n\\nunrelenting curiosity and imagination should be the hallmarks of Data Science. They are fundamental to the success of every Data Science project.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n43\\n\\n43\\n\\nSource: Booz Allen Hamilton Data Science Operating Models  (see [13] for complete descriptions)Selecting Your Operating ModelDepending on the size, complexity, and the business drivers, organizations should consider one of three Data Science operating models: Centralized, Deployed, or Diffused. These three models are shown in the figure, Data Science Operating Models.Centralized Data Science teams serve the organization across all business units. The team is centralized under a Chief Data Scientist and they all co-locate together. The domain experts come to this organization for brief rotational stints to solve challenges around the business. This model provides greater efficiency with limited Data Science resources but can also create the perceived need to compete with other business units for Data Science talent. To address this challenge, it is important to place emphasis on portfolio management and creating transparency on how organizations will identify and select Data Science projects. Deployed Data Science teams go to the business unit and reside there for short- or long-term assignments. They are their own entity and they work with the domain experts within the group to solve hard problems. In the deployed model, Data Science teams collectively develop knowledge across business units, with central leadership as a bridging mechanism for addressing organization-wide issues. However, Data Science teams are accountable to business unit leadership and their centralized leadership, which could cause confusion and conflict. In this model, it is important  to emphasize conflict management to avoid competing priorities. The Diffused Data Science team is one that is fully embedded with each group and becomes part of the long-term organization. These teams work best when the nature of the domain or business unit is already one focused on analytics. In the Diffused Model, teams can quickly react to high-priority business unit needs. However, the lack of central management can result in duplicate software and tools. Additionally, business units with the most money will often have full access to analytics while other units have none—this may not translate to the greatest organizational impact. In this model, it is important to establish cross-functional groups that promote organization-wide governance and peer collaboration. Full descriptions of each operating model can be found in Booz Allen’s Tips for Building a Data Science Capability [13].CENTRALIZEDBusiness units bring their problems to a centralized Data Science team.DIFFUSEDData Scientists are fully embedded within the business units.DEPLOYEDSmall Data Science teams are forward deployed to business units.THE FIELD GUIDE to  DATA SCIENCE\\n\\nHow to Generate Momentum\\n\\nA Data Science effort can start at the grass roots level by a few folks tackling hard problems, or as directed by the Chief Executive Officer, Chief Data Officer, or Chief Analytics Officer. Regardless of how an effort starts, political headwinds often present more of a challenge than solving any technical hurdles. To help battle the headwinds, it is important to generate momentum and prove the value a Data Science team can provide. The best way to achieve this is usually through a Data Science prototype or proof of concept. Proofs of concepts can generate the critical momentum needed to jump start any Data Science Capability Four qualities, in particular, are essential for every Data Science prototype:\\n\\n1.  Organizational Buy-in: A prototype will only succeed if the individuals involved believe in it and are willing to do what they can to make it successful. A good way to gauge interest is to meet with the middle managers; their views are usually indicative of the larger group.\\n\\n2.  Clear ROI: Before choosing a prototype problem, ensure that the ROI of the analytic output can be clearly and convincingly demonstrated for both the project and the organization as a whole. This outcome typically requires first reaching consensus on how the ROI will be determined and measured, so that the benefit can be quantified.\\n\\n3.  Necessary Data: Before selecting a prototype, you must first\\n\\ndetermine exactly what data is needed, whether it will actually be available, and what it will cost in terms of time and expense. It is important to note that organizations do not need all the possible data – they can still create successful analytics even with some gaps.\\n\\n4.  Limited Complexity and Duration:  The problem addressed\\n\\nby the prototype should achieve a balance between being too complex and too easy. Organizations new to Data Science often try to show its value with highly complex projects. However, the greater the complexity, the greater the risk of failure. At the same time, if the problem is too easy to solve, senior leaders and others in the organization may not see the need for Data Science. Look for efforts that could benefit from large datasets, or bringing together disparate datasets that have never been combined before, as opposed to those that require complex analytic approaches. In these cases, there is often low-hanging fruit that can lead to significant value for the organization.\\n\\nStart Here for the Basics Start Here for the Basics\\n\\n45\\n\\n45\\n\\nTA K E   O F F   the  T R A I N I N G   W H E E L S\\n\\nTHE PRACTITIONER’S GUIDE\\n\\nTO DATA SCIENCE\\n\\nRead this section to get beyond the hype and learn the secrets of being a Data Scientist.\\n\\nGuiding Principles\\n\\nFailing is good; failing quickly is even better.\\n\\nThe set of guiding principles that govern how we conduct the tradecraft of Data Science are based loosely on the central tenets of innovation, as the two areas are highly connected. These principles are not hard and fast rules to strictly follow, but rather key tenets that have emerged in our collective consciousness. You should use these to guide your decisions, from problem decomposition through implementation.\\n\\n› Be willing to fail. At the core of Data Science is the idea of\\n\\nexperimentation. Truly innovative solutions only emerge when you experiment with new ideas and applications. Failure is an acceptable byproduct of experimentation. Failures locate regions that no longer need to be considered as you search for a solution. › Fail often and learn quickly. In addition to a willingness to fail, be ready to fail repeatedly. There are times when a dozen approaches must be explored in order to find the one that works. While you shouldn’t be concerned with failing, you should strive to learn from the attempt quickly. The only way you can explore a large number of solutions is to do so quickly.\\n\\n› Keep the goal in mind. You can often get lost in the details and challenges of an implementation. When this happens, you lose sight of your goal and begin to drift off the path from data to analytic action. Periodically step back, contemplate your goal, and evaluate whether your current approach can really lead you where you want to go.\\n\\n› Dedication and focus lead to success. You must often explore\\n\\nmany approaches before finding the one that works. It’s easy to become discouraged. You must remain dedicated to your analytic goal. Focus on the details and the insights revealed by the data. Sometimes seemingly small observations lead to big successes.\\n\\n» Tips From the Pros\\n\\nIt can be easier to rule out a solution\\n\\nthan conﬁrm its correctness. As a\\n\\nresult, focus on exploring obvious\\n\\nshortcomings that can quickly\\n\\ndisqualify an approach. This will allow\\n\\nyou to focus your time on exploring\\n\\ntruly viable approaches as opposed to\\n\\ndead ends.\\n\\n» Tips From the Pros\\n\\nIf the ﬁrst thing you try to do is to\\n\\n› Complicated does not equal better. As technical practitioners, we\\n\\ncreate the ultimate solution, you will\\n\\nfail, but only after banging your head\\n\\nagainst a wall for several weeks.\\n\\nhave a tendency to explore highly complex, advanced approaches. While there are times where this is necessary, a simpler approach can often provide the same insight. Simpler means easier and faster to prototype, implement and verify.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nThe Importance of Reason\\n\\nBeware: in the world of Data Science, if it walks like a duck and quacks like a duck, it might just be a moose.\\n\\nData Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. Inductive reasoning and exploratory data analysis provide a means to form or refine hypotheses and discover new analytic paths. Models of reality no longer need to be static. They are constantly tested, updated and improved until better models are found.\\n\\nThe analysis of big data has brought inductive reasoning to the forefront. Massive amounts of data are analyzed to identify correlations. However, a common pitfall to this approach is confusing correlation with causation. Correlation implies but does not prove causation. Conclusions cannot be drawn from correlations until the underlying mechanisms that relate the data elements are understood. Without a suitable model relating the data, a correlation may simply be a coincidence.\\n\\n»  Correlation without\\n\\nCausation\\n\\nA common example of this phenomenon is the high correlation between ice cream consumption and the murder rate during the summer months. Does this mean ice cream consumption causes murder or, conversely, murder causes ice cream consumption? Most likely not, but you can see the danger in mistaking correlation for causation. Our job as Data Scientists is making sure we understand the difference.\\n\\nTake off the Training Wheels\\n\\n49\\n\\n››Paul YacciThe Dangers of RejectionIn the era of big data, one piece of analysis that is frequently overlooked is the problem of ﬁnding patterns when there are actually no apparent patterns. In statistics this is referred to as Type I error. As scientists, we are always on the lookout for a new or interesting breakthrough that could explain a phenomenon. We hope to see a pattern in our data that explains something or that can give us an answer. The primary goal of hypothesis testing is to limit Type I error. This is accomplished by using small α values. For example, a α value of 0.05 states that there is a 1 in 20 chance that the test will show that there is something signiﬁcant when in actuality there isn’t. This problem compounds when testing multiple hypotheses. When running multiple hypothesis tests, we are likely to encounter Type I error. As more data becomes available for analysis, Type I error needs to be controlled. One of my projects required testing the difference between the means of two microarray data samples. Microarray data contains thousands of measurements but is limited in the number of observations. A common analysis approach is to measure the same genes under different conditions. If there is a signiﬁcant enough difference in the amount of gene expression between the two samples, we can say that the gene is correlated with a particular phenotype. One way to do this is to take the mean of each phenotype for a particular gene and formulate a hypothesis to test whether there is a signiﬁcant difference between the means. Given that we were running thousands of these tests at α = 0.05, we found several differences that were signiﬁcant. The problem was that some of these could be caused by random chance. Many corrections exist to control for false indications of signiﬁcance. The Bonferroni correction is one of the most conservative. This calculation lowers the level below which you will reject the null hypothesis (your p value). The formula is alpha/n, where n equals the number of hypothesis tests that you are running. Thus, if you were to run 1,000 tests of signiﬁcance at α = 0.05, your p value should be less than 0.00005 (0.05/1,000) to reject the null hypothesis. This is obviously a much more stringent value.  A large number of the previously signiﬁcant values were no longer signiﬁcant, revealing the true relationships within the data.The corrected signiﬁcance gave us conﬁdence that the observed expression levels were due to differences in the cellular gene expression rather than noise. We were able to use this information to begin investigating what proteins and pathways were active in the genes expressing the phenotype of interest. By solidifying our understanding of the causal relationships, we focused our research on the areas that could lead to new discoveries about gene function and, ultimately to improved medical treatments.THE FIELD GUIDE to  DATA SCIENCE\\n\\nReason and common sense are foundational to Data Science. Without these, data is simply a collection of bits. Context, inferences and models are created by humans and carry with them biases and assumptions. Blindly trusting your analyses is a dangerous thing that can lead to erroneous conclusions. When you approach an analytic challenge, you should always pause to ask yourself the following questions: ›What problem are we trying  to solve? Articulate the answer as a sentence, especially when communicating with the end-user. Make sure that it sounds like an answer. For example, “Given a fixed amount of human capital, deploying people with these priorities will generate the best return on their time.” ›Does the approach make sense? Write out your analytic plan. Embrace the discipline of writing, as it brings structure to your thinking. Back of the envelope calculations are an existence proof of your approach. Without this kind of preparation, computers are power tools that can produce lots of bad answers really fast.  ›Does the answer make sense? Can you explain the answer? Computers, unlike children, do what they are told. Make sure you spoke to it clearly by validating that the instructions you provided are the ones you intended. Document your assumptions and make sure they have not introduced bias in your work. ›Is it a finding or a mistake?  Be skeptical of surprise findings. Experience says that it if seems wrong, it probably is wrong. Before you accept that conclusion, however, make sure you understand  and can clearly explain why  it is wrong.  ›Does the analysis address the original intent? Make sure that you are not aligning the answer with the expectations of the client. Always speak the truth, but remember that answers of “your baby is ugly” require more, not less, analysis. ›Is the story complete? The goal of your analysis is to tell an actionable story. You cannot rely on the audience to stitch the pieces together. Identify potential holes in your story and fill them to avoid surprises. Grammar, spelling and graphics matter; your audience will lose confidence in your analysis if your results look sloppy. ›Where would we head next? No analysis is ever finished, you just run out of resources. Understand and explain what additional measures could  be taken if more resources  are found. » Tips From the ProsBetter a short pencil than a long memory. End every day by documenting where you are; you may learn something along the way. Document what you learned and why you changed your plan. » Tips From the ProsTest your answers with a friendly audience to make sure your ﬁndings hold water. 51Take off the Training Wheels\\n\\nComponent Parts of\\n\\nData Science\\n\\nThere is a web of components that interact to create your solution space. Understanding how they are connected is critical to your ability to engineer solutions to Data Science problems.\\n\\nThe components involved in any Data Science project fall into a number of different categories including the data types analyzed, the analytic classes used, the learning models employed and the execution models used to run the analytics. The interconnection across these components, shown in the figure, Interconnection Among the Component Parts of Data Science, speaks to the complexity of engineering Data Science solutions. A choice made for one component exerts influence over choices made for others categories. For example, data types lead the choices in analytic class and learning models, while latency, timeliness and algorithmic parallelization strategy inform the execution model. As we dive deeper into the technical aspects of Data Science, we will begin with an exploration of these components and touch on examples of each.\\n\\nRead this to get the quick and dirty:\\n\\nWhen engineering a Data Science solution, work from an understanding of the components that define the solution space. Regardless of your analytic goal, you must consider the data types with which you will be working, the classes of analytics you will use to generate your data product,\\n\\nhow the learning models embodied will operate and evolve, and the execution models that will govern how the analytic will be run. You will be able to articulate a complete Data Science solution only after considering each of these aspects.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nSource: Booz Allen Hamilton datastreamingdatabatchdatastructureddataunstructuredanalyticstransforminganalyticslearninganalyticspredictivelearningsupervisedlearningunsupervisedlearningonlinelearningofﬂineexecutionbatchexecutionstreamingexecutionparallelexecutionserialEXECUTION MODELSDATA TYPESANALYTIC  CLASSESLEARNING MODELSInterconnection Among the Component Parts of Data Science53Take off the Training Wheels\\n\\nData Types\\n\\nData types and analytic goals go hand-in-hand much like the chicken and the egg; it is not always clear which comes first. Analytic goals are derived from business objectives, but the data type also influences the goals. For example, the business objective of understanding consumer product perception drives the analytic goal of sentiment analysis. Similarly, the goal of sentiment analysis drives the selection of a text-like data type such as social media content. Data type also drives many other choices when engineering your solutions.\\n\\nThere are a number of ways to classify data. It is common to characterize data as structured or unstructured. Structured data exists when information is clearly broken out into fields that have an explicit meaning and are highly categorical, ordinal or numeric. A related category, semi-structured, is sometimes used to describe structured data that does not conform to the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers. Unstructured data, such as natural language text, has less clearly delineated meaning. Still images, video and audio often fall under the category of unstructured data. Data in this form requires preprocessing to identify and extract relevant ‘features.’ The features are structured information that are used for indexing and retrieval, or training classification, or clustering models.\\n\\nData may also be classified by the rate at which it is generated, collected or processed. The distinction is drawn between streaming data that arrives constantly like a torrent of water from a fire hose, and batch data, which arrives in buckets. While there is rarely a connection between data type and data rate, data rate has significant influence over the execution model chosen for analytic implementation and may also inform a decision of analytic class or learning model.\\n\\nTake off the Training Wheels\\n\\n55\\n\\nSource: Booz Allen Hamilton Classes of Analytic TechniquesAs a means for helping conceptualize the universe of possible analytic techniques, we grouped them into nine basic classes. Note that techniques from a given class may be applied in multiple ways to achieve various analytic goals. Membership in a class simply indicates a similar analytic function. The nine analytic classes are shown in the figure, Classes of Analytic Techniques.›»Transforming Analytics ›Aggregation: Techniques to summarize the data. These include basic statistics (e.g., mean, standard deviation), distribution fitting, and graphical plotting. ›Enrichment: Techniques for adding additional information to the data, such as source information or other labels. ›Processing: Techniques that address data cleaning, preparation, and separation. This group also includes common algorithm pre-processing activities such as transformations and feature extraction.›»Learning Analytics ›Regression: Techniques for estimating relationships among variables, including understanding which variables are important in predicting future values. ›Clustering: Techniques to segment the data into naturally similar groups. ›Classification: Techniques to identify data element  group membership. ›Recommendation: Techniques to predict the rating or preference for a new entity, based on historic preference  or behavior.›»Predictive Analytics ›Simulation: Techniques to imitate the operation of a real-world process or system. These are useful for predicting behavior under new conditions. ›Optimization: Operations Research techniques focused on selecting the best element from a set of available alternatives to maximize a utility function.AggregationEnrichmentProcessingSimulationOptimizationRegressionClusteringClassiﬁcationRecommendTRANSFORMINGLEARNINGANALYTIC CLASSESPREDICTIVEClasses of Analytic TechniquesTHE FIELD GUIDE to  DATA SCIENCE\\n\\nSource: Booz Allen Hamilton Learning ModelsAnalytic classes that perform predictions, such as regression, clustering, classification and recommendation employ learning models. These models characterize how the analytic is trained to perform judgments on new data based on historic observation. Aspects of learning models describe both the types of judgments performed and how the models evolve over time, as shown in the figure, Analytic Learning Models.Learning models are typically described as belonging to the categories of unsupervised or supervised learning. Supervised learning takes place when a model is trained using a labeled dataset that has a known class or category associated with each data element. The model relates features found in training instances with labels so that predictions can be made for unlabeled instances. Unsupervised learning involves no a-priori knowledge about the classes into which data can be placed. Unsupervised learning uses the features in the dataset to form groupings based on feature similarity. Semi-supervised learning is a hybrid between these two approaches, using a small amount of labeled data in conjunction with a large amount of unlabeled data. This is done to improve learning accuracy in cases where only a small number of labeled observations are available for learning.There are a variety of ways to train learning models. A useful distinction is between those that are trained in a single pass, which are known as offline models, and those that are trained incrementally over time, known as online models. Many learning approaches have online or offline variants. The decision to use one or another is based on the analytic goals and execution models chosen.Generating an offline model requires taking a pass over the entire training dataset. Improving the model requires making separate passes over the data. These models are static in that once trained, their predictions will not change until a new model is created through a subsequent training stage. Offline model performance is easier to evaluate due to this deterministic behavior. Deployment of the model into a production environment involves swapping out the old model for the new.Online models dynamically evolve over time, meaning they only require a single deployment into a production setting. The fact that Analytic Learning Models »Reinforcement Learning  in ActionThe possibilities of Reinforcement Learning captured significant attention with the publication of a study in the journal Nature in which a computer agent learned to play 49 different video games with accuracy rivaling a professional game tester [14]. The agent was able to achieve these results using only the raw screen pixels and game score as input. This approach represents the first artificial intelligence agent that is capable of learning complex tasks while bridging between high-dimensional sensory inputs and actions.UnsupervisedSemi-SupervisedSupervisedOfﬂineReinforcementOnlineLEARNING STYLETRAINING STYLE57Take off the Training Wheels\\n\\nthese models do not have the entire dataset available when being trained is a challenge. They must make assumptions about the data based on the examples observed; these assumptions may be sub-optimal. The impact of sub-optimal predictions can be mitigated in cases where feedback on the model’s predictions is available. Online models can rapidly incorporate feedback to improve performance.One such training style is known as Reinforcement Learning. Under this approach, an algorithm takes action in an environment and incrementally learns how to achieve goals based on the response to a function used to determine the quality of its results. Reinforcement learning is generally applicable to complex, real-world tasks that involve optimization, such as navigation or trading. Due to the publication of many promising results from Reinforcement Learning algorithms, the popularity of this technique has risen dramatically in recent years along with Deep Learning. Execution ModelsExecution models describe how data is manipulated to perform an analytic function. They may be categorized across a number of dimensions. Execution Models are embodied by an execution framework, which orchestrates the sequencing of analytic computation. In this sense, a framework might be as simple as a programming language runtime, such as the Python interpreter, or a distributed computing framework that provides a specific API for one or more programming languages such as Hadoop, MapReduce or Spark. Grouping execution models based on how they handle data is common, classifying them as either batch or streaming execution models. The categories of execution model are shown in the figure, Analytic Execution Models.Analytic Execution ModelsA batch execution model implies that data is analyzed in large segments, that the analytic has a state where it is running and a state where it is not running and that little state is maintained in memory between executions. Batch execution may also imply that the analytic produces results with a frequency on the order of several minutes or more. Batch workloads tend to be fairly easy to conceptualize because SEQUENCINGSerialParallelBatchStreamingSCHEDULING Source: Booz Allen Hamilton THE FIELD GUIDE to  DATA SCIENCE\\n\\nthey represent discrete units of work. As such, it is easy to identify a specific series of execution steps as well as the proper execution frequency and time bounds based on the rate at which data arrives. Depending on the algorithm choice, batch execution models are  easily scalable through parallelism. There are a number of frameworks that support parallel batch analytic execution. Most famously,  Hadoop provides a distributed batch execution model in its MapReduce framework.Conversely, a streaming model analyzes data as it arrives. Streaming execution models imply that under normal operation, the analytic is always executing. The analytic can hold state in memory and constantly deliver results as new data arrives, on the order of seconds or less. Many of the concepts in streaming are inherent in the Unix-pipeline design philosophy; processes are chained together by linking the output of one process to the input of the next. As a result, many developers are already familiar with the basic concepts of streaming. A number of frameworks are available that support the parallel execution of streaming analytics such as Storm, S4 and Samza. The choice between batch and streaming execution models often hinges on analytic latency and timeliness requirements. Latency refers to the amount of time required to analyze a piece of data once it arrives at the system, while timeliness refers to the average age of an answer or result generated by the analytic system. For many analytic goals, a latency of hours and timeliness of days is acceptable and thus lend themselves to the implementation enabled by the batch approach. Some analytic goals have up-to-the-second requirements where a result that is minutes old has little worth. The streaming execution model better supports such goals.Batch and streaming execution models are not the only dimensions within which to categorize analytic execution methods. Another distinction is drawn when thinking about scalability. In many cases, scale can be achieved by spreading computation over a number of computers. In this context, certain algorithms require a large shared memory state, while others are easily parallelizable in a context where no shared state exists between machines. This distinction has significant impacts on both software and hardware selection when building out a parallel analytic execution environment. » Tips From the ProsIn order to understand system capacity in the context of streaming analytic execution, collect metrics including: the amount of data consumed, data emitted, and latency. This will help  you understand when scale limits  are reached.59Take off the Training Wheels\\n\\nFractal Analytic Model\\n\\nData Science analytics are a lot like broccoli.\\n\\nFractals are mathematical sets that display self-similar patterns. As you zoom in on a fractal, the same patterns reappear. Imagine a stalk of broccoli. Rip off a piece of broccoli and the piece looks much like the original stalk. Progressively smaller pieces of broccoli still look like the original stalk.\\n\\nData Science analytics are a lot like broccoli – fractal in nature in both time and construction. Early versions of an analytic follow the same development process as later versions. At any given iteration, the analytic itself is a collection of smaller analytics that often decompose into yet smaller analytics.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nSetupTryEvaluateDoEvaluateIterative by NatureGood Data Science is fractal in time  — an iterative process. Getting an imperfect solution out the door quickly will gain more interest from stakeholders than a perfect solution that is never completed. The figure, The Data Science Product Lifecycle, summarizes the lifecycle of the Data Science product. Set up the infrastructure, aggregate and prepare the data, and incorporate domain expert knowledge. Try different analytic techniques and models on subsets of the data. Evaluate the models, refine, evaluate again, and select a model. Do something with your models and results – deploy the models to inform, inspire action, and act. Evaluate the business results to improve the overall product.The Data Science Product Lifecycle  Source: Booz Allen Hamilton 61Take off the Training Wheels\\n\\nSmaller Pieces of Broccoli: A Data  Science ProductComponents inside and outside of the Data Science product will change with each iteration. Let’s take a look under the hood of a  Data Science product and examine the components during one  such iteration.In order to achieve a greater analytic goal, you need to first decompose the problem into sub-components to divide and conquer. The figure, The Fractal Analytic Model, shows a decomposition of the Data Science product into four component pieces.GOAL›  Describe›  Discover›  Predict›  AdviseACTION›  Productization›  Data Monetization›  Insights & RelationshipsDATA›  Text›  Imagery›  Waveform›  Geo›  Time SeriesCOMPUTATIONAggregationEnrichmentClusteringClassificationCLASSES OF ANALYTICSThe Fractal Analytic Model Source: Booz Allen Hamilton THE FIELD GUIDE to  DATA SCIENCE\\n\\nGOAL\\n\\nYou must first have some idea of your analytic goal and the end state of the analysis. Is it to Discover, Describe, Predict, or Advise? It is probably a combination of several of those. Be sure that before you start, you define the business value of the data and how you plan to use the insights to drive decisions, or risk ending up with interesting but non-actionable trivia.\\n\\nDATA\\n\\nData dictates the potential insights that analytics can provide. Data Science is about finding patterns in variable data and comparing those patterns. If the data is not representative of the universe of events you wish to analyze, you will want to collect that data through carefully planned variations in events or processes through A/B testing or design of experiments. Datasets are never perfect so don’t wait for perfect data to get started. A good Data Scientist is adept at handling messy data with missing or erroneous values. Just make sure to spend the time upfront to clean the data or risk generating garbage results.\\n\\nCOMPUTATION\\n\\nComputation aligns the data to goals through the process of creating insights. Through divide and conquer, computation decomposes into several smaller analytic capabilities with their own goals, data, computation and resulting actions, just like a smaller piece of broccoli maintains the structure of the original stalk. In this way, computation itself is fractal. Capability building blocks may utilize different types of execution models such as batch computation or streaming, that individually accomplish small tasks. When properly combined together, the small tasks produce complex, actionable results.\\n\\nACTION\\n\\nHow should engineers change the manufacturing process to generate higher product yield? How should an insurance company choose which policies to offer to whom and at what price? The output of computation should enable actions that align to the goals of the data product. Results that do not support or inspire action are nothing but interesting trivia.\\n\\nGiven the fractal nature of Data Science analytics in time and construction, there are many opportunities to choose fantastic or shoddy analytic building blocks. The Analytic Selection Process offers some guidance.\\n\\nTake off the Training Wheels\\n\\n63\\n\\nThe Analytic\\n\\nSelection Process\\n\\nIf you focus only on the science aspect of Data Science you will never become a data artist.\\n\\nA critical step in Data Science is to identify an analytic technique that will produce the desired action. Sometimes it is clear; a characteristic of the problem (e.g., data type) points to the technique you should implement. Other times, however, it can be difficult to know where to begin. The universe of possible analytic techniques is large. Finding your way through this universe is an art that must be practiced. We are going to guide you on the next portion of your journey - becoming a data artist.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nDecomposing the ProblemDecomposing the problem into manageable pieces is the first step in the analytic selection process. Achieving a desired analytic action often requires combining multiple analytic techniques into a holistic, end-to-end solution. Engineering the complete solution requires that the problem be decomposed into progressively smaller sub-problems. The Fractal Analytic Model embodies this approach. At any given stage, the analytic itself is a collection of smaller computations that decompose into yet smaller computations. When the problem is decomposed far enough, only a single analytic technique is needed to achieve the analytic goal. Problem decomposition creates multiple sub-problems, each with their own goals, data, computations, and actions. The concept behind problem decomposition is shown in the figure, Problem Decomposition Using the Fractal Analytic Model.GOAL›  Describe›  Discover›  Predict›  AdviseACTION›  Productization›  Data Monetization›  Insights & RelationshipsDATA› Text›  Imagery›  Waveform›  Geo›  Time SeriesDATAGOALACTIONDATAGOALACTIONAggregationEnrichmentClusteringClassificationCLASSES OF ANALYTICSProblem Decomposition Using the Fractal Analytic Model Source: Booz Allen Hamilton 65Take off the Training Wheels\\n\\nOn the surface, problem decomposition appears to be a mechanical, repeatable process. While this may be true conceptually, it is really the performance of an art as opposed to the solving of an engineering problem. There may be many valid ways to decompose the problem, each leading to a different solution. There may be hidden dependencies or constraints that only emerge after you begin developing a solution. This is where art meets science. Although the art behind problem decomposition cannot be taught, we have distilled some helpful hints to help guide you. When you begin to think about decomposing your problem, look for: ›Compound analytic goals that create natural segmentation. For example, many problems focused on predicting future conditions include both Discover and Predict goals. ›Natural orderings of analytic goals. For example, when extracting features you must first identify candidate features and then select the features set with the highest information value. These two activities form distinct analytic goals.   ›Data types that dictate processing activities. For example, text or imagery both require feature extraction. ›Requirements for human-in-the-loop feedback. For example, when developing alerting thresholds, you might need to solicit analyst feedback and update the threshold based on their assessment. ›The need to combine multiple data sources. For example, you may need to correlate two datasets to achieve your broader goal. Often this indicates the presence of a Discover goal. In addition to problem decomposition providing a tractable approach to analytic selection, it has the added benefit of simplifying a highly complex problem. Rather than being faced with understanding the entire end-to-end solution, the computations are discrete segments that can be explored. Note, however, that while this technique helps the Data Scientist approach the problem, it is the complete end-to-end solution that must be evaluated.  » Tips From the ProsOne of your ﬁrst steps should be to explore available data sources that have not been previously combined. Emerging relationships between data sources often allow you to pick low hanging fruit.THE FIELD GUIDE to  DATA SCIENCE\\n\\nCompare DatasetsList of recently registered company domainsList of candidate spoofed domainsList of recently registered company domainsDiscover spoofed domainsTest &EvaluationTest &EvaluationTest &EvaluationCalculateMetricSetThresholdDataCollectionSimulateSpoofedDataStoreGeneratedDomainsGenerateCandidateDomainsAlert on spoofed domains to provide opportuinity to minimize brand image and consumer conﬁdence damageDiscover likely candidates for spoofed domainsList of candidate spoofed domainsList of recently registered company domainsDescribe closeness of spoof to valid domainsQuantitative measure of feature information valueThreshold that balances false positive and false negative rateQuantitative threshold for automated result ranking› › List of candidate spoofed domainsList of recently registered company domainsQuantitative measure of feature information value› › › Stephanie RiveraIdentifying Spoofed DomainsIdentifying spoofed domains is important for an organization to preserve their brand image and to avoid eroded customer conﬁdence. Spoofed domains occur when a malicious actor  creates a website, URL or email address that users believe is associated with a valid organization. When users click the link,  visit the website or receive emails, they are subjected to some  type of nefarious activity.Our team was faced with the problem of identifying spoofed domains for a commercial company. On the surface, the problem sounded easy; take a recently registered domain, check to see if it is similar to the company’s domain and alert when the similarity is sufﬁciently high. Upon decomposing the problem, however, the main computation quickly became complicated.We needed a computation that determined similarity between two domains. As we decomposed the similarity computation, complexity and speed became a concern.  As with many security-related problems, fast alert speeds are vital. Result speed created an implementation constraint that forced us to re-evaluate how we decomposed the problem.Revisiting the decomposition process led us to a completely new approach. In the end, we derived a list of domains similar to those registered by the company. We then compared that list against a list of recently registered domains. The ﬁgure, Spoofed Domain Problem Decomposition, illustrates our approach. Upon testing and initial deployment, our analytic discovered a spoofed domain within 48 hours.  ››Spoofed Domain Problem Decomposition Source: Booz Allen Hamilton 67Take off the Training Wheels\\n\\nSPEED: The speed at which an analytic outcome must be produced (e.g., near real-time, hourly, daily) or the time it takes to develop and implement the analytic solutionANALYTIC COMPLEXITY: Algorithmic complexity (e.g., complexity class and execution resources) ACCURACY & PRECISION: The ability to produce exact versus approximate solutions as well as the ability to provide a measure of conﬁdenceDATA SIZE: The size of the dataset (e.g., number of rows)DATA COMPLEXITY: The data type, formal complexity measures including measures of overlap and linear separability, number of dimensions /columns, and linkages between datasetsSPEEDANALYTICCOMPLEXITYDATACOMPLEXITYACCURACY&PRECISIONDATA SIZEImplementation ConstraintsIn the spoofed domains case study, the emergence of an implementation constraint caused the team to revisit its approach. This demonstrates that analytic selection does not simply mean choosing an analytic technique to achieve a desired outcome. It also means ensuring that the solution is feasible to implement. The Data Scientist may encounter a wide variety of implementation constraints. They can be conceptualized, however, in the context of five dimensions that compete for your attention: analytic complexity, speed, accuracy & precision, data size, and data complexity. Balancing these dimensions is a zero sum game - an analytic solution cannot simultaneously exhibit all five dimensions, but instead must make trades between them. The figure, Balancing the Five Analytic Dimensions, illustrates this relationship.  Implementation constraints occur when an aspect of the problem dictates the value for one or more of these dimensions. As soon as  one dimension is fixed, the Data Scientist is forced to make trades among the others. For example, if the analytic problem requires actions to be produced in near real-time, the speed dimension is  fixed and trades must be made among the other four dimensions. Understanding which trades will achieve the right balance among  the five dimensions is an art that must be learned over time.As we compiled this section, we talked extensively about ways to group and classify implementation constraints. After much discussion we settled on these five dimensions. We present this model in hopes  that others weigh in and offer  their own perspectives.Balancing the Five Analytic Dimensions Source: Booz Allen Hamilton 69Take off the Training Wheels\\n\\nSome common examples of implementation  constraints include:• Computation frequency. The solution may need to run on a regular basis (e.g., hourly), requiring that computations be completed within a specified window of time. The best analytic is useless if it cannot solve the problem within the required time. • Solution timeliness. Some applications require near real-time results, pointing toward streaming approaches. While some algorithms can be implemented within streaming frameworks, many others cannot.  • Implementation speed. A project may require that you rapidly develop and implement a solution to quickly produce analytic insights. In these cases, you may need to focus on less complex techniques that can be quickly implemented and verified.• Computational resource limitations. Although you may be able to store and analyze your data, data size may be sufficiently large that algorithms requiring multiple computations across the full dataset are too resource intensive. This may point toward needing approaches that only require a single pass on the data (e.g., canopy cluster as opposed to k-means clustering).  • Data storage limitations. There are times when big data becomes  so big it cannot be stored or only a short time horizon can be stored. Analytic approaches that require long time horizons may not be possible. Organizational policies and regulatory requirements are a major source of implicit constraints that merit a brief discussion. Policies are often established around specific classes of data such as Personally Identifiable Information (PII) or Personal Health Information (PHI). While the technologies available today can safely house information with a variety of security controls in a single system, these policies force special data handling considerations including limited retention periods and data access. Data restrictions impact the data size and complexity dimensions outlined earlier, creating yet another layer of constraints that must be considered.  » Tips From the ProsWhen possible, consider approaches that make use of previously computed results. Your algorithm will run  much faster if you can avoid  re-computing values across  the full time horizon of data. » Tips From the ProsOur Data Science Product Lifecycle  has evolved to produce results quickly and then incrementally improve  the solution. » Tips From the ProsStreaming approaches may be useful for overcoming storage limitations.THE FIELD GUIDE to  DATA SCIENCE\\n\\n4 Data Science3DESCRIBE  DISCOVER PREDICT ADVISE 21Guide to Analytic SelectionYour senses are incapable of perceiving the entire universe, so we drew you a map.The universe of analytic techniques is vast and hard to comprehend. We created this diagram to aid you in finding your way from data  and goal to analytic action. Begin at the center of the universe  (Data Science) and answer questions about your analytic goals and problem characteristics. The answers to your questions will guide you through the diagram to the appropriate class of analytic techniques and provide recommendations for a few techniques to consider.(cid:110)  TIP: There are several situations where dimensionality reduction may be needed: ›Models fail to converge ›Models produce results equivalent to  random chance ›You lack the computational power to perform operations across the  feature space ›You do not know which aspects of the data are › the most important(cid:111)  Feature Extraction is a broad topic and is highly dependent upon the domain area.  This topic could be the subject of an entire book. As a result, a detailed exploration  has been omitted from this diagram. See the Featuring Engineering and Feature Selection sections in the Life in the Trenches chapter for additional information.(cid:112)  TIP: Always check data labels for correctness. This is particularly true for time stamps, which may have reverted to system default values.(cid:113)  TIP: Smart enrichment can greatly speed-up computational time. It can also be a huge differentiator between the accuracy of different end-to-end analytic solutions. Source: Booz Allen Hamilton 71Take off the Training Wheels\\n\\nData ScienceFILTERING How do I identify data based on its absolute or relative values?IMPUTATION How do I ﬁll in missing values in my data?DIMENSIONALITY REDUCTION  (cid:110)  How do I reduce the number of dimensions in my data?NORMALIZATION & TRANSFORMATION How do I reconcile duplication representations in the data?FEATURE EXTRACTION (cid:111)  PROCESSING (cid:112)  How do I clean and separate my data?If you want to add or remove data based on its value, start with:› Relational algebra projection and selection If early results are uninformative and duplicative, start with:› Outlier removal  › Gaussian ﬁlter› Exponential smoothing › Median ﬁlter If you are unfamiliar with the dataset, start with basic statistics: › Count › Standard deviation › Box plots› Mean › Range  › Scatter plots If your approach assumes the data follows a distribution, start with:› Distribution ﬁttingIf you want to understand all the information available on an entity, start with:› “Baseball card” aggregation    If you need to keep track of source information or other user-deﬁned parameters, start with:› AnnotationIf you often process certain data ﬁelds together or use one ﬁeld to compute the value of another, start with:› Relational algebra rename,  › Feature addition (e.g., Geography, Technology, Weather)If you want to generate values from other observations in your dataset, start with:› Random sampling› Markov Chain Monte Carlo (MC)If you want to generate values without using other observations in your dataset, start with:› Mean   › Regression models› Statistical distributionsIf you need to determine whether there is multi-dimensional correlation, start with:› PCA and other factor analysisIf you can represent individual observations by membership in a group, start with:› K-means clustering› Canopy clusteringIf you have unstructured text data, start with:› Term Frequency/Inverse Document Frequency (TF IDF) If you have a variable number of features but your algorithm requires a ﬁxed number, start with:› Feature hashingIf you are not sure which features are the most important, start with: › Wrapper methods› Sensitivity analysisIf you need to facilitate understanding of the probability distribution of the space, start with: › Self organizing mapsIf you suspect duplicate data elements, start with:› DeduplicationIf you want your data to fall within a speciﬁed range, start with:› Normalization  If your data is stored in a binary format, start with:› Format Conversion If you are operating in frequency space, start with:› Fast Fourier Transform (FFT),› Discrete wavelet transform If you are operating in Euclidian space, start with:› Coordinate transformDESCRIBE  How do I develop an understanding of the content of my data?ENRICHMENT (cid:113) How do I add new information to my data?AGGREGATION How do I collect and summarize my data?1243PREDICTDISCOVERADVISE  Source: Booz Allen Hamilton THE FIELD GUIDE to  DATA SCIENCE\\n\\nIf you are unfamiliar with the dataset, start with If you want to generate values from other observations in your dataset, start with:If you want to generate values without using other observations in your dataset, If you need to determine whether there is multi-dimensional correlation, If you can represent individual observations by membership in a group, start with:CLASSIFICATIONHow do I predict group membership?If you want an ordered set of clusters with variable precision, start with:  › Hierarchical(cid:114) If you have a known number of clusters, start with:    › X-means› Canopy› AprioriIf you have text data, start with:   › Topic modelingIf you have non-elliptical clusters, start with:   › Fractal› DB ScanIf you want soft membership in the clusters, start with:   › Gaussian mixture models(cid:115) If you have an known number of clusters, start with:   › K-meansIf your data has unknown  structure, start with:   › Tree-based methodsIf statistical measures of importance are needed, start with:   › Generalized linear modelsIf statistical measures of importance are not needed, start with:   › Regression with shrinkage (e.g., LASSO, Elastic net)› Stepwise regressionIf you want to compare two groups› T-testIf you want to compare multiple groups› ANOVA2DISCOVER What are the key relationships in the data?REGRESSION How do I determine which variables may be important?HYPOTHESIS TESTINGHow do I test ideas?CLUSTERING How do I segment the data to ﬁnd natural groupings?DESCRIBE Data Science3PREDICTADVISE14(cid:114)  TIP:  Canopy clustering is good when you only want to make a single pass over the data.(cid:115)   TIP:  Use canopy or hierarchical clustering to estimate the number of clusters you should generate. Source: Booz Allen Hamilton\\n\\nIf you have known dependent relationships between variables› Bayesian networkIf you are unsure of feature importance, start with:  › Neural nets,› Random forests› Deep learningIf you require a highly transparent model, start with:   › Decision treesIf you have <20 data dimensions, start with:   › K-nearest neighborsIf you have a large dataset with an unknown classiﬁcation signal, start with:   › Naive bayesIf you want to estimate an unobservable state based on observable variables, start with:    › Hidden markov model(cid:117) If you don\\'t know where else to begin, start with:   › Support vector machines (SVM)› Random forestsIf the data structure is unknown, start with:   › Tree-based methodsIf you require a highly transparent model, start with:   › Generalized linear modelsIf you have <20 data dimensions, start with:   › K-nearest neighborsPREDICT What are the likely future outcomes?REGRESSION How do I predict a future value?If you only have knowledge of how people interact with items, start with:    › Collaborative ﬁlteringIf you have a feature vector of item characteristics, start with:    › Content-based methodsIf you only have knowledge of how items are connected to one another, start with:    › Graph-based methodsRECOMMENDATION (cid:118)(cid:119)  How do I predict relevant conditions?CLASSIFICATION (cid:116) How do I predict group membership?32DISCOVER1DESCRIBE Data Science4ADVISEIf you want an ordered set of clusters with variable precision, start with:  If your data has unknown  structure, start with:   If statistical measures of importance are If statistical measures of importance are not › Regression with shrinkage (e.g., LASSO, Elastic net)(cid:116)  TIP: It can be difﬁcult to predict which classiﬁer will work best on your dataset.  Always try multiple classiﬁers. Pick the one or two that work the best to reﬁne and explore further.(cid:117) TIP: These are our favorite, go-to classiﬁcation algorithms. (cid:118)  TIP: Be careful of the “recommendation bubble”, the tendency of recommenders to recommend only what has been seen in the past.  You must ensure you add diversity to avoid this phenomenon. (cid:119)  TIP: SVD and PCA are good tools for creating better features for recommenders. Source: Booz Allen Hamilton\\n\\nIf the data structure is unknown, transparent model, start with:  If you have <20 data dimensions, If you only have knowledge of how people interact with items, If you have a feature vector of item characteristics, start with:   If you only have knowledge of how items are connected to one ADVISE What course of action should I take?If you have expert knowledge to capture › Expert systemsIf you\\'re looking for basic facts › Logical reasoningLOGICAL REASONINGHow do I sort through different evidence?If you must model discrete entities, start with:    › Discrete event simulation (DES)If there are a discrete set of possible states, start with:    › Markov modelsIf there are actions and interactions among autonomous entities, start with:    › Agent-based simulationIf you do not need to model discrete entities, start with:    › Monte Carlo simulationIf you are modeling a complex system with feedback mechanisms between actions, start with:    › Systems dynamicsIf you require continuous tracking of system behavior, start with:    › Activity-based simulationIf you already have an understanding of what factors govern the system, start with:    › ODES › PDESIf you have imprecise categories › Fuzzy logicSIMULATIONHow do I characterizea system that does nothave a closed-formrepresentation?If your problem is represented by a non-deterministic utility function, start with:    › Stochastic searchIf approximate solutions are acceptable, start with:    › Genetic algorithms › Simulated annealing › Gradient searchIf your problem is represented by a deterministic utility function, start with:    › Linear programming › Integer programming › Non-linear programmingIf you have limited resources to search with › Active learningIf you want to try multiple models › Ensemble learningOPTIMIZATION How do I identify the best course of action when my objective can be expressed as a utility function?42DISCOVERDESCRIBE Data Science3PREDICT1 Source: Booz Allen Hamilton\\n\\nCompiled by: Booz Allen HamiltonTechniqueDescriptionTips From the ProsReferences we  love to readActive LearningIntelligent sample selection to improve performance of model.  Samples are selected to provide the greatest information to a learning model.Can be paired with a human in-the-loop to help capture domain knowledge.Burr, Settles B. “Active Learning: Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning.” Morgan & Claypool, 2012. Print.Agent Based SimulationSimulates the actions and interactions of autonomous agents.In many systems, complex behavior results from surprisingly simple rules. Keep the logic of your agents simple and gradually build in sophistication.Macal, Charles, and Michael North. “Agent-based Modeling and Simulation.” Winter Simulation Conference. Austin, TX. 2009. Conference Presentation. ANOVAHypothesis testing for differences between more than two groups.Check model assumptions before utilizing, and watch out for Family Wise error when running multiple tests.Bhattacharyya, Gouri K., and Richard A. Johnson. Statistical Concepts and Models. Wiley, 1977. Print.Association Rule Mining (Apriori)Data mining technique to identify the common co-occurances of items.Utilize when you have a need to understand potential relationships between elements.Agrawal, Rakesh, and Ramakrishnan Srikant. “Fast Algorithms for Mining Association Rules.” Proc. Of 20th Intl. Conf. on VLDB. 1994. Conference Presentation.Bayesian NetworkModels conditional probabilities amongst elements, visualized as a Directed Acyclic Graph.Calculate by hand before using larger models to ensure understanding.Russel, Stuart, and Peter Norvig. “Artiﬁcial Intelligence: A Modern Approach.”\\xa0Prentice Hall, 2009 Print.\\xa0Detailed Table of AnalyticsGetting you to the right starting point isn’t enough. We also provide a translator so you understand what you’ve been told.Identifying several analytic techniques that can be applied to your problem is useful, but their name alone will not be much help. The Detailed Table of Analytics translates the names into something more meaningful. Once you’ve identified a technique in the Guide to Analytic Selection, find the corresponding row in the table. There you will find a brief description of the techniques, tips we’ve learned and a few references we’ve found helpful. THE FIELD GUIDE to  DATA SCIENCE\\n\\nTechnique\\n\\nDescription\\n\\nTips From the Pros\\n\\nReferences we\\n\\nlove to read\\n\\nCollaborative\\n\\nFiltering\\n\\nAlso known as \\'Recommendation,\\' suggest or eliminate items from a set by comparing a history of actions against items performed by users. Finds similar items based on who used them or similar users based on the items they use.\\n\\nCoordinate\\n\\nTransforma\\n\\n\\n\\ntion\\n\\nProvides a different\\n\\nperspective on data.\\n\\nDeep Learning\\n\\nMethod that learns\\n\\nfeatures that leads to\\n\\nhigher concept learning.\\n\\nUsually very deep neural\\n\\nnetwork architectures.\\n\\nUse Singular Value Decomposition based Recommendation for cases where there are latent factors in your domain, e.g., genres in movies.\\n\\nOwen, Sean, Robin Anil, Ted Dunning, and Ellen Friedman. Mahout in Action. New Jersey: Manning, 2012. Print.\\n\\nChanging the coordinate system for data, for example, using polar or cylindrical coordinates, may more readily highlight key structure in the data. A key step in coordinate transformations is to appreciate multidimensionality and to systematically analyze subspaces of the data.\\n\\nAbbott, Edwin A., Flatland: A Romance of Many Dimensions. United Kingdom: Seely & Co., 1884. Print.\\n\\nUtilize a GPU to efﬁciently train complex models.\\n\\nBengio, Yoshua, and Yann LeCun. “Scaling Learning Algorithms towards AI.” Large- Scale Kernel Machines. New York: MIT Press, 2007. Print.\\n\\nMontgomery, Douglas. Design\\n\\nand Analysis of Experiments.\\n\\nNew Jersey: John Wiley\\n\\n& Sons, 2012. Print.\\n\\nDesign of\\n\\nExperiments\\n\\nApplies controlled experiments to quantify effects on system output caused by changes to inputs.\\n\\nFractional factorial designs can signiﬁcantly reduce the number of different types of experiments you must conduct.\\n\\nDifferential\\n\\nEquations\\n\\nUsed to express\\n\\nrelationships between\\n\\nfunctions and their\\n\\nderivatives, for example,\\n\\nchange over time.\\n\\nDifferential equations can be used to formalize models and make predictions. The equations themselves can be solved numerically and tested with different initial conditions to study system trajectories.\\n\\nZill, Dennis, Warren Wright,\\n\\nand Michael Cullen. Differential\\n\\nEquations with Boundary\\n\\n\\n\\nValue\\n\\nProblems. Connecticut: Cengage\\n\\nLearning, 2012. Print.\\n\\nDiscrete Event\\n\\nSimulation\\n\\nSimulates a discrete sequence of events where each event occurs at a particular instant in time. The model updates its state only at points in time when events occur.\\n\\nDiscrete event simulation is useful when analyzing event based processes such as production lines and service centers to determine how system level behavior changes as different process parameters change. Optimization can integrate with simulation to gain efﬁciencies in a process.\\n\\nBurrus, C. Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.\\n\\nDiscrete\\n\\nWavelet\\n\\nTransform\\n\\nTransforms time series\\n\\ndata into frequency\\n\\ndomain preserving\\n\\nlocality information.\\n\\nOffers very good time and frequency localization. The advantage over Fourier transforms is that it preserves both frequency and locality.\\n\\nEnsemble\\n\\nLearning\\n\\nLearning multiple models\\n\\nand combining output to\\n\\nachieve better performance.\\n\\nBe careful not to overﬁt data by having too many model parameters and  overtraining.\\n\\nExpert\\n\\nSystems\\n\\nSystems that use symbolic logic to reason about facts. Emulates human reasoning.\\n\\nUseful to have a human readable explanation of why a system came to a conclusion.\\n\\nExponential\\n\\nSmoothing\\n\\nUsed to remove artifacts\\n\\nexpected from collection\\n\\nerror or outliers.\\n\\nIn comparison to a using moving average where past observations are weighted equally, exponential smoothing assigns exponentially decreasing weights over time.\\n\\nBurrus, C.Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard, and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.\\n\\nDietterich, Thomas G. “Ensemble\\n\\nMethods in Machine Learning.”\\n\\nLecture Notes in Computer\\n\\nScience. Springer, 2000. Print.\\n\\nShortliffe, Edward H., and Bruce G. Buchanan. “A Model of Inexact Reasoning in Medicine.” Mathematical Biosciences. Elsevier B.V., 1975. Print.\\n\\nChatﬁeld, Chris, Anne B. Koehler, J. Keith Ord, and Ralph D. Snyder. “A New Look at Models for Exponential Smoothing.” Journal of the Royal Statistical Society: Series D (The Statistician). Royal Statistical Society, 2001. Print.\\n\\nCompiled by: Booz Allen Hamilton\\n\\nTake off the Training Wheels\\n\\n79\\n\\nTechnique\\n\\nDescription\\n\\nTips From the Pros\\n\\nReferences we\\n\\nlove to read\\n\\nFactor\\n\\nAnalysis\\n\\nFast Fourier\\n\\nTransform\\n\\nFormat\\n\\nConversion\\n\\nDescribes variability among correlated variables with the goal of lowering the number of unobserved variables, namely, the factors.\\n\\nTransforms time series from time to frequency domain efﬁciently. Can also be used for image improvement by spatial transforms.\\n\\nCreates a standard representation of data regardless of source format. For example, extracting raw UTF-8 encoded text from binary ﬁle formats such as Microsoft Word or PDFs.\\n\\nIf you suspect there are inmeasurable inﬂuences on your data, then you may want to try factor analysis.\\n\\nChild, Dennis. The Essentials of Factor Analysis. United Kingdom: Cassell Educational, 1990. Print.\\n\\nFiltering a time varying signal can be done more effectively in the frequency domain. Also, noise can often be identiﬁed in such signals by observing power at aberrant frequencies.\\n\\nMitra, Partha P., and Hemant Bokil. Observed Brain Dynamics. United Kingdom: Oxford University Press, 2008. Print.\\n\\nThere are a number of open source software packages that support format conversion and can interpret a wide variety of formats. One notable package is Apache Tikia.\\n\\nIngersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.\\n\\nFuzzy Logic\\n\\nLogical reasoning that allows for degrees of truth for a statement.\\n\\nUtilize when categories are not clearly deﬁned. Concepts such as \"warm\", \"cold\", and \"hot\" can mean different things at different temperatures and domains.\\n\\nZadeh L.A., \"Fuzzy Sets.”\\n\\nInformation and Control.\\n\\nCalifornia: University of\\n\\nCalifornia, Berkeley, 1965. Print.\\n\\nGaussian\\n\\nFiltering\\n\\nActs to remove noise\\n\\nor blur data.\\n\\nCan be used to remove speckle noise from images.\\n\\nGeneralized\\n\\nLinear Models\\n\\nExpands ordinary linear\\n\\nregression to allow\\n\\nfor error distribution\\n\\nthat is not normal.\\n\\nUse if the observed error in your system does not follow the normal distribution.\\n\\nGenetic\\n\\nAlgorithms\\n\\nEvolves candidate\\n\\nmodels over generations\\n\\nby evolutionary\\n\\ninspired operators of\\n\\nmutation and crossover\\n\\nof parameters.\\n\\nIncreasing the generation size adds diversity in considering parameter combinations, but requires more objective function evaluation. Calculating individuals within a generation is strongly parallelizable. Representation of candidate solutions can impact performance.\\n\\nGrid Search\\n\\nSystematic search across\\n\\ndiscrete parameter\\n\\nvalues for parameter\\n\\nexploration problems.\\n\\nA grid across the parameters is used to visualize the parameter landscape and assess whether multiple minima are present.\\n\\nParker, James R. Algorithms for Image Processing and Computer Vision. New Jersey: John Wiley & Sons, 2010. Print.\\n\\nMacCullagh, P., and John A. Nelder. Generalized Linear Models. Florida: CRC Press, 1989. Print.\\n\\nDe Jong, Kenneth A. Evolutionary Computation - A Uniﬁed Approach. Massachusetts: MIT Press, 2002. Print.\\n\\nKolda, Tamara G., Robert M. Lewis, and Virginia Torczon. “Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods.” SIAM Review. Society for Industrial and Applied Mathematics, 2003. Print.\\n\\nBishop, Christopher M. Pattern\\n\\nRecognition and Machine\\n\\nLearning. New York: Springer,\\n\\n2006. Print.\\n\\nOne of the most powerful properties of Hidden Markov Models is their ability to exhibit some degree of invariance to local warping (compression and stretching) of the time axis. However, a signiﬁcant weakness of the Hidden Markov Model is the way in which it represents the distribution of times for which the system remains in a given state.\\n\\nProvides views of clusters at multiple resolutions of closeness. Algorithms begin to slow for larger datasets due to most implementations exhibiting O(N3) or O(N2) complexity.\\n\\nRui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.\\n\\nWhen applying clustering techniques, make sure to understand the shape of your data. Clustering techniques will return poor results if your data is not circular or ellipsoidal shaped.\\n\\nRui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.\\n\\nCompiled by: Booz Allen Hamilton\\n\\nHidden Markov\\n\\nModels\\n\\nModels sequential data by\\n\\ndetermining the discrete\\n\\nlatent variables, but\\n\\nthe observables may be\\n\\ncontinuous or discrete.\\n\\nHierarchical\\n\\nClustering\\n\\nK\\n\\n\\n\\nmeans\\n\\nand X\\n\\n\\n\\nmeans\\n\\nClustering\\n\\nConnectivity based\\n\\nclustering approach\\n\\nthat sequentially builds\\n\\nbigger (agglomerative)\\n\\nor smaller (divisive)\\n\\nclusters in the data.\\n\\nCentroid based clustering algorithms, where with K means the number of clusters is set and X means the number of clusters is unknown.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nTechnique\\n\\nDescription\\n\\nTips From the Pros\\n\\nReferences we\\n\\nlove to read\\n\\nLinear,\\n\\nNon\\n\\n\\n\\nlinear,\\n\\nand Integer\\n\\nProgramming\\n\\nSet of techniques for\\n\\nminimizing or maximizing a\\n\\nfunction over a constrained\\n\\nset of input parameters.\\n\\nStart with linear programs because algorithms for integer and non-linear variables can take much longer to run.\\n\\nWinston, Wayne L. Operations\\n\\nResearch: Applications and\\n\\nAlgorithms. Connecticut:\\n\\nCengage Learning, 2003. Print.\\n\\nMarkov Chain\\n\\nMonte Carlo\\n\\n(MCMC)\\n\\nA method of sampling typically used in Bayesian models to estimate the joint distribution of parameters given the data.\\n\\nProblems that are intractable using analytic approaches can become tractable using MCMC, when even considering high-dimensional problems. The tractability is a result of using statistics on the underlying distributions of interest, namely, sampling with Monte Carlo and considering the stochastic sequential process of Markov Chains.\\n\\nAndrieu, Christophe, Nando de Freitas, Amaud Doucet, and Michael I. Jordan. “An Introduction to MCMC for Machine Learning.” Machine Learning. Kluwer Academic Publishers, 2003. Print.\\n\\nMonte Carlo\\n\\nMethods\\n\\nSet of computational\\n\\ntechniques to generate\\n\\nrandom numbers.\\n\\nParticularly useful for numerical integration, solutions of differential equations, computing Bayesian posteriors, and high dimensional multivariate sampling.\\n\\nFishman, George S. Monte\\n\\nCarlo: Concepts, Algorithms, and\\n\\nApplications. New York: Springer,\\n\\n2003. Print.\\n\\nNaïve Bayes\\n\\nPredicts classes following Bayes Theorem that states the probability of an outcome given a set of features is based on the probability of features given an outcome.\\n\\nAssumes that all variables are independent, so it can have issues learning in the context of highly interdependent variables. The model can be learned on a single pass of data using simple counts and therefore is useful in determining whether exploitable patterns exist in large datasets with minimal development time.\\n\\nIngersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.\\n\\nNeural\\n\\nNetworks\\n\\nLearns salient features in\\n\\ndata by adjusting weights\\n\\nbetween nodes through a\\n\\nlearning rule.\\n\\nTraining a neural network takes substantially longer than evaluating new data with an already trained network. Sparser network connectivity can help to segment the input space and improve performance on classiﬁcation tasks.\\n\\nHaykin, Simon O. Neural\\n\\nNetworks and Learning\\n\\nMachines. New Jersey:\\n\\nPrentice Hall, 2008. Print.\\n\\nOutlier\\n\\nRemoval\\n\\nMethod for identifying and\\n\\nremoving noise or artifacts\\n\\nfrom data.\\n\\nBe cautious when removing outliers. Sometimes the most interesting behavior of a system is at times when there are aberrant data points.\\n\\nMaimon, Oded, and Lior Rockach. Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers. The Netherlands: Kluwer Academic Publishers, 2005. Print.\\n\\nPrincipal\\n\\nComponents\\n\\nAnalysis\\n\\nEnables dimensionality\\n\\nreduction by identifying\\n\\nhighly correlated\\n\\ndimensions.\\n\\nMany large datasets contain correlations between dimensions; therefore part of the dataset is redundant. When analyzing the resulting principal components, rank order them by variance as this is the highest information view of your data. Use skree plots to infer the optimal number of components.\\n\\nWallisch, Pascal, Michael E. Lusignan, Marc D. Benayoun, Tanya I. Baker, Adam Seth Dickey, and Nicholas G. Hatsopoulos. Matlab for Neuroscientists. New Jersey: Prentice Hall, 2009. Print.\\n\\nRandom\\n\\nSearch\\n\\nRandomly adjust\\n\\nparameters to ﬁnd a\\n\\nbetter solution than\\n\\ncurrently found.\\n\\nUse as a benchmark for how well a search algorithm is performing. Be careful to use a good random number generator and new seed.\\n\\nRegression\\n\\nwith Shrinkage\\n\\n(Lasso)\\n\\nA method of variable\\n\\nselection and prediction\\n\\ncombined into a possibly\\n\\nbiased linear model.\\n\\nThere are different methods to select the lambda parameter. A typical choice is cross validation with MSE as the metric.\\n\\nSensitivity\\n\\nAnalysis\\n\\nInvolves testing individual parameters in an analytic or model and observing the magnitude of the effect.\\n\\nInsensitive model parameters during an optimization are candidates for being set to constants. This reduces the dimensionality of optimization problems and provides an opportunity for speed up.\\n\\nBergstra J. and Bengio Y. Random Search for Hyper- Parameter Optimization, Journal of Machine Learning Research 13, 2012.\\n\\nTibshirani, Robert. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological). Toronto: Royal Statistical Society, 1996. Print.\\n\\nSaltelli, A., Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. Global Sensitivity Analysis: the Primer. New Jersey: John Wiley & Sons, 2008. Print.\\n\\nCompiled by: Booz Allen Hamilton\\n\\nTake off the Training Wheels\\n\\n81\\n\\nTechnique\\n\\nDescription\\n\\nTips From the Pros\\n\\nReferences we\\n\\nlove to read\\n\\nSimulated\\n\\nAnnealing\\n\\nStepwise\\n\\nRegression\\n\\nStochastic\\n\\nGradient\\n\\nDescent\\n\\nNamed after a controlled\\n\\ncooling process in\\n\\nmetallurgy, and by\\n\\nanalogy using a changing\\n\\ntemperature or annealing\\n\\nschedule to vary\\n\\nalgorithmic convergence.\\n\\nA method of variable selection and prediction. Akaike\\'s information criterion AIC is used as the metric for selection. The resulting predictive model is based upon ordinary least squares, or a general linear model with parameter estimation via maximum likelihood.\\n\\nGeneral\\n\\n\\n\\npurpose\\n\\noptimization for learning of\\n\\nneural networks, support\\n\\nvector machines, and\\n\\nlogistic regression models.\\n\\nThe standard annealing function allows for initial wide exploration of the parameter space followed by a narrower search. Depending on the search priority the annealing function can be modiﬁed to allow for longer explorative search at a high temperature.\\n\\nBertsimas, Dimitris, and John\\n\\nTsitsiklis. “Simulated Annealing.”\\n\\nStatistical Science. 1993. Print.\\n\\nCaution must be used when considering Stepwise Regression, as over ﬁtting often occurs. To mitigate over ﬁtting try to limit the number of free variables used.\\n\\nHocking, R.R. “The Analysis and Selection of Variables in Linear Regression.” Biometrics. 1976. Print.\\n\\nApplied in cases where the objective function is not completely differentiable when using sub-gradients.\\n\\nSupport Vector\\n\\nMachines\\n\\nProjection of feature vectors using a kernel function into a space where classes are more separable.\\n\\nTry multiple kernels and use k-fold cross validation to validate the choice of the best one.\\n\\nTerm\\n\\nFrequency\\n\\nInverse\\n\\nDocument\\n\\nFrequency\\n\\nTopic Modeling\\n\\n(Latent\\n\\nDirichlet\\n\\nAllocation)\\n\\nA statistic that measures the relative importance of a term from a corpus.\\n\\nTypically used in text mining. Assuming a corpus of news articles, a term that is very frequent such as “the” will likely appear many times in many documents, having a low value. A term that is infrequent such as a person’s last name that appears in a single article will have a higher TD IDF score.\\n\\nIngersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.\\n\\nIdentiﬁes latent topics\\n\\nin text by examining\\n\\nword co\\n\\n\\n\\noccurrence.\\n\\nEmploy part-of-speech tagging to eliminate words other than nouns and verbs. Use raw term counts instead of TF/IDF weighted terms.\\n\\nTree Based\\n\\nMethods\\n\\nModels structured as graph\\n\\ntrees where branches\\n\\nindicate decisions.\\n\\nCan be used to systematize a process or act as a classiﬁer.\\n\\nT\\n\\n\\n\\nTest\\n\\nHypothesis test used to\\n\\ntest for differences between\\n\\ntwo groups.\\n\\nMake sure you meet the tests assumptions and watch out for Family Wise error when running multiple tests.\\n\\nWrapper\\n\\nMethods\\n\\nFeature set reduction method that utilizes performance of a set of features on a model, as a measure of the feature set’s performance. Can help identify combinations of features in models that achieve high performance.\\n\\nUtilize k\\n\\n\\n\\nfold cross validation\\n\\nto control over ﬁtting.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\nWitten, Ian H., Eibe Frank, and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques. Massachusetts: Morgan Kaufmann, 2011. Print.\\n\\nHsu, Chih-Wei, Chih-Chung Chang, and Chih-Jen Lin. “A Practical Guide to Support Vector Classiﬁcation.” National Taiwan University Press, 2003. Print.\\n\\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet Allocation.” Journal of Machine Learning Research. 2003. Print.\\n\\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. “Tree Based Methods.” An Introduction to Statistical Learning. New York: Springer, 2013. Print.\\n\\nBhattacharyya, Gouri K., and\\n\\nRichard A. Johnson. Statistical\\n\\nConcepts and Models. Wiley,\\n\\n1977. Print.\\n\\nJohn, George H., Ron\\n\\nKohavi, and Karl Pﬂeger.\\n\\n“Irrelevant Features and the\\n\\nSubset Selection Problem.”\\n\\nProceedings of ICML\\n\\n\\n\\n94, 11th\\n\\nInternational Converence\\n\\non Machine Learning. New\\n\\nBrunswick, New Jersey. 1994.\\n\\nConference Presentation.\\n\\nCompiled by: Booz Allen Hamilton\\n\\nLI F E   in    T H E   T R E N C H E S\\n\\nNAVIGATING NECK DEEP IN DATA\\n\\nOur Data Science experts have learned and developed new solutions over the years from properly framing or reframing analytic questions. In this section, we list a few important topics to Data Science coupled with firsthand experience from our experts.\\n\\nGoing Deep into  Machine LearningMachines are getting better at learning by mimicking the human brain.Think about where you were 10 years ago. Could computers understand and take action based upon your spoken word? Recently, speech-to-text quality has improved dramatically to nearly perfect accuracy, much to the delight of many mobile phone users. In other complex tasks, similar magic capabilities have emerged. The world-record high scores in 29 video games are now held by a machine learning algorithm with no specific knowledge of Atari or computer games in general.These impressive feats were made possible by deep learning, a different way of approaching machine learning problems. Most approaches to machine learning require humans to encode logic and rules to create features, which are then fed into machine learning models. In some domains, such as audio, text, image, and signal processing, effective feature engineering requires considerable human expertise to achieve decent model performance. Deep learning avoids the necessity of human-encoded features and instead incorporates the feature engineering, feature selection, and model fitting into one step.Deep learning is an extension of an idea originating in the 1950s called neural networks, which are loosely inspired by our understanding of how neurons in our brains operate. Recent hardware developments, originally designed for faster renderings of graphics, birthed a renaissance in neural networks. The latest graphical processing units, or GPUs, have more than 3,000 processing cores that are well suited for parallel processing of complex matrix manipulations required for rendering graphics – and for executing computations on neural networks.In the late 2000s, the combination of GPUs, advances in algorithms, and collections of big data reignited interest in neural networks. GPUs enabled computers to process much larger networks in much less time, and clever advances in algorithms made the model fitting process more efficient. Large collections of image, video, and text data provided content for the deep learning algorithms to learn. The ability to train larger networks with more data drove the exploration of new neural network architectures featuring additional hidden layers and widening the breadth of the networks. Presently, deep learning has moved beyond academic applications and is finding its way into our daily lives. Deep learning powers speech-to-text on our mobile phones and smart devices, image search provided by major tech companies, language translation services for text and spoken word, and even drug discovery within advanced pharmaceutical companies. THE FIELD GUIDE to  DATA SCIENCE\\n\\n››Aaron SanderNational Data Science BowlThe ﬁrst-ever National Data Science Bowl offered Data Scientists a platform through which individuals could harness their passion, unleash their curiosity and amplify their impact to affect change on a global scale. The competition presented participants with more than 100,000 underwater images provided by the Hatﬁeld Marine Science Center. Participants were challenged to develop a classiﬁcation algorithm that would enable researchers to monitor ocean health at a speed and scale never before possible. More than 1,000 teams submitted a  total of approximately 15,000 solutions over the 90 days of the competition.  A large proportion of the participants’ implemented solutions used deep learning-based approaches, speciﬁcally Convolutional Neural Nets (CNNs). The competition forum exploded with competitors collectively sharing knowledge and collaborating to advance the state-of-the-art in computer vision. Participants tested new techniques for developing CNNs and contributed to the development of open source software for creating CNN models. The top three competitors, Team Deep Sea, Happy Lantern Festival, and Poisson Process, all used CNNs in their solutions. Their results increased algorithm accuracy by 10% over the state of the art. Without their algorithms, it would have taken marine researchers more than two lifetimes to manually complete the classiﬁcation process. The work submitted by all the participants represents major advances for both the marine research and Data Science communities. INPUT LAYERHIDDEN LAYEROUTPUTLAYERA Representation of Deep Learning Source: Booz Allen Hamilton  »Visit www.DataScienceBowl.com to learn more about the first-ever National Data Science Bowl87Life in the Trenches\\n\\nFeature EngineeringFeature engineering is a lot like oxygen. You can’t do without it, but you rarely give it much thought. Feature engineering is the process by which one establishes the representation of data in the context of an analytic approach. It is the foundational skill in Data Science. Without feature engineering, it would not be possible to understand and represent the world through a mathematical model. Feature engineering is a challenging art. Like other arts, it is a creative process that manifests uniquely in each Data Scientist. It will be influenced substantially by the scientist’s experiences, tastes and understanding of the field. When faced with the problem of feature engineering, there are several paths that one may initially take. Generally speaking, better features can be developed with more knowledge of the domain. One approach to feature engineering is to begin by describing smaller elements of the domain and continuously constructing more intricate features as the model becomes more complex. These more complicated features can be defined by considering other attributes of the domain, aggregating features together into different groups or using advanced statistical and mathematical techniques to devise new features. The ultimate judge in this process is the performance of the machine learning algorithm which makes decisions based on this feature vector.Consider the example of email spam classification. Because the domain is a set of emails, one possible choice of an initial feature vector is the integer array that counts the number of times a given word appears in the email. This is called the \"bag of words\" assumption, where the order that words appear in a text is ignored.  If an algorithm with this feature vector does not adequately distinguish between spam and non-spam emails, a feature could  be added that counts the number of misspelled words in the text.  This new feature uses the spam recognition domain knowledge that asserts many spam emails misspell words. These misspelled words  alert filters saying that the existence of certain words automatically label an email as spam. If this new feature is not enough, there are  still additional features to be considered such as whether or not a  first or last name is used in the email.Feature engineering represents a complex, but crucial aspect of Data Science. The Learning Optimal Features sidebar goes into detail about feature learning – an automated approach to feature engineering that applies machine learning techniques.  »What differentiates a great model from a bad one? In most cases, the inputs to the model matter even more than the choice of the algorithm. The traditional approach has a two-step process where heuristics and subject-matter expertise are used to ﬁnd a good set of features, and then algorithms optimize model parameters. Deep learning combines these steps into one. Feature engineering, feature selection, and model parameter estimation are accomplished simultaneously. This reduces the need for highly-specialized domain knowledge and often results in better models.For example, in the context of images from the natural world, deep networks may learn low-level features such as lines at various angles and curved lines. Middle layers may combine the lower-level features into more complex geometric shapes and patterns. Higher-level layers combine the mid-level features into more complicated features that begin to resemble faces and shapes of animals. Applied to other types of data, such as audio and text, deep neural networks learn increasingly sophisticated features at each layer in a similar manner.THE FIELD GUIDE to  DATA SCIENCE\\n\\n››Ed KohlweyChemoinformatic SearchOn one assignment, my team was confronted with the challenge of developing a search engine over chemical compounds. The goal of chemoinformatic search is to predict the properties that a molecule will exhibit as well as to provide indices over those predicted properties to facilitate data discovery in chemistry-based research. These properties may either be discreet (e.g., “a molecule treats disease x well”) or continuous (e.g., “a molecule may be dissolved up to 100.21 g/ml”).Molecules are complex 3D structures, which are typically represented as a list of atoms joined by chemical bonds of differing lengths with varying electron domain and molecular geometries. The structures are speciﬁed by the 3-space coordinates and the electrostatic potential surface of the atoms in the molecule. Searching this data is a daunting task when one considers that naïve approaches to the problem bear signiﬁcant semblance to the Graph Isomorphism Problem.[15]The solution we developed was based on previous work in molecular ﬁngerprinting (sometimes also called hashing or locality sensitive hashing). Fingerprinting is a dimensionality reduction technique that dramatically reduces the problem space by summarizing many features, often with relatively little regard to the importance of the feature. When an exact solution is likely to be infeasible, we often turn to heuristic approaches such as ﬁngerprinting.Our approach used a training set where all the measured properties of the molecules were available. We created a model of how molecular structural similarities might affect their properties. We began by ﬁnding all the sub-graphs of each molecule with length n, resulting in a representation similar to the bag-of-words approach from natural language processing. We summarized each molecule fragment in a type of ﬁngerprint called a “Counting Bloom Filter.”Next, we used several exemplars from the set to create new features. We found the distance from each member of the full training set to each of the exemplars. We fed these features into a non-linear regression algorithm to yield a model that could be used on data that was not in the original training set. This approach can be conceptualized as a “hidden manifold,” whereby a hidden surface or shape deﬁnes how a molecule will exhibit a property. We approximate this shape using a non-linear regression and a set of data with known properties. Once we have the approximate shape, we can use it to predict the properties of new molecules. Our approach was multi-staged and complex – we generated sub-graphs, created bloom ﬁlters, calculated distance metrics and ﬁt a linear-regression model. This example provides an illustration of how many stages may be involved in producing a sophisticated feature representation. By creatively combining and building “features on features,” we were able to create new representations of data that were richer and more descriptive, yet were able to execute faster and produce better results. 89Life in the Trenches\\n\\nFeature SelectionModels are like honored guests; you should only feed them the good parts.Feature selection is the process of determining the set of features with the highest information value to the model. Two main approaches are filtering and wrapper methods. Filtering methods analyze features using a test statistic and eliminate redundant or non-informative features. As an example, a filtering method could eliminate features that have little correlation to the class labels. Wrapper methods utilize a classification model as part of feature selection. A model is trained on a set of features and the classification accuracy is used to measure the information value of the feature set. One example is that of training a neural network with a set of features and evaluating the accuracy of the model. If the model scores highly on the test set, then the features have high information value. All possible combinations of features are tested to find the best feature set. There are tradeoffs between these techniques. Filtering methods are faster to compute since each feature only needs to be compared against its class label. Wrapper methods, on the other hand, evaluate feature sets by constructing models and measuring performance.  This requires a large number of models to be trained and evaluated  (a quantity that grows exponentially in the number of features).  Why would anyone use a wrapper method? Feature sets may perform better than individual features.[16] With filter methods, a feature with weak correlation to its class labels is eliminated. Some of these eliminated features, however, may have performed well when combined with other features.THE FIELD GUIDE to  DATA SCIENCE\\n\\n››Paul YacciCancer Cell ClassiﬁcationOn one project, our team was challenged to classify cancer cell proﬁles. The overarching goal was to classify different types of Leukemia, based on Microarray proﬁles from 72 samples[17] using a small set of features. We utilized a hybrid Artiﬁcial Neural Network (ANN)[18] and Genetic Algorithm[19] to identify subsets of 10 features selected from thousands.[20] We trained the ANN and tested performance using cross-fold validation. The performance measure was used as feedback into the Genetic Algorithm. When a set of features contained no useful information, the model performed poorly and a different feature set would be explored. Over time, this method selected a set of features that performed with high accuracy. The down-selected feature set increased speed and performance as well as allowed for better insight into the factors that may govern the system. This allowed our team to design a diagnostic test for only a few genetic markers instead of thousands, substantially reducing diagnostic test complexity and cost.91Life in the Trenches\\n\\nEnsemble ModelsNone of us is as smart as all of us, but some are smarter  than others.In 1906, Sir Francis Galton attended a fair at which there was a contest to guess the weight of an ox. Galton had the idea to collect the guesses of the 787 entrants and compute the mean. To his surprise, the mean was only one pound off from the ox’s real weight of 1,198 pounds. Together, the estimates made by many amateurs formed a prediction that was more accurate than that of individual experts. Galton’s “wisdom of crowds” extends to Data Science in the form of ensemble learning, which is colloquially and somewhat equivalently called ensembling, blending, or stacking. An ensemble takes the predictions of many individual models and combines them to make a single prediction. Like the people guessing the ox’s weight, Data Science models have unique strengths and weaknesses (i.e., determined by their design), and are influenced by varied perspectives based on past experience (i.e., the data they have observed). Ensembles overcome individual weaknesses to make predictions with more accuracy than their constituent models. These models need not stem from different methodologies; an ensemble might employ the same method with different parameters or weights (e.g., boosting), feature subsets (e.g., random forests), or sub-samples of data (e.g., bagging). The ensembling methodology may be as simple as averaging two outputs, or as complex as using a “meta model” to learn an optimal combination. An ensemble’s ability to reduce individual errors arises from the diversity of its members. If one model over fits the data, it is balanced by a different model that under fits the data. If one subset is skewed by outlier values, another subset is included without them. If one method is unstable to noisy inputs, it is bolstered by another method that is more robust. In practice, ensembling typically improves a model by a few percent. The price of this accuracy is paid in complexity. The accuracy vs. complexity tradeoff can make it difficult to know when ensembling is justified. On one hand, ensembles appear to be a fit for high-stakes problems—think detecting cancer in MRI images vs. detecting unripe blueberries on a conveyer belt. On the other hand, high-stakes problems mandate higher standards for auditing model functionality. The Data Scientist must manage a balance between ensemble interpretability and black-box complexity. If this seems easy, it isn’t! Put yourself in the driver’s seat of the machine learning code for a self-driving car. If a well-behaved regression model makes a right decision 99.5% of the time, but a complex, less-explainable ensemble is right 99.8% of the time, which would you pick? THE FIELD GUIDE to  DATA SCIENCE\\n\\n››Will CukierskiThe Value of  Ensemble ModelsSeveral years ago, the Kaggle Photo Quality Prediction competition posed the question “Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as \\'good\\'.” Participants were supplied a large collection of user-generated photos. The goal was to create an algorithm that could automatically pick out particularly enjoyable or impressive photos from the collection.Log likelihood (lower is better)Final Team Rank      Ensemble      Private LeaderboardRank=1Value of Ensembling for the Kaggle Photo Quality Prediction Competition  (results courtesy of Ben Hamner, Kaggle CTO)The-blue line shows the individual scores for each of the top 50 teams. The orange line shows the ensembled score for the top n teams, where n ranges from 1 to the value on the axis. For example, the ensemble point for Final Team Rank 5 is an ensemble of the entries for teams 1 through 5. As shown in the graph, the ensembled score is lower than any single individual score. The diversity of models included within the ensemble causes the respective errors to cancel out, resulting in an overall lower score. This holds true for all points across the top 50 teams. However, after we increase the number of models in the ensemble beyond 15, we begin to see the ensembled score increase. This occurs because we are introducing less accurate (i.e., potentially overﬁt) models into the ensemble. The results of this simple experiment quantify the value of creating an ensemble model, while reinforcing the idea that we must be thoughtful when selecting the individual models contained within the ensemble.Over the course of the competition, 207 people submitted entries. The log likelihood metric was used to evaluate the accuracy of the entries. Scores for the top 50 teams ranged from 0.18434 to 0.19884, where lower is better. Kaggle data scientist Ben Hamner used the results to illustrate the value of ensembling by means of averaging the top 50 scores. The ﬁgure below shows the results.93Life in the Trenches\\n\\nData VeracityWe’re Data Scientists, not data alchemists. We can’t make analytic gold from the lead of data.While most people associate data volume, velocity, and variety with big data, there is an equally important yet often overlooked dimension – data veracity. Data veracity refers to the overall quality and correctness of the data. You must assess the truthfulness and accuracy of the data as well as identify missing or incomplete information. As the saying goes, “Garbage in, garbage out.” If your data is inaccurate or missing information, you can’t hope to make analytic gold.Assessing data truthfulness is often subjective. You must rely on your experience and an understanding of the data origins and context. Domain expertise is particularly critical for the latter. Although data accuracy assessment may also be subjective, there are times when quantitative methods may be used. You may be able to re-sample from the population and conduct a statistical comparison against the stored values, thereby providing measures of accuracy.The most common issues you will encounter are missing or incomplete information. There are two basic strategies for dealing with missing values – deletion and imputation. In the former, entire observations are excluded from analysis, reducing sample size and potentially introducing bias. Imputation, or replacement of missing or erroneous values, uses a variety of techniques such as random sampling (hot deck imputation) or replacement using the mean, statistical distributions or models. »Tips From the ProsFind an approach that works, implement it, and move on. You can worry about optimization and tuning your approaches later during incremental improvement.THE FIELD GUIDE to  DATA SCIENCE\\n\\nBrian Keller››Time Series Modeling On one of our projects, the team was faced with correlating the time series for various parameters. Our initial analysis revealed that the correlations were almost non-existent. We examined the data and quickly discovered data veracity issues. There were missing and null values, as well as negative-value observations, an impossibility given the context of the measurements (see the ﬁgure, Time Series Data Prior to Cleansing). Garbage data meant garbage results. Because sample size was already small, deleting observations was undesirable. The volatile nature of the time series meant that imputation through sampling could not be trusted to produce values in which the team would be conﬁdent. As a result, we quickly realized that the best strategy was an approach that could ﬁlter and correct the noise in  the data. We initially tried a simplistic approach in which we replaced each observation with a moving average. While this corrected some noise, including the outlier values in our moving-average computation shifted the time series. This caused undesirable distortion in the underlying signal, and we quickly abandoned the approach.One of our team members who had experience in signal processing suggested a median ﬁlter. The median ﬁlter is a windowing technique that moves through the data point-by-point, and replaces it with the median value calculated for the current window. We experimented with various window sizes to achieve an acceptable tradeoff between smoothing noise and smoothing away signal. The ﬁgure, Time Series Data After Cleansing, shows the same two time series after median ﬁlter imputation. The application of the median ﬁlter approach was hugely successful. Visual inspection of the time series plots reveals smoothing of the outliers without dampening the naturally occurring peaks and troughs (no signal loss). Prior to smoothing, we saw no correlation in our data, but afterwards, Spearman’s Rho was ~0.5 for almost all parameters. By addressing our data veracity issues, we were able to create analytic gold. While other approaches may also have been effective, implementation speed constraints prevented us from doing any further analysis. We achieved the success we were after and moved on to address other aspects of the problem. Time Series Data Prior to CleansingTime Series Data After Cleansing Source: Booz Allen Hamilton  Source: Booz Allen Hamilton Brian Keller››Time Series Modeling On one of our projects, the team was faced with correlating the time series for various parameters. Our initial analysis revealed that the correlations were almost non-existent. We examined the data and quickly discovered data veracity issues. There were missing and null values, as well as negative-value observations, an impossibility given the context of the measurements (see the ﬁgure, Time Series Data Prior to Cleansing). Garbage data meant garbage results. Because sample size was already small, deleting observations was undesirable. The volatile nature of the time series meant that imputation through sampling could not be trusted to produce values in which the team would be conﬁdent. As a result, we quickly realized that the best strategy was an approach that could ﬁlter and correct the noise in  the data. We initially tried a simplistic approach in which we replaced each observation with a moving average. While this corrected some noise, including the outlier values in our moving-average computation shifted the time series. This caused undesirable distortion in the underlying signal, and we quickly abandoned the approach.One of our team members who had experience in signal processing suggested a median ﬁlter. The median ﬁlter is a windowing technique that moves through the data point-by-point, and replaces it with the median value calculated for the current window. We experimented with various window sizes to achieve an acceptable tradeoff between smoothing noise and smoothing away signal. The ﬁgure, Time Series Data After Cleansing, shows the same two time series after median ﬁlter imputation. The application of the median ﬁlter approach was hugely successful. Visual inspection of the time series plots reveals smoothing of the outliers without dampening the naturally occurring peaks and troughs (no signal loss). Prior to smoothing, we saw no correlation in our data, but afterwards, Spearman’s Rho was ~0.5 for almost all parameters. By addressing our data veracity issues, we were able to create analytic gold. While other approaches may also have been effective, implementation speed constraints prevented us from doing any further analysis. We achieved the success we were after and moved on to address other aspects of the problem. Time Series Data Prior to CleansingTime Series Data After Cleansing Source: Booz Allen Hamilton  Source: Booz Allen Hamilton 95Life in the Trenches\\n\\nApplication of  Domain Knowledge We are all special in our own way. Don’t discount what  you know.Knowledge of the domain in which a problem lies is immensely valuable and irreplaceable. It provides an in-depth understanding  of your data and the factors influencing your analytic goal. Many times domain knowledge is a key differentiator to a Data Science team’s success. Domain knowledge influences how we engineer and select features, impute data, choose an algorithm, and determine success. One person cannot possibly be a domain expert in every  field, however. We rely on our team, other analysts and domain  experts as well as consult research papers and publications to build  an understanding of the domain. THE FIELD GUIDE to  DATA SCIENCE\\n\\n››GEOSPATIAL HOTSPOTSTEMPORAL HOTSPOTSunSatFriThuWedTueMon9pm6pm3pmNoon9am6am3am9pm6pm3pmNoon9am6am3amArmen KherlopianMotor Vehicle TheftOn one project, our team explored how Data Science could be applied to improve public safety. According to the FBI, approximately $8 Billion is lost annually due to automobile theft. Recovery of the one million vehicles stolen every year in the U.S. is less than 60%. Dealing with these crimes represents a signiﬁcant investment of law enforcement resources. We wanted to see if we could identify how to reduce auto theft while efﬁciently using law enforcement resources.Our team began by parsing and verifying San Francisco crime data. We enriched stolen car reporting with general city data. After conducting several data experiments across both space and time, three geospatial and one temporal hotspot emerged (see ﬁgure, Geospatial and Temporal Car Theft Hotspots). The domain expert on the team was able to discern that the primary geospatial hotspot corresponded to an area surrounded by parks. The parks created an urban mountain with a number of over-foot access points that were conducive to car theft. Our team used the temporal hotspot information in tandem with the insights from the domain expert to develop a Monte Carlo model to predict the likelihood of a motor vehicle theft at particular city intersections. By prioritizing the intersections identiﬁed by the model, local governments would have the information necessary to efﬁciently deploy their patrols. Motor vehicle thefts could be reduced and law enforcement resources could be more efﬁciently deployed. The analysis, enabled by domain expertise, yielded actionable insights that could make the streets safer.Geospatial and Temporal Car Theft Hotspots Source: Booz Allen Hamilton 97Life in the Trenches\\n\\nThe Curse of DimensionalityThere is no magical potion to cure the curse, but there is PCA.The “curse of dimensionality” is one of the most important results in machine learning. Most texts on machine learning mention this phenomenon in the first chapter or two, but it often takes many years of practice to understand its true implications.Classification methods, like most machine learning methods, are subject to the implications of the curse of dimensionality. The basic intuition in this case is that as the number of data dimensions increases, it becomes more difficult to create generalizable classification models (models that apply well over phenomena not observed in the training set). This difficulty is usually impossible to overcome in real world settings. There are some exceptions in domains where things happen to work out, but usually you must work to minimize the number of dimensions. This requires a combination of clever feature engineering and use of dimensionality reduction techniques (see Feature Engineering and Feature Selection Life in the Trenches). In our practical experience, the maximum  number of dimensions seems to be ~10 for linear model-based approaches. The limit seems to be in the tens of thousands for more sophisticated methods such as support vector machines, but the limit still exists nonetheless.A counterintuitive consequence of the curse of dimensionality is  that it limits the amount of data needed to train a classification  model. There are roughly two reasons for this phenomenon. In one case, the dimensionality is small enough that the model can be  trained on a single machine. In the other case, the exponentially expanding complexity of a high-dimensionality problem makes it (practically) computationally impossible to train a model. In our experience, it is quite rare for a problem to fall in a “sweet spot” between these two extremes. Rather than trying to create super-scalable algorithm implementations, focus your attention on solving your immediate problems with basic methods. Wait until you encounter a problem where an algorithm fails to converge or provides poor cross-validated results, and then seek new approaches. Only when you find that alternate approaches don’t already exist, should you begin building new implementations. The expected cost of this work pattern is lower than over-engineering right out of the gate. Put otherwise, “Keep it simple, stupid”.THE FIELD GUIDE to  DATA SCIENCE\\n\\nBaking the Cake I was once given a time series set of roughly 1,600 predictor variables and 16 target variables and asked to implement a number of modeling techniques to predict the target variable values. The client was challenged to handle the complexity associated with the large number of variables and needed help. Not only did I have a case of the curse, but the predictor variables were also quite diverse. At ﬁrst glance, it looked like trying to bake a cake with everything in the cupboard.  That is not a good way to bake or to make predictions!\\xa0The data diversity could be partially explained by the fact that the time series predictors did not all have the same periodicity. The target time series were all daily values whereas the predictors were daily, weekly, quarterly, and monthly. This was tricky to sort out, given that imputing zeros isn’t likely to produce good results. For this speciﬁc reason, I chose to use neural networks for evaluating the weekly variable contributions. Using this approach, I was able to condition upon week, without greatly increasing the dimensionality. For the other predictors, I\\xa0used a variety of techniques, including projection and correlation, to make heads or tails of the predictors.\\xa0My approach successfully reduced the number of variables, accomplishing the client’s goal of making the problem space tractable. As a result, the cake turned out just ﬁne.››Stephanie Rivera99Life in the Trenches\\n\\nModel ValidationRepeating what you just heard does not mean that you  learned anything. Model validation is central to construction of any model. This answers the question “How well did my hypothesis fit the observed data?”If we do not have enough data, our models cannot connect the dots. On the other hand, given too much data the model cannot think outside of the box. The model learns specific details about the training data that do not generalize to the population. This is the problem of model over fitting. Many techniques exist to combat model over fitting. The simplest method is to split your dataset into training, testing and validation sets. The training data is used to construct the model. The model constructed with the training data is then evaluated with the testing data. The performance of the model against the testing set is used to further reduce model error. This indirectly includes the testing data within model construction, helping to reduce model over fit. Finally, the model is evaluated on the validation data to assess how well the model generalizes.A few methods where the data is split into training and testing sets include: k-fold cross-validation, Leave-One-Out cross-validation, bootstrap methods, and resampling methods. Leave-One-Out cross-validation can be used to get a sense of ideal model performance over the training set. A sample is selected from the data to act as the testing sample and the model is trained on the rest of the data. The error on the test sample is calculated and saved, and the sample is returned to the dataset. A different sample is then selected and the process is repeated. This continues until all samples in the testing set have been used. The average error over the testing examples gives a measure of the model’s error.There are other approaches for testing how well your hypothesis reflects the data. Statistical methods such as calculating the coefficient of determination, commonly called the R-squared value are used to identify how much variation in the data your model explains. Note that as the dimensionality of your feature space grows, the R-squared value also grows. An adjusted R-squared value compensates for this phenomenon by including a penalty for model complexity. When testing the significance of the regression as a whole, the F-test compares the explained variance to unexplained variance. A regression result with a high F-statistic and an adjusted R-squared over 0.7 is almost surely significant. »Do we really need a case study to know that you should  check your work?THE FIELD GUIDE to  DATA SCIENCE\\n\\nPUTTING  it    ALL TOGETHER\\n\\nStreamlining Medication Review Analytic ChallengeThe U.S. Food and Drug Administration (FDA) is responsible for advancing public health by supporting the delivery of new treatments to patients; assessing the safety, efﬁcacy and quality of regulated products; and conducting research to drive medical innovation. Although the FDA houses one of the world’s largest repositories of regulatory and scientiﬁc data, reviewers are not able to easily leverage data-driven approaches and analytics methods to extract information, detect signals and uncover trends to enhance regulatory decision-making and protect public health. In addition, a rapid increase in the volume, velocity and variety of data that must be analyzed to address and respond to regulatory challenges, combined with variances in data standards, formats, and quality, severely limit the ability of FDA Center for Drug Evaluation and Research (CDER) regulatory scientists to conduct cross-study, cross-product, retrospective, and meta-analysis during product reviews.Booz Allen Hamilton was engaged to research, develop, and evaluate emerging informatics tools, methods, and techniques to determine their ability to address regulatory challenges faced by the FDA Center for Drug Evaluation and Research (CDER). The main goal was to enable the CDER community to fully utilize the agency’s expansive data resources for efﬁcient and effective drug review through the design and development of informatics capabilities based on Natural Language Processing (NLP), data integration, and data visualization methodologies.Our ApproachTo support transformational change at CDER, we designed and developed a set of informatics prototypes for the analysis and modeling of complex structured, unstructured, and fragmented datasets. We developed multiple prototypes to enable the evaluation of emerging informatics tools, methods, and techniques and their ability to enable a critical-value driver – e.g., co-locate complex, heterogeneous data to identify patterns and foster development of strategies to protect public health. For example, we implemented NLP algorithms to compare adverse events across datasets, and geographic visualization capabilities to support the inspection of pharmaceutical manufacturing facilities. Product Safety Analytics. The review, surveillance, and analysis of adverse events throughout the product lifecycle require signiﬁcant resources. The ability to identify actionable insights that lead to informed decision-making requires signiﬁcant investment of effort, including the active and passive surveillance of adverse events. To address these challenges, we developed a Product Safety Dashboard that compares adverse events listed in the product label (i.e., ›› » Our Case StudiesHey, we have given you a lot of really good technical content. We know that this section has the look and feel of marketing material, but there is still a really good story here. Remember, storytelling comes in many forms and styles, one of which is the marketing version. You should read this chapter for what it is – great information told with a marketing voice. THE FIELD GUIDE to  DATA SCIENCE\\n\\npackage inserts) with data from the FDA Adverse Event Reporting System (FAERS). Using NLP, we extracted adverse events from the product label to create a structured table of label data out of unstructured text. This dashboard allows safety evaluators to view whether or not a reported adverse event is already known, without having to access an external data source and read through product labels.Product Quality Analytics.  To support CDER’s mission of reviewing and managing product quality, novel methodologies and tools are needed to improve the efﬁciency and efﬁcacy of the product quality-review process. Integration of disparate data sources is the ﬁrst step in building a comprehensive proﬁle of manufacturers, facilities, and the products associated with individual facilities. To address these challenges, we developed a Facility Inventory Report to show the geographic location of facilities and their associated metadata. This geovisualization tool processes and transforms raw data into a user-friendly visual interface with mapping features to enhance the surveillance capabilities of CDER and provide reviewers with the ability to establish connections between facility data and product quality.Our ImpactSince the FDA is responsible for regulating 25 cents of every dollar that Americans spend, the agency’s ability to fully use regulatory datasets and meaningfully integrate previously incompatible data to rapidly detect product quality and safety issues is critical for safeguarding public health. NLP approaches provide CDER with the ability to more efﬁciently search a broader range of textual data and enhances the ability to gain insight from additional data forms that may seem unrelated. Data integration and visualization directly increase the efﬁciency of researchers by reducing their time spent on searching for frequently-performed aggregate or granular calculations, and by proactively presenting the most frequently desired data to the reviewer through thoughtful and contextual dashboards designed to reveal patterns and trends in disparate data sources. These new capabilities position the FDA to enhance regulatory decision-making, drive advances in personalized medicine, and enable earlier detection of safety signals in the general population. 105Putting it all Together\\n\\nReducing Flight DelaysAnalytic ChallengeDomestic airline departure delays are estimated to cost the U.S. economy $32.9 billion annually. The Federal Aviation Administration’s (FAA’s) Trafﬁc Flow Management System (TFMS) is used to strategically manage ﬂights and includes a ﬂight departure-delay prediction engine which applies simple heuristics to predict ﬂight delays.However, the limited predictive power of these heuristics constrains the FAA’s ability to act in accordance with its existing departure-delay management plan. In response, the FAA’s NextGen Advanced Concepts and Technology Development Group wanted to create a predictive probabilistic model to improve aircraft departure time predictions. This new model would help the FAA understand the causes of departure delays and develop policies and actions to improve the reliability of departure time predictions for real-time air trafﬁc ﬂow management.Our ApproachThe commercial aviation industry is rich in ﬂight operations data, much of which is publicly available through government websites and a few subscription vendors. Booz Allen Hamilton leveraged these sources to gather over 4 TB of data detailing tarmac and airspace congestion, weather conditions, network effects, Trafﬁc Management Initiatives, and airline and aircraft-speciﬁc attributes for every commercial ﬂight departing from U.S. airports between 2008 and 2012. This data included over 50 million ﬂights and around 100 variables for each ﬂight. The data included composite variables (e.g. incoming ﬂight delay) that were constructed from the raw data to capture relevant dynamics of ﬂight operations. Data acquisition, processing, quality control, and accuracy between disparate datasets were important steps during this process. The team applied supervised learning algorithms to develop Bayesian Belief Network (BBN) models to predict ﬂight departure deviation. The most critical steps in model development were the selection of optimal algorithms to discretize model variables, and the selection of appropriate machine learning techniques to learn the model from the data. The team followed information theory principles to discretize model variables to maximize the model’s predictive power, and to represent the data as closely as possible with the least amount of network complexity. Booz Allen segmented the model variables into three different categories based on the time to ﬂight departure: 24 hours, 11 hours, and one hour. Certain ﬂight variables could only be known for speciﬁc pre-departure times. For example, the tarmac and airspace congestion variables for a ﬂight are only known just before the ﬂight, and hence those variables feature only in the one hour category. Departure delays were predicted for each of the three time horizons. ››THE FIELD GUIDE to  DATA SCIENCE\\n\\nOur ImpactFor a typical airport, the model delivers a delay prediction improvement of between 100,000 and 500,000 minutes annually over previous FAA predictions. The model can be used by a range of aviation stakeholders, such as airlines, to better understand and predict network ﬂight delays. This can improve the airlines’ operational decisions to include more proactive schedule adjustments during times of disruption (e.g. weather or sector load). The improved reliability of departure prediction will improve FAA\\'s predictions for airports, sectors, and other resources, and has the potential to enable improved real-time trafﬁc ﬂow management, which can signiﬁcantly reduce airline departure, delays, and the associated economic costs. This means a more efﬁcient and effective air transportation network.107Putting it all Together\\n\\nMaking Vaccines Safer  Analytic ChallengeThe U.S. Food and Drug Administration (FDA) Center for Biologics Evaluation and Research (CBER) is responsible for protecting public health by assuring the safety and efﬁcacy of biologics, including vaccines, blood and blood products. CBER’s current surveillance process, which requires resource-intensive manual review by expert Medical Ofﬁcers, does not scale well to short-term workload variation and limits long-term improvements in review cycle-time. In addition, the large volume of Adverse Event (AE) reports received by the Agency makes it difﬁcult for reviewers to compare safety issues across products and patient populations. CBER engaged Booz Allen Hamilton to develop advanced analytics approaches for the triage and analysis of AE reports. The main goal was to leverage (1) Natural Language Processing (NLP) to alleviate resource pressures by semi-automating some of the manual review steps through techniques, such as text classiﬁcation and entity extraction, and (2) network visualizations to offer alternative interactions with datasets and support AE pattern recognition. By integrating NLP and network analysis capabilities into the Medical Ofﬁcer’s review process, Booz Allen successfully provided decision-makers with important information concerning product risks and possible mitigations that can reduce risk.Our ApproachWe designed and developed a set of prototypes for the analysis and visualization of complex structured and unstructured AE datasets. We developed tools that leverage NLP and network analysis to extend and enhance CBER’s efforts to monitor the safety of biologics and manage safety throughout the product lifecycle. Adverse Event Text Mining Analytics. Reviewing AE reports involves sorting based on the likelihood of a relationship between a product and reported adverse events. Since much of an AE report is unstructured text, and most reports are not directly related to the use of the implicated biologic, manual review is time consuming and inefﬁcient. To address these challenges, we enhanced and integrated tools for text mining and NLP of AE reports using open source tools, including Python and R. By extracting diagnosis, cause of death, and time to onset, and presenting relevant information for review, the text-mining tool streamlines and enhances the post market surveillance process.Adverse Event Network Analytics. Visualizing relationships between vaccines and AEs can reveal new patterns and trends in the data, leading reviewers to uncover safety issues. To assist CBER Medical Ofﬁcers and researchers in identifying instances where certain vaccines or combinations of vaccines might have harmful effects on patients, we developed an AE network analysis tool using open source tools. This network analyzer allows users to select partitions of the FDA Vaccine Adverse Event Reporting System (VAERS) database, generate a co-occurrence matrix, view networks and network metrics (e.g., betweenness, ››THE FIELD GUIDE to  DATA SCIENCE\\n\\ncloseness, degree, strength), and interact with network nodes to gain insights into product safety issues.Other Analytics Solutions. In addition, Booz Allen refactored, modularized, and expended the capabilities of CBER’s computer simulation model of the Bovine Spongiform Encephalopathy (BSE) agent to improve estimates of variant Creutzfeldt-Jakob disease (vCJD) risk for blood products, developed code to handle large amounts of data generated by a Monte Carlo Markov Chain analysis of the spread of inﬂuenza, developed a large database analysis strategy involving the application of classiﬁcation algorithms to simulated genomic data, and implemented a Statistical Analysis Software (SAS) macro that automatically compares the relative potency of a given lot of vaccine using a matched set of dose response curves.  Our ImpactNew methods for post market surveillance of biologics are critical for FDA reviewers who must determine whether reported adverse events are actually a result of a biologic product. With more than 10 million vaccines administered each year to children less than one-year old, CBER reviewers are under pressure to quickly evaluate potential safety signals through manual evaluation of AE reports, review of scientiﬁc literature, and analysis of cumulative data using frequency calculations or statistical algorithms. Booz Allen’s support resulted in the development of innovative and data-driven approaches for the analysis of structured and unstructured AE reports. We increased the speed of existing text mining tools by two thousand times, allowing CBER reviewers to run a text mining algorithm to extract information contained in VAERS reports in seconds, instead of hours. We also increased the productivity of Medical Ofﬁcers through the implementation of text mining and network analysis tools. These new capabilities allow CBER to streamline the post market review process, extract knowledge from scientiﬁc data, and address public concerns regarding vaccine safety more quickly and efﬁciently.109Putting it all Together\\n\\nForecasting the Relative Risk for the Onset of Mass Killings to Help Prevent Future AtrocitiesAnalytic ChallengeMass atrocities are rare yet devastating crimes. They are also preventable. Studies of past atrocities show that we can detect early warning signs of atrocities and that if policy makers act on those warnings and develop preventive strategies, we can save lives. Yet despite this awareness, all too often we see warning signs missed and action taken too late, if at all, in response to threats of mass atrocities. The Early Warning Project, an initiative of the United States Holocaust Memorial Museum (Holocaust Museum), aims to assess a country’s level of risk for the onset of future mass killings. Over time, the hope is to learn which models and which indicators are the best at helping anticipate future atrocities to aid in the design and implementation of more targeted and effective preventive strategies. By seeking to understand why and how each countries’ relative level of risk rises and falls over time, the system will deepen understanding of where new policies and resources can help make a difference in averting atrocities and what strategies are most effective. This will arm governments, advocacy groups, and at-risks societies with earlier and more reliable warning, and thus more opportunity to take action, well before mass killings occur.The project’s statistical risk assessment seeks to build statistical and machine learning algorithms to predict the onset of a mass killing in the succeeding 12 months for each country with a population larger than 500,000. The publically available system aggregates and provides access to open source datasets as well as democratizes the source code for analytic approaches developed by the Holocaust Museum staff and consultants, the research community, and the general public. The Holocaust Museum engaged Booz Allen to validate existing approaches as well as explore new and innovative approaches for the statistical risk assessment. Our ApproachTaking into account the power of crowdsourcing, Booz Allen put out a call to employees to participate in a hack-a-thon—just the start of the team’s support as the Museum reﬁned and implemented the recommendations. More than 80 Booz Allen Hamilton software engineers, data analysts, and social scientists devoted a Saturday to participate. Interdisciplinary teams spent 12 hours identifying new datasets, building new machine learning models, and creating frameworks for ensemble modeling and interactive results visualization.  Following the hack-a-thon, Booz Allen Data Scientists worked with Holocaust Museum staff to create a data management framework to automate the download, aggregation, and transformation of the open ››THE FIELD GUIDE to  DATA SCIENCE\\n\\nsource datasets used by the statistical assessment. This extensible framework allows integration of new datasets with minimal effort, thereby supporting greater engagement by the Data Science community.   Our ImpactPublically launched in the fall of 2015, the Early Warning Project can now leverage advanced quantitative and qualitative analyses to provide governments, advocacy groups and at-risk societies with assessments regarding the potential for mass atrocities around the world. Integration of the project’s statistical risk assessment models and expert opinion pool created a publicly available source of invaluable information and positioned Data Science at the center of global diplomacy.  The machine learning models developed during the hack-a-thon achieved performance on par with state of the art approaches as well as demonstrated the efﬁcacy of predictions 2-5 years into the future. Teams also identiﬁed approaches for constructing test/validation sets that support more robust model evaluation. These risk assessments are an important technological achievement in and of themselves, but what this initiative means for the Data Science community’s position in global diplomatic dialogue marks an entirely new era for those on the frontiers of Big Data.The data management framework developed from the lessons learned of the hack-a-thon represents a great leap forward for the Holocaust Museum. The periodicity of aggregating and transforming data was reduced from twice per year to once per week. In addition to providing the community with more up-to-date data, the reduced burden on researchers enables them to spend more time analyzing data and identifying new and emergent trends. The extensible framework will also allow the Holocaust Museum to seamlessly integrate new datasets as they become available or are identiﬁed by the community as holding analytic value for the problem at hand. Through this project, the Holocaust Museum was able to shift the dynamic from monitoring ongoing violence to determining where it is likely to occur 12 to 24 months into the future by integrating advanced quantitative and qualitative analyses to assess the potential for mass atrocities around the world. The Early Warning Project is an invaluable predictive resource supporting the global diplomatic dialogue. While the focus of this effort was on the machine learning and data management technologies behind the initiative, it demonstrates the growing role the Data Science community is playing at the center of global diplomatic discussions.111Putting it all Together\\n\\nPredicting Customer ResponseAnalytic ChallengeIt is very challenging to know how a customer will respond to a given promotional campaign.  Together with the InterContinental Hotels Group (IHG), Booz Allen Hamilton explored approaches to predict customer-by-customer response to a state-of-the-art promotional campaign in order to better understand and increase return on investment (ROI). In the past, conventional statistics have worked well for analyzing the impact of direct marketing promotions on purchase behavior. Today, modern multi-channel promotions often result in datasets that are highly dimensional and sometimes sparse, which strains the power of conventional statistical methods to accurately estimate the effect of a promotion on individual purchase decisions. Because of the growing frequency of multi-channel promotions, IHG was driven to investigate new approaches. In particular, IHG and Booz Allen studied one recent promotional campaign using hotel, stay, and guest data for a group of loyalty program customers.  Our ApproachWorking closely with IHG experts, Booz Allen investigated three key elements related to different stages of analytic maturity:Describe: Using initial data mining, what insights or tendencies in guest behaviors can be identiﬁed after joining multiple, disparate datasets?Discover: Can we determine which control group members would be likely to register for a promotion if offered? If so, can we also quantify their registration?Predict: How would a hotel guest that received the promotion have responded if they were not offered the promotion? How would a hotel guest that did not receive the promotion have responded if they were offered the promotion?For the promotion that was the focus of this case study, not everything about customers could be controlled as required by traditional statistics. However, because a probabilistic Bayesian Belief Network (BBN) can learn the pairwise relationships between all individual customer attributes and their impact on promotional return, Booz Allen investigated how this technique could be used to model each treated customer without an exact controlled look-alike. Speciﬁcally, Booz Allen developed a BBN to predict customer-by-customer impacts driven by promotional campaign offers, subsequently estimating the aggregated ROI of individual campaigns. We used six machine learning techniques (support vector machine, random forest, decision tree, neural network, linear model, and AdaBoost) in unison with the BBN to predict how each customer would be inﬂuenced by a promotional offer.››THE FIELD GUIDE to  DATA SCIENCE\\n\\nOur ImpactThe probabilistic model was capable of predicting customer response to the promotion, without relying on a costly control group. This ﬁnding represents millions of dollars of savings per promotional campaign. This analysis is an industry-ﬁrst for the travel and hospitality sector, where demand for more data-driven approaches to optimize marketing ROI at an individual level is rapidly growing.Because Booz Allen and IHG’s approach enabled estimation of ROI for each hypothetical customer, even when no exact look-alikes exist, there are a number of valuable future applications. One such application is optimal campaign design—the ability to estimate the promotional attributes for an individual customer that are likely to drive the greatest incremental spend. Another application is efﬁcient audience selection - which would reduce the risk of marketing “spam” that prompts costly unsubscriptions and can negatively impact a hotel’s brand. 113Putting it all Together\\n\\nC L O S I N G    T I M E\\n\\nTHE FUTURE  of           DATA SCIENCE Data Science is rapidly evolving as it touches every aspect of our lives on a daily basis. As Data Science changes the way we interact with, and explore our world, the algorithms and applications of Data Science continue to advance. We expect this trend to continue as Data Science has an increasingly profound effect on humanity. We describe here some of the trends and developments we anticipate emerging in the field of Data Science over the coming years. The advancements in some Data Science algorithms will deliberately track the evolution of data structures and data models that Data Scientists are using to represent their domains of study. One of the clearest examples of this linkage is in the development of massive-scale graph analytics algorithms, which are deployed on graph databases (including network data and semantically linked databases). It is sometimes said “all the world is a graph,” and consequently the most natural data structure is not a table with rows and columns, but a network graph with nodes and edges. Graph analytics encompasses traditional methods of machine learning, but with a graph-data twist.Another growth area in Data Science algorithms is in the domain of geospatial temporal predictive analytics, which can be applied to any dataset that involves geospatial location and time – that describes just about everything in our lives! We expect increasingly sophisticated deployments of this methodology in the areas of law enforcement, climate change, disaster management, population health, sociopolitical change, and more. It is obvious that bigger, faster, and more complex datasets will require faster (hyperfast!) analytics. We anticipate advanced Data Science algorithms that take advantage of technological advancements in quantum machine-learning, in-memory data operations, and machine learning on specialized devices (e.g., the GPU, Raspberry Pi, or the next-generation mobile handheld “supercomputer”). In such commodity devices, we expect to see development of more embedded machine learning (specifically, deep learning) algorithms that perform time-critical data-to-insights transformations at the point of data collection. Such use cases will be in great abundance within the emerging Internet of Things (IoT), including the industrial IoT and the internet of everything.Advances in cognitive machine learning are on the horizon, including open source and configurable algorithms that exploit streaming real-time data’s full content, context, and semantic meaning. The ability to use the 360-degree view of a situation will enable the delivery of the right action, at the right time, at the right place, in the right context – this is the essence of cognitive analytics. Another way to view cognitive analytics is that, given all of the data and the context for a given object or population, the algorithm identifies the right question that you should be asking of your data (which might not be the question that you traditionally asked).››Kirk BorneTHE FIELD GUIDE to  DATA SCIENCE\\n\\nAnother area of Data Science evolution that tracks with the growth in a particular data type is that of unstructured data, specifically text. The growth of such unstructured data is phenomenal, and demands richer algorithms than those used on numerical data, since there are many more shades of meaning in natural language than in tables of numbers. The new Data Science algorithms for unstructured data will be applied in multiple directions. Natural Language Generation will be used to convert data points into text, which can be used to generate the data’s story automatically.  Structured Database Generation will transform text documents or other unstructured data into data points (i.e., converting qualitative data into machine-computable quantitative data).All of these technical advancements, plus others that we cannot yet imagine, will be brought to bear on new domains. Some of the hottest, most critical domains in which Data Science will be applied in the coming years include:  ›Cybersecurity including advanced detection, modeling, prediction, and  prescriptive analytics ›Healthcare including genomics, precision medicine, population health, healthcare delivery, health data sharing and integration, health record mining, and wearable device analytics ›IoT including sensor analytics, smart data, and emergent discovery alerting and response ›Customer Engagement and Experience including 360-degree view, gamification, and just-in-time personalization ›Smart X, where X = cities, highways, cars, delivery systems, supply chain, and more ›Precision Y, where Y = medicine, farming, harvesting, manufacturing, pricing, and more ›Personalized Z, where Z = marketing, advertising, healthcare, learning, and more ›Human capital (talent) and organizational analytics ›Societal goodTHE FUTURE OFDATA SCIENCEAlgorithmsApplications› Massive-scale Graph› Geospatial Temporal Predictive Analytics› Hyperfast Analytics› Embedded Deep Learning› Cognitive Machine Learning› Natural Language Generation› Structured Database Generation › Cybersecurity› Healthcare› Internet of Things› Customer Engagement & Experience› Smart Everything› Human Capital› Data for Societal Good 117Closing Time\\n\\n››\\n\\nP A R T I N G T H O U G H T S\\n\\nData Science capabilities are creating data analytics that are touching every aspect of our lives on a daily basis. From visiting the doctor, to driving our cars, to shopping for services Data Science is quietly changing the way we interactive with and explore our world. We hope we have helped you truly understand the potential of your data and how to become extraordinary thinkers by asking the right questions. We hope we have helped continue to drive forward the science and art of Data Science. Most importantly, we hope you are leaving with a newfound passion and excitement for Data Science.\\n\\nThank you for taking this journey with us. Please join our conversation and let your voice be heard. Email us your ideas and perspectives at data_science@bah.com or submit them via a pull request on the Github repository.\\n\\nTell us and the world what you know. Join us. Become an author of this story.\\n\\nClosing Time\\n\\n119\\n\\n››\\n\\nR E F E R E N C E S\\n\\n1.  Commonly attributed to: Nye, Bill. Reddit Ask Me Anything (AMA).\\n\\nJuly 2012. Web. Accessed 15 October 2013. SSRN: <http://www.reddit. com/r/IAmA/comments/x9pq0/iam_bill_nye_the_science_guy_ama>\\n\\n2.  Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “From Data Mining to Knowledge Discovery in Databases.” AI Magazine 17.3 (1996): 37-54. Print.\\n\\n3.\\n\\n“Mining Data for Nuggets of Knowledge.” Knowledge@Wharton,1999. Web. Accessed 16 October 2013. SSRN: <http://knowledge.wharton. upenn.edu/article/mining-data-for-nuggets-of-knowledge>\\n\\n4.  Cleveland, William S. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” International Statistical Review 69.1 (2001): 21-26. Print.\\n\\n5.  Davenport, Thomas H., and D.J. Patil. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review 90.10 (October 2012): 70–76. Print.\\n\\n6.  Smith, David. “Statistics vs Data Science vs BI.” Revolutions, 15 May 2013. Web. Accessed 15 October 2013. SSRN:<http://blog. revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html>\\n\\n7.  Brynjolfsson, Erik, Lorin M. Hitt, and Heekyung H. Kim. “Strength in Numbers: How Does Data-Driven Decision Making Affect Firm Performance?” Social Science Electronic Publishing, 22 April 2011. Web. Accessed 15 October 2013. SSRN: <http://ssrn.com/abstract=1819486 or http://dx.doi.org/10.2139/ssrn.1819486>\\n\\n8.\\n\\n“The Stages of an Analytic Enterprise.” Nucleus Research. February 2012. Whitepaper.\\n\\n9.  Barua, Anitesh, Deepa Mani, and Rajiv Mukherjee. “Measuring the\\n\\nBusiness Impacts of Effective Data.” University of Texas. Web. Accessed 15 October 2013. SSRNL: <http://www.sybase.com/files/White_Papers/ EffectiveDataStudyPt1-MeasuringtheBusinessImpactsofEffectiveDa ta-WP.pdf>\\n\\n10.  Zikopoulos, Paul, Dirk deRoos, Kirshnan Parasuraman, Thomas Deutsch, David Corrigan and James Giles. Harness the Power of Big Data: The IBM Big Data Platform. New York: McGraw Hill, 2013. Print. 281pp.\\n\\nT H E   F I ELD   G U I D E   to  D A T A   S C I E N C E\\n\\n11.  Booz Allen Hamilton. Cloud Analytics Playbook. 2013. Web. Accessed 15 October 2013. SSRN: <http://www.boozallen.com/media/file/Cloud- playbook-digital.pdf>\\n\\n12.  Conway, Drew. “The Data Science Venn Diagram.” March 2013.\\n\\nWeb. Accessed 15 October 2013. SSRN: <http://drewconway.com/ zia/2013/3/26/the-data-science-venn-diagram>\\n\\n13.  Booz Allen Hamilton. Tips for Building a Data Science Capability. 2015.\\n\\nWeb Accessed 2 September 2015. SSRN: < https://www.boozallen.com/ content/dam/boozallen/documents/2015/07/DS-Capability-Handbook.pdf>\\n\\n14.  Mnih et al. 2015. Human-level control through deep reinforcement\\n\\nlearning. Nature. 518: 529\\n\\n\\n\\n533.\\n\\n15.  Torán, Jacobo. “On the Hardness of Graph Isomorphism.” SIAM Journal\\n\\non Computing. 33.5 (2004): 1093-1108. Print.\\n\\n16.  Guyon, Isabelle and Andre Elisseeff. “An Introduction to Variable and Feature Selection.” Journal of Machine Learning Research 3 (March 2003):1157-1182. Print.\\n\\n17.  Golub T., D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov, H. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloomfield, and E. Lander. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science. 286.5439 (1999): 531-537. Print.\\n\\n18.  Haykin, Simon O. Neural Networks and Learning Machines. New Jersey:\\n\\nPrentice Hall, 2008. Print.\\n\\n19.  De Jong, Kenneth A. Evolutionary Computation - A Unified Approach.\\n\\nMassachusetts: MIT Press, 2002. Print.\\n\\n20.  Yacci, Paul, Anne Haake, and Roger Gaborski. “Feature Selection of\\n\\nMicroarray Data Using Genetic Algorithms and Artificial Neural Networks.” ANNIE 2009. St Louis, MO. 2-4 November 2009. Conference Presentation.\\n\\nClosing Time\\n\\n121\\n\\n››\\n\\nAbout B O O Z   A LLEN H A M I L T O N\\n\\nBooz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years. Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering, and innovation expertise.\\n\\nWith international headquarters in McLean, Virginia, the firm employs more than 22,500 people globally, and had revenue of $5.27 billion for the 12 months ended March 31, 2015. To learn more, visit www.boozallen.com. (NYSE: BAH)\\n\\nClosing Time\\n\\n123', metadata={'source': '/tmp/tmpw4mg91tz'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "offline_pdf_loader = load_document(\"Rapid Trance - Technical Specification Document (TSD).pdf\")\n",
        "offline_pdf_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayww3scYLSDs",
        "outputId": "cdb5ea24-dd1f-4028-857a-03b111053240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:unstructured:detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Technical Specification Document (TSD)\\n\\nRapid Trance\\n\\nVersion 1.0\\n\\nTable of Contents\\n\\n1. Document History\\n\\n2. Technology Stack\\n\\n3. Assumptions and Risks\\n\\n4. Repository Naming Convention\\n\\n5. Git Branch Naming Convention\\n\\n6. Git Commit Naming Convention\\n\\n7. Pull Request Guidelines\\n\\n8. Quality Process and Methodologies\\n\\n8.1 Security Testing\\n\\n8.2 QA Deliverables\\n\\n9. Deployment Guidelines\\n\\n9.1. Frontend deployment\\n\\n10. Architecture\\n\\n10.1. High\\n\\n\\n\\nLevel Architecture\\n\\n10.2. Backend Flask Architecture\\n\\n10.3. Backend Workflow\\n\\n10.4. Cloud Infrastructure\\n\\n11. User Stories and Functional Requirements\\n\\n11.1. Persona #1: Platform User\\n\\n12. Models\\n\\n13. Cases the bot will not join/ FAIL.\\n\\n14.Reference and Supporting Documents\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n6\\n\\n6\\n\\n7\\n\\n8\\n\\n8\\n\\n8\\n\\n9\\n\\n9\\n\\n10\\n\\n10\\n\\n10\\n\\n10\\n\\n13\\n\\n11\\n\\n11\\n\\n16\\n\\n17\\n\\n13\\n\\n1. Document History\\n\\nVersion Modification\\n\\nDate\\n\\nAuthor\\n\\nSummary of Changes\\n\\nv1.0\\n\\nv2.0\\n\\nv2.1\\n\\nSeptember 09,\\n\\n2022\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nPublished on September 09, 2022 - use major version for new phases or major changes\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nsummary of changes (can be bulleted)\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nuse minor version for fixes or updates after client reviews\\n\\n2. Technology Stack\\n\\nWeb Application\\n\\nPurpose\\n\\nCommon\\n\\nFrontend stack\\n\\nBackend\\n\\nDatabase\\n\\nInfrastructure\\n\\nPurpose\\n\\nStorage of Metadata\\n\\nFrontend\\n\\nContainerisation\\n\\nVPC\\n\\nCI/CD\\n\\nPython\\n\\nPurpose\\n\\nTechnology\\n\\nPython | Pycharm\\n\\nReactJS | Redux | Webpack | Styled Components | Formik\\n\\nPython | Flask | MongoDB\\n\\nMongoDB\\n\\nTechnology\\n\\nAWS S3\\n\\nAWS Amplify | AngularJS\\n\\nAWS ECS\\n\\nPrivate | Public Subnets\\n\\nAWS ECR | AWS Code Build | AWS Code Pipeline\\n\\nTechnology\\n\\nMeetings, Transcripts, User\\n\\nMajor SDKs/Libraries\\n\\nGoogle Calendar API\\n\\nBeautifulsoup | google oauth\\n\\n| Python Selenium | Python\\n\\nFlask, pymongo, nltk, boto3,\\n\\nsummarizer, Openai, sentence\\n\\n\\n\\ntransformers\\n\\nsentry, Bert extractive\\n\\n3. Assumptions and Risks\\n\\nMaximum 15 meetings can run in one instance\\n\\n4. Repository Naming Convention\\n\\nSyntax of a Github repository\\n\\n{platform}\\n\\n\\n\\n{technology}\\n\\n\\n\\n{application\\n\\n\\n\\ntype}\\n\\n\\n\\n{project\\n\\n\\n\\nname}\\n\\nNo uppercase, or special characters allowed, besides dashes “-“. A few examples of this might be:\\n\\nRapid\\n\\n\\n\\nTrance (Backend Server)\\n\\nRapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction (ML Metrics server)\\n\\n5. Git Branch Naming Convention\\n\\nInclude the issue key in the branch name when you create the branch to link it to your Jira issue.\\n\\nFor example: git checkout\\n\\n\\n\\nb feature/<branch\\n\\n\\n\\nname>\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, GitHub Enterprise, and Fisheye tools.\\n\\nIf you’re using Bitbucket Cloud, GitHub or GitLab, you can create a branch directly from your Jira issue. Under Development, select Create branch. If you create a branch this way, the issue key is automatically added to the name of the branch.\\n\\n6. Git Commit Naming Convention\\n\\nInclude the issue key in the commit message to link the commit to your Jira issue.\\n\\nFor example: git commit\\n\\n\\n\\nm \"feat: <commit message>\"\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, GitHub Enterprise, and Fisheye tools.\\n\\nThe <type> of a commit message should be a single word or abbreviation drawn from an ontology,\\n\\naccording to the nature of the project. This document specifies a programming ontology, with the\\n\\nfollowing elements:\\n\\nfeat: A new feature\\n\\nfix: A bug fix\\n\\ndocs: Documentation only changes\\n\\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing\\n\\nsemicolons, etc)\\n\\nrefactor: A code change that neither fixes a bug nor adds a feature\\n\\nperf: A code change that improves performance\\n\\ntest: Adding missing tests\\n\\nbuild: Changes to the build/compilation/packaging process or auxiliary tools such as\\n\\ndocumentation generation\\n\\nci: Changes in the continuous integration/delivery setup\\n\\n7. Pull Request Guidelines\\n\\nInclude a commit in the pull request that has the issue key in the commit message. Note, the\\n\\ncommit cannot be a merge commit.\\n\\nInclude the issue key in the pull request title.\\n\\nEnsure that the source branch name also includes the issue key in the branch name.\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, and GitHub Enterprise tools.\\n\\nIf you create the pull request from the development panel in a Jira issue, the issue key is added\\n\\nautomatically.\\n\\n8. Quality Process and Methodologies\\n\\nRI Quality Assurance process is an integral part of every sprint of the iterative Software Development Life Cycle. A risk-based testing approach as mentioned below ensures adequate coverage of testing requirements. Jira is used at RI as a defect tracking and logging software.\\n\\n8.1 Security Testing\\n\\nSecurity Testing at RI covers vulnerabilities of the application and protects the\\n\\napplication from possible intruders\\n\\nVulnerability Scanning, Security Scanning, Penetration Testing, Ethical Hacking and\\n\\nRisk Assessment are few of the activities that are carried out as part of security\\n\\ntesting\\n\\n8.2 QA Deliverables\\n\\nWith a mature testing process with RI, following deliverables are created by the QA team ensuring adequate coverage of requirements.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHigh Level Test Plan\\n\\nRequirement Traceability\\n\\nTest Execution Results\\n\\nTest Summary Reports\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDetailed Test Cases\\n\\nSmoke/Sanity Test\\n\\nDefects Logs\\n\\nUser Acceptance Testing\\n\\n9. Deployment Guidelines\\n\\nAutomated build pipelines will increase the productivity and seamless release to dev, staging or production environments.\\n\\nGit repositories are configured with pipelines which create docker images and get deployed via K8 pods as shown above.\\n\\n9.1. Frontend deployment\\n\\nFront-end code is managed via Github and AWS Code Pipeline. The frontend versions are managed via AWS Amplify.\\n\\n10. Architecture\\n\\n10.1. High\\n\\n\\n\\nLevel Architecture\\n\\n10.2. Backend Flask Architecture\\n\\n10.3. Backend Workflow\\n\\nBackend workflow depends on the following components.\\n\\n1. User\\n\\nLogin with google account to get authorised from google , after login successfully will get response ie. user id, expiry time and tokens which can use to create “Events”\\n\\n1.1. Login\\n\\nLogin: To login there will be 2 ways of it, as shown in above fig. One of it is, from the browser with the link http://rapid-trance.rapidinnovation.tech:5000/google-login after access we got the response with oauth access token, refresh token, expiry token, user id, email, name. The access token expires in 1 hour. After that we can refresh it with a refresh token. After login it creates a json response and saves this user details to the database.\\n\\nThe another way is also same in this we need to just login with rapidview portal with\\n\\nlink https://rapid-trance.rapidinnovation.tech/api/v1/users here we got the response with oauth access token, refresh token, oauthExpiryIn. this redirected to google apis portal with oauth access token with this we got the user details with verified email address and after this saved this details to database.\\n\\nThis both ways are secured with the google oauth. Login with google account to get\\n\\nauthorised from google\\n\\n2. Meetings\\n\\n2.1 Create events After login successfully done, To create events with the required inputs are, event(meet) title, start time, end time, guest list, meeting notes, attachments if required, and other details. Below is the detailed architecture of the meetings.\\n\\nTo create an event after login, https://rapid-trance.rapidinnovation.tech/api/v1/meeting this api end point is used. All the attachments of the event are saved in the organiser\\'s google drive and he is able to give access to a particular person. If the meeting is created on a daily basis or weekly basis then, that event and required details is fetched from the database.\\n\\n3. Transcripts\\n\\nTranscripts: After successfully create event or schedule meet, by using crontab the post API is hit and the celery worker is create a task in a celery queue and then the Rapid Innovation bot is join the meet, with cation on and fetch all transcript (live captions) from the meet by using selenium python and also use beautiful soup python library to get accurate captions from the meet, and after meeting ends, we save that transcripts to csv and in json format. CSV is stored in amazon s3 bucket and the json of transcript is saved in the database after this, that celery task is completed.\\n\\nBasically, after generating the json of transcripts we also capture the attendance list of people who are joining the meet and absent in the meet. With the help of transcript json and the attendance list, we create a statistical metric, this will be explained below.\\n\\n3.1. Silence duration Silence Duration: This is the silence count of people during the meeting. This shows how much time all people will silence in the meeting. According to this we can calculate silence duration and save this duration into the database after the meeting ended.\\n\\n3.2. Statistical metrics Statistical Metric: The statistical metric is dependent on the transcript and the attendance list of the particular meet, there are following component of statistical metric which are,\\n\\n1. Participation: This one is to calculate the percentage of participation of people which\\n\\ngives you, how much talk a person has in a meet.\\n\\n2. Planned Duration: This gives an actual meet duration decided. 3. Spent Duration: This is an actual spend time in a meet.\\n\\n4. Present List: This is a list of people who are present in that meeting according to the\\n\\nattendance list.\\n\\n5. Absent List: This is a list of people who are absent in that meeting according to the\\n\\nattendance list. Late Joiner: This gives a list of people who join the meeting late.\\n\\n6. 7. Early Exit: This gives a list of people who exit the meet early as scheduled time.\\n\\nAll components are saved into the database and we have a get API to get all components as response of statistical metric.\\n\\n10.4. Cloud Infrastructure\\n\\nMore architecture document goes here like Application level diagram, Sequence Diagram, Dataflow diagram, ER Diagram etc.\\n\\n11. User Stories and Functional Requirements\\n\\n11.1. Persona #1: Platform User\\n\\n1.1\\n\\nPlatform User: User Story\\n\\nAs a Platform User, I want to be able to view the landing page so that I can learn about the platform\\n\\n1.1.1\\n\\nFeature\\n\\nThis feature allows the user to explore and create NFT pages by clicking on relevant buttons from hero section\\n\\n1.1.2\\n\\nDescription\\n\\nThe landing page consists of content describing how the platform helps the user in buying and selling NFTs. The landing page comprises seven sections: title content, notable drops, top collection, trending, creating and selling your NFTs, categories, and introductory video.\\n\\n1.1.3\\n\\nPurpose\\n\\nThe purpose of this feature allows the user to get to know the platform and provide a better introduction\\n\\n1.1.4\\n\\nPage Sections\\n\\nLanding Page Header: Explains the platform in a short\\n\\nand crisp manner.\\n\\nNotable Drops: Shows the collections that are popular\\n\\nand available for purchase\\n\\nTop Collection: Shows the collectibles that are popular\\n\\nfor a certain period.\\n\\nTrending: Shows the collectibles that are trending in\\n\\nplatform rankings\\n\\nCreate and Sell your NFTs: Explains to the user how\\n\\nthey can create, store and sell their NFTs on the\\n\\nplatform.\\n\\nCategories: Shows all categories of the NFTs available\\n\\non the marketplace.\\n\\n1.1.5\\n\\nAPIs Called\\n\\n1. NotableDrop\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collection/fetchNotabl\\n\\neDrops\\n\\n2. Top Collection\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collections/topCollecti\\n\\nons\\n\\n3. Trending Collection\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collections/trendingCo\\n\\nllections/all\\n\\n4. Categories\\n\\n\\n\\n{{BASE_URL}}/api/v1/user/getAllCategories\\n\\n1.1.6\\n\\nUser Flow\\n\\nEnter the URL(Jungle.Domain) >> Land on the homepages\\n\\n1.2\\n\\nPlatform User: User Story\\n\\nAs a Platform User, I want to be able to sign up and create my profile so that I can share and view my collections\\n\\n1.2.1\\n\\nFeature\\n\\nThis feature allows users to sign up and create their profile on the platform by connecting their wallet\\n\\n1.2.2\\n\\nDescription\\n\\nThis feature enables users to view their created and collected NFTs/bundles and view the NFTs that have been marked as favourite.\\n\\n1.2.3\\n\\nPurpose\\n\\nTo enable the platform user to sign up and use the platform by adding their profile\\n\\n1.2.4\\n\\nPage Sections\\n\\nAdd Wallet: The user would need an Ethereum wallet\\n\\nto connect to the platform.\\n\\nFilter: Shows options to filter out NFTs according to the\\n\\nuser’s desire.\\n\\n1.2.5\\n\\nAPIs Called\\n\\nLogin/Create Account\\n\\n\\n\\nPOST {{BASE_URL}}/api/v1/user\\n\\n1.2.6\\n\\nUser Flow\\n\\nHome page >> Connect wallet (right top corner) >> Profile section\\n\\n12. Models\\n\\n1. Sentiment Model Path:\\n\\nhttps://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/\\n\\npolarity (Open Source)\\n\\n2. questions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/question_extraction (Open Source)\\n\\n3. keybert_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\nProduction/tree/main/all-mpnet-base-v2 (Open Source)\\n\\n4. action_bert_model_path=\\'https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\nML-Production/tree/main/bert-base-uncased (Open Source)\\n\\n5. action_classifier_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTransc\\n\\nriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/action_item_model (Open Source)\\n\\n6. summary_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/MEETING_SUMMARY (Open Source)\\n\\n7. emotions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nM\\n\\nL\\n\\n\\n\\nProduction/tree/main/emotions (Open Source)\\n\\n8. all\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Sentence Transformer Model)=\\n\\nhttps://www.sbert.net/docs/pretrained_models.html (Open Source)\\n\\n9. paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (SBertSummarizer Model)\\n\\n=https://huggingface.co/sentence\\n\\n\\n\\ntransformers/paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Open\\n\\nSource)\\n\\n13. Cases the bot will not join / FAIL.\\n\\nS.No Meeting Titles\\n\\nError\\n\\nDescription\\n\\n1\\n\\n2\\n\\n1. PixelPark Challenges,\\n\\n2. PM Standup\\n\\nYou can’t join this\\n\\nvideo call.\\n\\n1. BUSTA Games\\n\\n\\n\\nBOps\\n\\nMeeting,\\n\\n2. Weely meet with\\n\\nShreyas Bafna\\n\\nThe meeting code\\n\\nthat you entered\\n\\ndoesn’t work\\n\\nMake sure that you’ve entered the correct meeting code in the URL, e.g. https://meet.google.com/xxx-yyyy-zz z\\n\\n14. Reference and Supporting Documents\\n\\nList of important references and documents goes here', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_character_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "recursive_character_text_splitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n77PW1gdES-0",
        "outputId": "5331330d-5e74-4a61-bf4e-fb0000002afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.text_splitter.RecursiveCharacterTextSplitter at 0x7ff4fefb2760>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the document into paragraphs\n",
        "def split_document(document):\n",
        "    return recursive_character_text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "eSBrSGvcEtLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_paragraphs = split_document(online_pdf_loader)\n",
        "print(online_paragraphs)"
      ],
      "metadata": {
        "id": "6i_sxq5AEtNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offline_paragraphs = split_document(offline_pdf_loader)\n",
        "print(offline_paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMctIyDSLfbD",
        "outputId": "289f390b-074a-4999-c432-6679a1076548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Technical Specification Document (TSD)\\n\\nRapid Trance\\n\\nVersion 1.0\\n\\nTable of Contents\\n\\n1. Document History\\n\\n2. Technology Stack\\n\\n3. Assumptions and Risks\\n\\n4. Repository Naming Convention\\n\\n5. Git Branch Naming Convention\\n\\n6. Git Commit Naming Convention\\n\\n7. Pull Request Guidelines\\n\\n8. Quality Process and Methodologies\\n\\n8.1 Security Testing\\n\\n8.2 QA Deliverables\\n\\n9. Deployment Guidelines\\n\\n9.1. Frontend deployment\\n\\n10. Architecture\\n\\n10.1. High\\n\\n\\n\\nLevel Architecture\\n\\n10.2. Backend Flask Architecture\\n\\n10.3. Backend Workflow\\n\\n10.4. Cloud Infrastructure\\n\\n11. User Stories and Functional Requirements\\n\\n11.1. Persona #1: Platform User\\n\\n12. Models\\n\\n13. Cases the bot will not join/ FAIL.\\n\\n14.Reference and Supporting Documents\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n6\\n\\n6\\n\\n7\\n\\n8\\n\\n8\\n\\n8\\n\\n9\\n\\n9\\n\\n10\\n\\n10\\n\\n10\\n\\n10\\n\\n13\\n\\n11\\n\\n11\\n\\n16\\n\\n17\\n\\n13\\n\\n1. Document History\\n\\nVersion Modification\\n\\nDate\\n\\nAuthor\\n\\nSummary of Changes\\n\\nv1.0\\n\\nv2.0\\n\\nv2.1\\n\\nSeptember 09,\\n\\n2022\\n\\n<<author\\n\\n\\n\\nname>>', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Published on September 09, 2022 - use major version for new phases or major changes\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nsummary of changes (can be bulleted)\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nuse minor version for fixes or updates after client reviews\\n\\n2. Technology Stack\\n\\nWeb Application\\n\\nPurpose\\n\\nCommon\\n\\nFrontend stack\\n\\nBackend\\n\\nDatabase\\n\\nInfrastructure\\n\\nPurpose\\n\\nStorage of Metadata\\n\\nFrontend\\n\\nContainerisation\\n\\nVPC\\n\\nCI/CD\\n\\nPython\\n\\nPurpose\\n\\nTechnology\\n\\nPython | Pycharm\\n\\nReactJS | Redux | Webpack | Styled Components | Formik\\n\\nPython | Flask | MongoDB\\n\\nMongoDB\\n\\nTechnology\\n\\nAWS S3\\n\\nAWS Amplify | AngularJS\\n\\nAWS ECS\\n\\nPrivate | Public Subnets\\n\\nAWS ECR | AWS Code Build | AWS Code Pipeline\\n\\nTechnology\\n\\nMeetings, Transcripts, User\\n\\nMajor SDKs/Libraries\\n\\nGoogle Calendar API\\n\\nBeautifulsoup | google oauth\\n\\n| Python Selenium | Python\\n\\nFlask, pymongo, nltk, boto3,\\n\\nsummarizer, Openai, sentence\\n\\n\\n\\ntransformers\\n\\nsentry, Bert extractive\\n\\n3. Assumptions and Risks', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Maximum 15 meetings can run in one instance\\n\\n4. Repository Naming Convention\\n\\nSyntax of a Github repository\\n\\n{platform}\\n\\n\\n\\n{technology}\\n\\n\\n\\n{application\\n\\n\\n\\ntype}\\n\\n\\n\\n{project\\n\\n\\n\\nname}\\n\\nNo uppercase, or special characters allowed, besides dashes “-“. A few examples of this might be:\\n\\nRapid\\n\\n\\n\\nTrance (Backend Server)\\n\\nRapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction (ML Metrics server)\\n\\n5. Git Branch Naming Convention\\n\\nInclude the issue key in the branch name when you create the branch to link it to your Jira issue.\\n\\nFor example: git checkout\\n\\n\\n\\nb feature/<branch\\n\\n\\n\\nname>\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, GitHub Enterprise, and Fisheye tools.\\n\\nIf you’re using Bitbucket Cloud, GitHub or GitLab, you can create a branch directly from your Jira issue. Under Development, select Create branch. If you create a branch this way, the issue key is automatically added to the name of the branch.\\n\\n6. Git Commit Naming Convention', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Include the issue key in the commit message to link the commit to your Jira issue.\\n\\nFor example: git commit\\n\\n\\n\\nm \"feat: <commit message>\"\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, GitHub Enterprise, and Fisheye tools.\\n\\nThe <type> of a commit message should be a single word or abbreviation drawn from an ontology,\\n\\naccording to the nature of the project. This document specifies a programming ontology, with the\\n\\nfollowing elements:\\n\\nfeat: A new feature\\n\\nfix: A bug fix\\n\\ndocs: Documentation only changes\\n\\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing\\n\\nsemicolons, etc)\\n\\nrefactor: A code change that neither fixes a bug nor adds a feature\\n\\nperf: A code change that improves performance\\n\\ntest: Adding missing tests\\n\\nbuild: Changes to the build/compilation/packaging process or auxiliary tools such as\\n\\ndocumentation generation\\n\\nci: Changes in the continuous integration/delivery setup\\n\\n7. Pull Request Guidelines', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Include a commit in the pull request that has the issue key in the commit message. Note, the\\n\\ncommit cannot be a merge commit.\\n\\nInclude the issue key in the pull request title.\\n\\nEnsure that the source branch name also includes the issue key in the branch name.\\n\\nThis works by default in connected Bitbucket, GitLab, GitHub, and GitHub Enterprise tools.\\n\\nIf you create the pull request from the development panel in a Jira issue, the issue key is added\\n\\nautomatically.\\n\\n8. Quality Process and Methodologies\\n\\nRI Quality Assurance process is an integral part of every sprint of the iterative Software Development Life Cycle. A risk-based testing approach as mentioned below ensures adequate coverage of testing requirements. Jira is used at RI as a defect tracking and logging software.\\n\\n8.1 Security Testing\\n\\nSecurity Testing at RI covers vulnerabilities of the application and protects the\\n\\napplication from possible intruders', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Vulnerability Scanning, Security Scanning, Penetration Testing, Ethical Hacking and\\n\\nRisk Assessment are few of the activities that are carried out as part of security\\n\\ntesting\\n\\n8.2 QA Deliverables\\n\\nWith a mature testing process with RI, following deliverables are created by the QA team ensuring adequate coverage of requirements.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHigh Level Test Plan\\n\\nRequirement Traceability\\n\\nTest Execution Results\\n\\nTest Summary Reports\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDetailed Test Cases\\n\\nSmoke/Sanity Test\\n\\nDefects Logs\\n\\nUser Acceptance Testing\\n\\n9. Deployment Guidelines\\n\\nAutomated build pipelines will increase the productivity and seamless release to dev, staging or production environments.\\n\\nGit repositories are configured with pipelines which create docker images and get deployed via K8 pods as shown above.\\n\\n9.1. Frontend deployment\\n\\nFront-end code is managed via Github and AWS Code Pipeline. The frontend versions are managed via AWS Amplify.\\n\\n10. Architecture\\n\\n10.1. High\\n\\n\\n\\nLevel Architecture', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='10.2. Backend Flask Architecture\\n\\n10.3. Backend Workflow\\n\\nBackend workflow depends on the following components.\\n\\n1. User\\n\\nLogin with google account to get authorised from google , after login successfully will get response ie. user id, expiry time and tokens which can use to create “Events”\\n\\n1.1. Login\\n\\nLogin: To login there will be 2 ways of it, as shown in above fig. One of it is, from the browser with the link http://rapid-trance.rapidinnovation.tech:5000/google-login after access we got the response with oauth access token, refresh token, expiry token, user id, email, name. The access token expires in 1 hour. After that we can refresh it with a refresh token. After login it creates a json response and saves this user details to the database.\\n\\nThe another way is also same in this we need to just login with rapidview portal with', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='link https://rapid-trance.rapidinnovation.tech/api/v1/users here we got the response with oauth access token, refresh token, oauthExpiryIn. this redirected to google apis portal with oauth access token with this we got the user details with verified email address and after this saved this details to database.\\n\\nThis both ways are secured with the google oauth. Login with google account to get\\n\\nauthorised from google\\n\\n2. Meetings\\n\\n2.1 Create events After login successfully done, To create events with the required inputs are, event(meet) title, start time, end time, guest list, meeting notes, attachments if required, and other details. Below is the detailed architecture of the meetings.', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content=\"To create an event after login, https://rapid-trance.rapidinnovation.tech/api/v1/meeting this api end point is used. All the attachments of the event are saved in the organiser's google drive and he is able to give access to a particular person. If the meeting is created on a daily basis or weekly basis then, that event and required details is fetched from the database.\\n\\n3. Transcripts\\n\\nTranscripts: After successfully create event or schedule meet, by using crontab the post API is hit and the celery worker is create a task in a celery queue and then the Rapid Innovation bot is join the meet, with cation on and fetch all transcript (live captions) from the meet by using selenium python and also use beautiful soup python library to get accurate captions from the meet, and after meeting ends, we save that transcripts to csv and in json format. CSV is stored in amazon s3 bucket and the json of transcript is saved in the database after this, that celery task is completed.\", metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Basically, after generating the json of transcripts we also capture the attendance list of people who are joining the meet and absent in the meet. With the help of transcript json and the attendance list, we create a statistical metric, this will be explained below.\\n\\n3.1. Silence duration Silence Duration: This is the silence count of people during the meeting. This shows how much time all people will silence in the meeting. According to this we can calculate silence duration and save this duration into the database after the meeting ended.\\n\\n3.2. Statistical metrics Statistical Metric: The statistical metric is dependent on the transcript and the attendance list of the particular meet, there are following component of statistical metric which are,\\n\\n1. Participation: This one is to calculate the percentage of participation of people which\\n\\ngives you, how much talk a person has in a meet.', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='2. Planned Duration: This gives an actual meet duration decided. 3. Spent Duration: This is an actual spend time in a meet.\\n\\n4. Present List: This is a list of people who are present in that meeting according to the\\n\\nattendance list.\\n\\n5. Absent List: This is a list of people who are absent in that meeting according to the\\n\\nattendance list. Late Joiner: This gives a list of people who join the meeting late.\\n\\n6. 7. Early Exit: This gives a list of people who exit the meet early as scheduled time.\\n\\nAll components are saved into the database and we have a get API to get all components as response of statistical metric.\\n\\n10.4. Cloud Infrastructure\\n\\nMore architecture document goes here like Application level diagram, Sequence Diagram, Dataflow diagram, ER Diagram etc.\\n\\n11. User Stories and Functional Requirements\\n\\n11.1. Persona #1: Platform User\\n\\n1.1\\n\\nPlatform User: User Story\\n\\nAs a Platform User, I want to be able to view the landing page so that I can learn about the platform\\n\\n1.1.1', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Feature\\n\\nThis feature allows the user to explore and create NFT pages by clicking on relevant buttons from hero section\\n\\n1.1.2\\n\\nDescription\\n\\nThe landing page consists of content describing how the platform helps the user in buying and selling NFTs. The landing page comprises seven sections: title content, notable drops, top collection, trending, creating and selling your NFTs, categories, and introductory video.\\n\\n1.1.3\\n\\nPurpose\\n\\nThe purpose of this feature allows the user to get to know the platform and provide a better introduction\\n\\n1.1.4\\n\\nPage Sections\\n\\nLanding Page Header: Explains the platform in a short\\n\\nand crisp manner.\\n\\nNotable Drops: Shows the collections that are popular\\n\\nand available for purchase\\n\\nTop Collection: Shows the collectibles that are popular\\n\\nfor a certain period.\\n\\nTrending: Shows the collectibles that are trending in\\n\\nplatform rankings\\n\\nCreate and Sell your NFTs: Explains to the user how\\n\\nthey can create, store and sell their NFTs on the\\n\\nplatform.', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='Categories: Shows all categories of the NFTs available\\n\\non the marketplace.\\n\\n1.1.5\\n\\nAPIs Called\\n\\n1. NotableDrop\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collection/fetchNotabl\\n\\neDrops\\n\\n2. Top Collection\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collections/topCollecti\\n\\nons\\n\\n3. Trending Collection\\n\\n\\n\\n{{BASE_URL}}/api/v1/collections/collections/trendingCo\\n\\nllections/all\\n\\n4. Categories\\n\\n\\n\\n{{BASE_URL}}/api/v1/user/getAllCategories\\n\\n1.1.6\\n\\nUser Flow\\n\\nEnter the URL(Jungle.Domain) >> Land on the homepages\\n\\n1.2\\n\\nPlatform User: User Story\\n\\nAs a Platform User, I want to be able to sign up and create my profile so that I can share and view my collections\\n\\n1.2.1\\n\\nFeature\\n\\nThis feature allows users to sign up and create their profile on the platform by connecting their wallet\\n\\n1.2.2\\n\\nDescription\\n\\nThis feature enables users to view their created and collected NFTs/bundles and view the NFTs that have been marked as favourite.\\n\\n1.2.3\\n\\nPurpose', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content=\"To enable the platform user to sign up and use the platform by adding their profile\\n\\n1.2.4\\n\\nPage Sections\\n\\nAdd Wallet: The user would need an Ethereum wallet\\n\\nto connect to the platform.\\n\\nFilter: Shows options to filter out NFTs according to the\\n\\nuser’s desire.\\n\\n1.2.5\\n\\nAPIs Called\\n\\nLogin/Create Account\\n\\n\\n\\nPOST {{BASE_URL}}/api/v1/user\\n\\n1.2.6\\n\\nUser Flow\\n\\nHome page >> Connect wallet (right top corner) >> Profile section\\n\\n12. Models\\n\\n1. Sentiment Model Path:\\n\\nhttps://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/\\n\\npolarity (Open Source)\\n\\n2. questions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/question_extraction (Open Source)\\n\\n3. keybert_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\nProduction/tree/main/all-mpnet-base-v2 (Open Source)\\n\\n4. action_bert_model_path='https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\", metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='ML-Production/tree/main/bert-base-uncased (Open Source)\\n\\n5. action_classifier_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTransc\\n\\nriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/action_item_model (Open Source)\\n\\n6. summary_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/MEETING_SUMMARY (Open Source)\\n\\n7. emotions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nM\\n\\nL\\n\\n\\n\\nProduction/tree/main/emotions (Open Source)\\n\\n8. all\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Sentence Transformer Model)=\\n\\nhttps://www.sbert.net/docs/pretrained_models.html (Open Source)\\n\\n9. paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (SBertSummarizer Model)\\n\\n=https://huggingface.co/sentence\\n\\n\\n\\ntransformers/paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Open\\n\\nSource)\\n\\n13. Cases the bot will not join / FAIL.\\n\\nS.No Meeting Titles\\n\\nError\\n\\nDescription\\n\\n1\\n\\n2\\n\\n1. PixelPark Challenges,\\n\\n2. PM Standup\\n\\nYou can’t join this\\n\\nvideo call.\\n\\n1. BUSTA Games\\n\\n\\n\\nBOps\\n\\nMeeting,', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'}), Document(page_content='2. Weely meet with\\n\\nShreyas Bafna\\n\\nThe meeting code\\n\\nthat you entered\\n\\ndoesn’t work\\n\\nMake sure that you’ve entered the correct meeting code in the URL, e.g. https://meet.google.com/xxx-yyyy-zz z\\n\\n14. Reference and Supporting Documents\\n\\nList of important references and documents goes here', metadata={'source': 'Rapid Trance - Technical Specification Document (TSD).pdf'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "9IhIEvyzEtQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the Pinecone vector store\n",
        "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "              environment=os.getenv(\"PINECONE_ENVIRONMENT\"),\n",
        "              service_name=os.getenv(\"PINECONE_SERVICE_NAME\"))"
      ],
      "metadata": {
        "id": "BDJqCeJ6EtS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create index in pinecone\n",
        "def create_index(index_name, vector_dim):\n",
        "    active_indexes = pinecone.list_indexes()\n",
        "    if index_name not in active_indexes:\n",
        "        pinecone.create_index(index_name, dimension=vector_dim)\n",
        "    return pinecone.Index(index_name=index_name)"
      ],
      "metadata": {
        "id": "Z710rKu1EtVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'rapid-trance-index'\n",
        "namespace = 'pdf-namespace'\n",
        "\n",
        "\n",
        "online_doc_search = Pinecone.from_texts(\n",
        "    [t.page_content for t in online_paragraphs],\n",
        "    embeddings,\n",
        "    index_name=index_name,\n",
        "    namespace=namespace\n",
        ")\n",
        "    \n",
        "print(online_doc_search)"
      ],
      "metadata": {
        "id": "8TZ0D1cfFECc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'rapid-trance-index'\n",
        "namespace = 'pdf-namespace'\n",
        "\n",
        "\n",
        "offline_doc_search = Pinecone.from_texts(\n",
        "    [t.page_content for t in offline_paragraphs],\n",
        "    embeddings,\n",
        "    index_name=index_name,\n",
        "    namespace=namespace\n",
        ")\n",
        "    \n",
        "print(offline_doc_search)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXbtzwmJLmwL",
        "outputId": "40a04d55-5590-4cfd-a4a5-89d35531c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain.vectorstores.pinecone.Pinecone object at 0x7ff4fcc5e3d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"List of all the functional requirement?\""
      ],
      "metadata": {
        "id": "ivV4EmgQKG-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_docs = online_doc_search.similarity_search(query, include_metadata=True)\n",
        "print(online_docs)"
      ],
      "metadata": {
        "id": "e-kC9_AmKEJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offline_docs = offline_doc_search.similarity_search(query, include_metadata=True)\n",
        "print(offline_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf9-8rMBLttN",
        "outputId": "e33fe24a-7aa3-432a-f994-52357befc8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='your Data Science project, the skills requirements of the team will change. You will need to re-balance the equation to ensure the reactants balance with  the products.THE FIELD GUIDE to  DATA SCIENCE', metadata={}), Document(page_content='meta-analysis during product reviews.Booz Allen Hamilton was engaged to research, develop, and evaluate emerging informatics tools, methods, and techniques to determine their ability to address regulatory challenges faced by the FDA Center for Drug Evaluation and Research (CDER). The main goal was to enable the CDER community to fully utilize the agency’s expansive data resources for efﬁcient and effective drug review through the design and development of informatics capabilities based on Natural Language Processing (NLP), data integration, and data visualization methodologies.Our ApproachTo support transformational change at CDER, we designed and developed a set of informatics prototypes for the analysis and modeling of complex structured, unstructured, and fragmented datasets. We developed multiple prototypes to enable the evaluation of emerging informatics tools, methods, and techniques and their ability to enable a critical-value driver – e.g., co-locate complex, heterogeneous data', metadata={}), Document(page_content='There are a number of open source software packages that support format conversion and can interpret a wide variety of formats. One notable package is Apache Tikia.\\n\\nIngersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.\\n\\nFuzzy Logic\\n\\nLogical reasoning that allows for degrees of truth for a statement.\\n\\nUtilize when categories are not clearly deﬁned. Concepts such as \"warm\", \"cold\", and \"hot\" can mean different things at different temperatures and domains.\\n\\nZadeh L.A., \"Fuzzy Sets.”\\n\\nInformation and Control.\\n\\nCalifornia: University of\\n\\nCalifornia, Berkeley, 1965. Print.\\n\\nGaussian\\n\\nFiltering\\n\\nActs to remove noise\\n\\nor blur data.\\n\\nCan be used to remove speckle noise from images.\\n\\nGeneralized\\n\\nLinear Models\\n\\nExpands ordinary linear\\n\\nregression to allow\\n\\nfor error distribution\\n\\nthat is not normal.\\n\\nUse if the observed error in your system does not follow the normal distribution.\\n\\nGenetic', metadata={}), Document(page_content='Case studies from which you have learned\\n\\nCitations from journal articles or papers that inspire you\\n\\nAlgorithms and techniques that you love\\n\\nYour thoughts and comments on other people’s additions\\n\\nEmail us your ideas and perspectives at data_science@bah.com or submit them via a pull request on the GitHub repository.\\n\\nJoin our conversation and take the journey with us. Tell us and the world what you know. Become an author of this story.', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "IscFLinGFEFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stuff\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "pIgplPjgFEHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "i7-V1qtokYXn",
        "outputId": "fdc9bf8a-1851-4791-c5fc-73cd2b283b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. As an admin User should have the option to select the gas fee depending on the demand for each particular function.\\n2. The system should display Liquity Loans protocol statistics (a. ETH, LUSD and LQTY prices with links to Coingecko page for token, price charts, etherscan details and local DEX pair, b. Current Borrowing fee rate, c. TVL for the LL protocol, d. Number of vaults, e. LUSD supply, f. LUSD in stability pool (total $ and % of supply), g. Total collateral ratio (for the protocol), h. Recovery mode status, i. Recovery mode price threshold, j. Kickback rate for the front-end, k. LL contract version, date/time deployed, front-end version).\\n3. User Should be able to borrow and repay loans.\\n4. User Should be able to create new vault/trove, deposit ETH.\\n5. User Should be able to know available loan size (target CR loan value - current debt) - in RED if current CR is under 103% of target CR Green if equal to or greater than 103% of target CR (Green means recommended to compound, RED means wait).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map_reduce\n",
        "chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "fcxnv1soMEz4",
        "outputId": "147c3eb1-eef1-435a-98d6-a3d2f1378cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The functional requirements are: \\n- ETH, LUSD and LQTY prices with links to Coingecko page for token, price charts, etherscan details and local DEX pair \\n- Current Borrowing fee rate \\n- TVL for the LL protocol \\n- Number of vaults \\n- LUSD supply \\n- LUSD in stability pool (total $ and % of supply) \\n- Total collateral ratio (for the protocol) \\n- Recovery mode status \\n- Recovery mode price threshold- \\n- Kickback rate for the front-end \\n- LL contract version, date/time deployed, front-end version \\n- User should be able to create new vault/trove, deposit ETH\\n- Loan Compounding target selector \\n- Top up Vault (LUSD->ETH ->LL Vault)\\n- New BOXABL NFT (LUSD->BOXABL NFT market)\\n- Top up Stability Pool (LUSD->Stability Pool)\\n- Top up Staking Pool (LUSD->LQTY->Staking Pool)\\n- Top up staked WATT (LUSD->WATT->CORE Staking)\\n- Select up to three places to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# refine\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\")\n",
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "hgTuffd2Us9J",
        "outputId": "a940d1bb-53e8-4e5f-caee-cd2ad090381a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. Admin users should have the option to select the gas fee depending on the demand for each particular function.\\n2. Logic/financial flow schematic should be provided.\\n3. Wireframes should be provided.\\n4. Figma designs should be provided.\\n5. Logo, colors and layout should be provided.\\n6. Investor pitch deck should be provided.\\n7. Other designs should be provided.\\n8. Committed deliverables should be outlined in Appendix A.\\n9. Requirements for later phases should be captured in Appendix B.\\n10. Important references and similar platforms should be provided.\\n11. The system should display Liquity Loans protocol statistics, including ETH, LUSD and LQTY prices with links to Coingecko page for token, price charts, etherscan details and local DEX pair, current borrowing fee rate, TVL for the LL protocol, number of vaults, LUSD supply, LUSD in stability pool (total $ and % of supply), total collateral ratio (for the protocol), recovery mode status, recovery mode price threshold, kickback rate for the front-end, LL contract version, date/time deployed, and front-end version.\\n12. User should be able to create new'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map_rerank\n",
        "chain = load_qa_chain(llm, chain_type=\"map_rerank\")\n",
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "65iC9w3OUwud",
        "outputId": "c832f1f9-b564-4a4d-9174-3f598f2d673c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'User Should be able to create new vault/trove, deposit ETH'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which all Technology you are using in the project?\""
      ],
      "metadata": {
        "id": "RF0ezkccurgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offline_docs = offline_doc_search.similarity_search(query, include_metadata=True)\n",
        "print(offline_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPW7-XNEwrNa",
        "outputId": "07fde911-3cc4-4715-c789-cc49fa572bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Published on September 09, 2022 - use major version for new phases or major changes\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nsummary of changes (can be bulleted)\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nuse minor version for fixes or updates after client reviews\\n\\n2. Technology Stack\\n\\nWeb Application\\n\\nPurpose\\n\\nCommon\\n\\nFrontend stack\\n\\nBackend\\n\\nDatabase\\n\\nInfrastructure\\n\\nPurpose\\n\\nStorage of Metadata\\n\\nFrontend\\n\\nContainerisation\\n\\nVPC\\n\\nCI/CD\\n\\nPython\\n\\nPurpose\\n\\nTechnology\\n\\nPython | Pycharm\\n\\nReactJS | Redux | Webpack | Styled Components | Formik\\n\\nPython | Flask | MongoDB\\n\\nMongoDB\\n\\nTechnology\\n\\nAWS S3\\n\\nAWS Amplify | AngularJS\\n\\nAWS ECS\\n\\nPrivate | Public Subnets\\n\\nAWS ECR | AWS Code Build | AWS Code Pipeline\\n\\nTechnology\\n\\nMeetings, Transcripts, User\\n\\nMajor SDKs/Libraries\\n\\nGoogle Calendar API\\n\\nBeautifulsoup | google oauth\\n\\n| Python Selenium | Python\\n\\nFlask, pymongo, nltk, boto3,\\n\\nsummarizer, Openai, sentence\\n\\n\\n\\ntransformers\\n\\nsentry, Bert extractive\\n\\n3. Assumptions and Risks', metadata={}), Document(page_content='Published on September 09, 2022 - use major version for new phases or major changes\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nsummary of changes (can be bulleted)\\n\\nMonth DD, YYYY\\n\\n<<author\\n\\n\\n\\nname>>\\n\\nuse minor version for fixes or updates after client reviews\\n\\n2. Technology Stack\\n\\nWeb Application\\n\\nPurpose\\n\\nCommon\\n\\nFrontend stack\\n\\nBackend\\n\\nDatabase\\n\\nInfrastructure\\n\\nPurpose\\n\\nStorage of Metadata\\n\\nFrontend\\n\\nContainerisation\\n\\nVPC\\n\\nCI/CD\\n\\nPython\\n\\nPurpose\\n\\nTechnology\\n\\nPython | Pycharm\\n\\nReactJS | Redux | Webpack | Styled Components | Formik\\n\\nPython | Flask | MongoDB\\n\\nMongoDB\\n\\nTechnology\\n\\nAWS S3\\n\\nAWS Amplify | AngularJS\\n\\nAWS ECS\\n\\nPrivate | Public Subnets\\n\\nAWS ECR | AWS Code Build | AWS Code Pipeline\\n\\nTechnology\\n\\nMeetings, Transcripts, User\\n\\nMajor SDKs/Libraries\\n\\nGoogle Calendar API\\n\\nBeautifulsoup | google oauth\\n\\n| Python Selenium | Python\\n\\nFlask, pymongo, nltk, boto3,\\n\\nsummarizer, Openai, sentence\\n\\n\\n\\ntransformers\\n\\nsentry, Bert extractive\\n\\n3. Assumptions and Risks', metadata={}), Document(page_content='The components involved in any Data Science project fall into a number of different categories including the data types analyzed, the analytic classes used, the learning models employed and the execution models used to run the analytics. The interconnection across these components, shown in the figure, Interconnection Among the Component Parts of Data Science, speaks to the complexity of engineering Data Science solutions. A choice made for one component exerts influence over choices made for others categories. For example, data types lead the choices in analytic class and learning models, while latency, timeliness and algorithmic parallelization strategy inform the execution model. As we dive deeper into the technical aspects of Data Science, we will begin with an exploration of these components and touch on examples of each.\\n\\nRead this to get the quick and dirty:', metadata={}), Document(page_content='your Data Science project, the skills requirements of the team will change. You will need to re-balance the equation to ensure the reactants balance with  the products.THE FIELD GUIDE to  DATA SCIENCE', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stuff\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "rTQHxobiwhg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-IJpCAgLwksC",
        "outputId": "08d69343-b656-4b90-ccbf-0211ceb35c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The technologies used in the project include Python, Pycharm, ReactJS, Redux, Webpack, Styled Components, Formik, Flask, MongoDB, AWS S3, AWS Amplify, AngularJS, AWS ECS, Private and Public Subnets, AWS ECR, AWS Code Build, AWS Code Pipeline, Google Calendar API, Beautifulsoup, google oauth, Python Selenium, Flask, pymongo, nltk, boto3, summarizer, Openai, sentence transformers, sentry, and Bert extractive.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which all Models you are using in the project?\""
      ],
      "metadata": {
        "id": "5F--IvRTwmKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offline_docs = offline_doc_search.similarity_search(query, include_metadata=True)\n",
        "print(offline_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGRw4irnw9uI",
        "outputId": "58d744ec-605a-4223-ebbb-0c9ae669a1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Model ValidationRepeating what you just heard does not mean that you  learned anything. Model validation is central to construction of any model. This answers the question “How well did my hypothesis fit the observed data?”If we do not have enough data, our models cannot connect the dots. On the other hand, given too much data the model cannot think outside of the box. The model learns specific details about the training data that do not generalize to the population. This is the problem of model over fitting. Many techniques exist to combat model over fitting. The simplest method is to split your dataset into training, testing and validation sets. The training data is used to construct the model. The model constructed with the training data is then evaluated with the testing data. The performance of the model against the testing set is used to further reduce model error. This indirectly includes the testing data within model construction, helping to reduce model over fit. Finally, the', metadata={}), Document(page_content='ML-Production/tree/main/bert-base-uncased (Open Source)\\n\\n5. action_classifier_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTransc\\n\\nriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/action_item_model (Open Source)\\n\\n6. summary_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/MEETING_SUMMARY (Open Source)\\n\\n7. emotions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nM\\n\\nL\\n\\n\\n\\nProduction/tree/main/emotions (Open Source)\\n\\n8. all\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Sentence Transformer Model)=\\n\\nhttps://www.sbert.net/docs/pretrained_models.html (Open Source)\\n\\n9. paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (SBertSummarizer Model)\\n\\n=https://huggingface.co/sentence\\n\\n\\n\\ntransformers/paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Open\\n\\nSource)\\n\\n13. Cases the bot will not join / FAIL.\\n\\nS.No Meeting Titles\\n\\nError\\n\\nDescription\\n\\n1\\n\\n2\\n\\n1. PixelPark Challenges,\\n\\n2. PM Standup\\n\\nYou can’t join this\\n\\nvideo call.\\n\\n1. BUSTA Games\\n\\n\\n\\nBOps\\n\\nMeeting,', metadata={}), Document(page_content='ML-Production/tree/main/bert-base-uncased (Open Source)\\n\\n5. action_classifier_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTransc\\n\\nriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/action_item_model (Open Source)\\n\\n6. summary_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nML\\n\\n\\n\\nProduction/tree/main/MEETING_SUMMARY (Open Source)\\n\\n7. emotions_model_path=https://github.com/Rapidinnovation\\n\\n\\n\\nAIML/Rapid\\n\\n\\n\\nTranscriber\\n\\n\\n\\nM\\n\\nL\\n\\n\\n\\nProduction/tree/main/emotions (Open Source)\\n\\n8. all\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Sentence Transformer Model)=\\n\\nhttps://www.sbert.net/docs/pretrained_models.html (Open Source)\\n\\n9. paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (SBertSummarizer Model)\\n\\n=https://huggingface.co/sentence\\n\\n\\n\\ntransformers/paraphrase\\n\\n\\n\\nMiniLM\\n\\n\\n\\nL6\\n\\n\\n\\nv2 (Open\\n\\nSource)\\n\\n13. Cases the bot will not join / FAIL.\\n\\nS.No Meeting Titles\\n\\nError\\n\\nDescription\\n\\n1\\n\\n2\\n\\n1. PixelPark Challenges,\\n\\n2. PM Standup\\n\\nYou can’t join this\\n\\nvideo call.\\n\\n1. BUSTA Games\\n\\n\\n\\nBOps\\n\\nMeeting,', metadata={}), Document(page_content=\"If the data structure is unknown, transparent model, start with:  If you have <20 data dimensions, If you only have knowledge of how people interact with items, If you have a feature vector of item characteristics, start with:   If you only have knowledge of how items are connected to one ADVISE What course of action should I take?If you have expert knowledge to capture › Expert systemsIf you're looking for basic facts › Logical reasoningLOGICAL REASONINGHow do I sort through different evidence?If you must model discrete entities, start with:    › Discrete event simulation (DES)If there are a discrete set of possible states, start with:    › Markov modelsIf there are actions and interactions among autonomous entities, start with:    › Agent-based simulationIf you do not need to model discrete entities, start with:    › Monte Carlo simulationIf you are modeling a complex system with feedback mechanisms between actions, start with:    › Systems dynamicsIf you require continuous tracking\", metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stuff\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "2bYJz2wrw-Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=offline_docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6eeBp8wFw_JS",
        "outputId": "9cd32bec-b3f5-41f1-876a-81d43c4dcd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' We are using the following models in the project: bert-base-uncased, action_item_model, MEETING_SUMMARY, emotions, all MiniLM L6 v2 (Sentence Transformer Model), paraphrase MiniLM L6 v2 (SBertSummarizer Model).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oq32TQtCxAIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}